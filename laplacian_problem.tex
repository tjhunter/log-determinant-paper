
\section{Making the problem laplacian}

\label{sec:Making-the-problem}

From now on, we consider all matrices to be in $SDD_{n}$. Most techniques
we will be using work on Laplacian matrices instead of SDD matrices.
An SDD matrix is positive semi-definite while a Laplacian matrix is
always singular, since its nullspace is spanned by $\mathbf{1}$.
We generalize the definition of the determinant to handle this technicality. 

\begin{definition} \emph{Pseudo-log-determinant (PLD):} Let $A\in\mathcal{S}^{n+}$
be a non-null positive semi-definite matrix. The pseudo-log-determinant
is defined by the sum of the logarithms of all the positive eigenvalues:
\[
\text{ld}\left(A\right)=\sum_{\lambda_{i}>0}\log\left(\lambda_{i}\right)
\]
where $\lambda_{i}$ are the eigenvalues of $A$. \end{definition}

The interest of the PLD lies in the connection between SDD matrices
and their associated Laplacian. Recall that an SDD matrix can be transformed
into a Laplacian (which has all off-diagonal terms non-positive, and
for which the sum of each row and each column is zero). As underlined
in GREMBAN, solving an equation involving an SDD can be turned into
solving an equation with an SDD in which all the off-diagonal terms
are non-positive of twice the same size. This transform is also useful
for computing determinants. The following lemma shows that computing
the log-determinant of an SDD with positive off-diagonal entries can
be turned into computing the log-determinants of two SDD matrices
with non-positive off-diagonal entries.

\begin{lemma}\label{non-negative-conversion}Consider the decomposition
of a SDD matrix into a positive and a negative sum:
\[
A=P-N
\]
with $P_{ij}\geq0$ and $N_{ij}\geq0$. Then the matrices $\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)\in SDD_{2n}$ and $P+N\in SDD_{n}$ and we have the relation:
\[
\log\left|\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)\right|=\log\left|P+N\right|+\log\left|P-N\right|
\]


\end{lemma}

\begin{proof} Call $C=\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)$

Consider $\mu$ an eigenvalue of $A$, and $x$ an associated eigenvector:
$Ax=\mu x$. Then:
\[
\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)\left(\begin{array}{c}
x\\
-x
\end{array}\right)=\left(\begin{array}{c}
Ax\\
-Ax
\end{array}\right)=\mu\left(\begin{array}{c}
x\\
-x
\end{array}\right)
\]
which shows that the vector $\left(\begin{array}{c}
x\\
-x
\end{array}\right)$ is an eigenvector of $C$ with associated eigenvalue $\mu$ . Similarly,
if $\eta$ is an eigenvector of $B$ with associated eigenvalue $y$,
then $\eta$ is also an eigenvector of $C$ with associated eigenvalue
$\left(\begin{array}{c}
y\\
y
\end{array}\right)$. Since $C$ is exactly of size $2n$, the set of eigenvalues of $C$
is exactly the union of sets of eigenvalues of $A$ and $B$. Thus
$\log\left|C\right|=\sum_{i}\log\mu_{i}+\sum_{i}\log\eta_{i}=\log\left|A\right|+\log\left|B\right|$

\end{proof}

Recall that a Laplacian matrix can be constructed from any SDD with
non-positive off-diagonal entries matrix by adding a ``ground''
node (see Gremban's thesis \cite{Gremban1996}, Chapter 4).

\begin{definition} \emph{Augmented Laplacian of an SDD matrix (Definition
XXX in GREMBAN):} Let $A\in SDD_{n}$. Let $L,D$ be the decomposition
of $A$ into $L$ a Laplacian matrix and $D$ a (non-negative) diagonal
matrix such that $A=L+D$. The augmented matrix of $A$ is: 
\[
L_{A}=\left(\begin{array}{cc}
A & -\mathbf{d}\\
-\mathbf{d}^{T} & h
\end{array}\right)
\]
with $\mathbf{d}\in\mathbb{R}^{n}$: $\mathbf{d}_{i}=D_{ii}$ and
$h=\sum_{i}D_{ii}$. \end{definition}

From now on, given any $A\in SDD_{n}$, we will implicitly define
a Laplacian $L_{A}$ associated to it, using the augmented Laplacian.
This process is also reversible and can be used to define a matrix
$\tilde{A}\in SDD_{n}$ from $L_{A}$ with appealing properties in
a very simple way. We call this process ``floating'' of the Laplacian,
by opposition to ``grounding'' the SDD matrix as a Laplacian. 

The PLD enjoys some of the usual properties of the log-determinant.
The following proposition shows the interest of the PLD in the study
of the Laplacian of a graph. 

\begin{proposition} \label{pro:pld-properties}Let $A\in SDD_{n}$,
and $L_{A}$ its augmented Laplacian. Let $P=\left(I_{n}\,\mathbf{0}\right)$
be a projection matrix over the first $n$ columns of $L_{A}$. Then:\end{proposition} 
\begin{itemize}
\item If $\lambda$ is an eigenvalue of $A$ with associated eigenvector
$x$, it is also an eigenvalue of $PL_{A}P^{T}$ with the same eigenvector 
\item $\text{ld}\left(L_{A}\right)=\log\left|A\right|$ 
\item Let $A,B\in SDD_{n}$. $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(L_{A}\right)+\text{ld}\left(L_{B}\right)$
and $\text{ld}\left(L_{B}^{+}L_{A}\right)=\text{ld}\left(L_{A}\right)-\text{ld}\left(L_{B}\right)=-\text{ld}\left(L_{A}^{+}L_{B}\right)$
with $L_{A}^{+}$the pseudo inverse of $L_{A}$. 
\item Let $A,B\in SDD_{n}$, then $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(\sqrt{L_{A}}L_{B}\sqrt{L_{A}}\right)$
and $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(\left(\sqrt{L_{A}}\right)^{+}L_{B}\left(\sqrt{L_{A}}\right)^{+}\right)=\text{ld}\left(\sqrt{L_{A}^{+}}L_{B}\sqrt{L_{A}^{+}}\right)$ 
\item Let $A\in SDD_{n}$ with $\left\Vert I-A\right\Vert <1$ then $\mbox{ld}\left(L_{A}\right)=-\mbox{Tr}\left(\log_{P}^{+}\left(L_{A}\right)\right)=-\mbox{Tr}\left(\log\left(A\right)\right)$.
\end{itemize}
\begin{proof} The proofs of this proposition are technical and are
contained in Appendix A. \end{proof} Note in particular that the
PLD is well defined over connected graphs. From now on, we will consider
connected graphs only. If graphs have several unconnected components,
the log-determinant of the corresponding SDD is the sum of the PLDs
of the Laplacians of each component. \begin{lemma} The pseudo-log-determinant
of a connected graph is well-defined.\end{lemma} \begin{proof} A
connected graph $G$ has non-zero conductance $\Phi_{G}$ and using
Cheeger's inequality (see Theorem 4.1 in \cite{Spielman2010a}) $\lambda_{2}\geq\Phi_{G}^{2}>0$.
Hence $\lambda_{i}\geq\lambda_{2}>0$ for all $i\geq2$ and $\text{ld}\left(G\right)\in\mathbb{R}$.
\end{proof} From now on, we will focus on computing the PLD of Laplacians.
Note that the kernel of all Laplacian matrices of connected graphs
is precisely $\mathbb{R}\mathbf{1}_{n}$, so we can use the PLD with
pseudo inverses as if we were using the log determinant with SDD matrices:
Consider a graph $G=\left(V,E,w\right)$, identified with its Laplacian
matrix $L_{G}$. Consider a weighted subgraph $H=\left(V,\tilde{E},\tilde{w}\right)$
of $G$, for which it is easier to compute the determinant, and that
closely approximates $G$ in the spectral sense. From Proposition
\ref{pro:pld-properties}: 
\[
\log\left|G\right|=\text{ld}\left(L_{G}\right)=\text{ld}\left(L_{H}\right)+\text{ld}\left(L_{H}^{+}L_{G}\right)=\log\left|H\right|+\text{ld}\left(L_{H}^{+}L_{G}\right)
\]


We will see how to adapt Spielman and Teng's remarkable work on \emph{ultra-sparsifiers}
to produce good preconditioners $H$ for the determinant. In particular,
once a good preconditioners is available for $G$, we present an algorithm
that computes an upper bound and a lower bound of the PLD \emph{for
free}. We will improve this results to produce an algorithm that computes
an $\epsilon$-approximation with high probability.
