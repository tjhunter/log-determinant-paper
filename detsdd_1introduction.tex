
\section{Introduction}

We consider the problem of computing the determinant of symmetric,
diagonally dominant (SDD) matrices, i.e. real symmetric matrices $A$
for which: 
\[
A_{ii}\geq\sum_{j\neq i}\left|A_{ij}\right|
\]
The set of all such matrices of size $n\times n$ is denoted $SDD_{n}$,
and the set of all symmetric real matrices is called $\mathcal{S}_{n}$.
Call $m$ the number of non-zero entries in $A$. We are interested
in computing the determinant of sparse matrices, i.e. matrices for
which $m\ll n^{2}$.

The best exact algorithm known for computing the determinant of general
matrices, the Cholesky factorization, runs in a cubic complexity $O\left(n^{3}\right)$.
Computing the factorization can be sped up for a few specific patterns
such as trees, but no algorithm has been shown to work in a generic
way for $SDD_{n}$, let alone general symmetric matrices. We present
an algorithm that returns an approximation of the logarithm of the
determinant in time quasi-linear with the number of non-zero entries
of $A$. More specifically, we show that our algorithm, \texttt{UltraLogDet},
computes an $\epsilon$-approximation of the logarithm of the determinant
with high probability and in expected time%
\footnote{We use the notation $\tilde{O}$ to hide a factor at most $\left(\log\log n\right)^{8}$%
}: 
\[
\tilde{O}\left(m\epsilon^{-2}\log^{3}n\log^{2}\left(\frac{n\kappa_{A}}{\epsilon}\right)\right)
\]
%\left(B\right)


where $\kappa_{A}$ is the condition number of $A$. This algorithm
builds upon the work of Spielmann and Teng on \emph{ultra-sparsifiers}
\cite{Spielman2009a}, and it critically exploits the recent improvements
from Koutis, Miller and Peng \cite{Koutis2010}. This is to our knowledge
the first algorithm that presents a nearly linear complexity which
depends neither on the condition number of $A$ (except through a
log-term) nor on a specific pattern for the non-zero coefficients
of $A$.

The high sophistication of the algorithm transpires through the large
exponent of $\log\log n$. However, our algorithm will directly benefit
from any improvement on ultra-sparsifiers. Given the considerable
practical importance of such preconditioners, we expect some fast
improvements in this area. Also, the bulk of the work is performed
in a Monte Carlo procedure that is straightforward to parallelize.
Furthermore, we also present simpler, non-optimal algorithms that
compute upper and lower bounds of the logarithm of the determinant,
and that may be of more immediate practical interest.


\subsection{Background}

There are two approaches in numerical linear algebra to approximately
compute a determinant (or the log of the determinant): by performing
a (partial) Cholesky factorization of $A$, or by considering the
trace of some power series.

As mentioned above, the Cholesky factorization performs a decomposition
of the form: $A=PLDL^{T}P^{T}$ with $P$ a permutation matrix, $L$
a low-triangular matrix with $1$ on the diagonal and $D$ a diagonal
matrix of non-negative coefficients. Then the log-determinant of $A$
is simply%
\footnote{We will use the $\left|\cdot\right|$ operator to denote the determinant,
it will be clear from the context that it is different from the absolute
value.%
}: 
\[
\log\left|A\right|=\sum_{i}\log D_{ii}
\]
The complexity of dense Cholesky factorization for dense matrices
is $O\left(n^{3}\right)$. Unfortunately, Cholesky factorization usually
does not gain much from the knowledge of the sparsity pattern due
to the \emph{fill-in problem} (see \cite{meurant1999computer}, section
3.2). There is one case, though, for which Cholesky factorization
is efficient: if the sparsity pattern of $A$ is a tree, then performing
Cholesky factorization takes $O\left(n\right)$ time, and the matrix
$L$ is a banded matrix \cite{liu1990eliminationtrees}. If the sparsity
pattern of $A$ is not a tree, however, this advantageous decomposition
does not hold anymore.

When the matrix $A$ is close to the identity, more precisely when
the spectral radius of $M=A-I$ is less than~$1$, one can use the
remarkable Martin expansion of the log-determinant \cite{martin1992approximations}:
\begin{equation}
\log\left|A\right|=\text{Tr}\left(\log A\right)\label{eq:martin-expansion}
\end{equation}
where $\log A$ is the matrix logarithm defined by the series expansion:
\begin{equation}
\log A=\sum_{i=0}^{\infty}\frac{\left(-1\right)^{i}}{i+1}M^{i}\label{eq:matrix-log}
\end{equation}
The determinant can then be computed by a sum of traces of the power
of $M$, and the rate of convergence of this series is driven by the
spectral radius $M$. This line of reasoning has led researchers to
look for decompositions of $A$ of the form $A=U+V$ with the determinant
of $U$ being easier to compute and $U^{-1}V+I$ having a small spectral
radius. Then $\log\left|A\right|=\log\left|U\right|+\log\left|U^{-1}V+I\right|$.
The most common decomposition $U,V$ is in terms of block diagonal
and off-diagonal terms, which can then use Hadamard inequalities on
the determinant to bound the error \cite{Ipsen2006}. Diagonal blocks
also have the advantage of having determinants easy to compute. However,
this approach requires some strong assumptions on the condition number
of $A$, which may not hold in practice.

The trace approach is driven by \emph{spectral properties }(the condition
number) while the Cholesky approach is driven by \emph{graphical }properties\emph{
}(the non-zero pattern)\emph{. }We\emph{ }propose to combine these
two approaches by decomposing the problem with one component that
is close to a tree (and is more amenable to Cholesky methods), and
one component that has a bounded condition number. Our solution is
to use a \emph{spectral sparsifier} introduced by Spielman in \cite{Spielman2008}.


\subsection{Applications}

The problem of estimating determinants has important applications
in spatial data analysis, statistical physics and statistics. In spatial
statistics, it is often convenient to interpolate measurements in
a 2-, 3- or 4-dimensional volume using a sparse Gaussian process,
a technique known in the geospatial community as \emph{kriging }\cite{zhang2010kriging,li2005analysis}\emph{.
}Computing the optimal parameters of this Gaussian process involves
repeated evaluations of the partition function, which is a log-determinant.
In this context, a diagonally dominant matrix for the Gram matrix
of the process corresponds to distant interactions between points
of measure (which is verified in some contexts, see \cite{KelleyPace1997291}).
Determinants also play a crucial role in quantum physics and in theoretical
physics. The wave function of a system of multiple fermion particles
is an antisymmetric function which can be described as a determinant
(Slatter determinant, \cite{atkins2011molecular,lowdin1955quantum}).
In the theory of quantum chromodynamics (QCD), the interaction between
particles can be discretized on a lattice, and the energy level of
particles is the determinant of some functional operators over this
lattice \cite{duncan1998efficient}. It is itself a very complex problem
because of the size of the matrices involved for any non-trivial problem,
for which the number of variables is typically in the millions \cite{bernardson1994monte}.
In this setting, the restriction to diagonally dominant matrices can
be interpreted as an interaction between relatively massive particles
\cite{deForcrand1989516}, or as a bound on the propagation of interactions
between sites in the lattice \cite{bernardson1994monte}.

For these reasons, computing estimates of the log-determinant has
been an active problem in physics and statistics. In particular, the
Martin expansion presented in Equation \eqref{eq:martin-expansion}
is extensively used in quantum physics \cite{Ipsen2006}, and it can
be combined with sampling method to estimate the trace of a matrix
series (\cite{Zhang2008},\cite{McCourt2008},\cite{Zhang2007}).
Another different line of research has worked on bounds on the values
of the determinant itself. This is deeply connected to simplifying
statistical models using variational methods. Such a relaxation using
a message-passing technique is presented in \cite{Wainwright2006}.
Our method is close in spirit to Reuksen's work \cite{Reusken2002}
by the use of a preconditioner. However, Reuksen considers preconditioners
based on a clever approximation of the Cholesky decomposition, and
its interaction with the eigenvalues of the complete matrix is not
well understood. Using simpler methods based on sampling, we are able
to carefully control the spectrum of the remainder, which in turn
leads to strong convergence guarantees.


\subsection{A note on scaling}

Unlike other common characteristics of linear operators, the determinant
and the log-determinant are very sensitive to dimensionality. We will
follow the approach of Reuksen \cite{Reusken2002} and consider the
\emph{regularized log-determinant} $f\left(A\right)=n^{-1}\log\left|A\right|$
instead of the log-determinant. The regularized determinant has appealing
properties with respect to dimensionality. In particular, its sensitivity
to perturbations does not increase with the dimensionality, but only
depends on spectral properties of the operator $A$. For example,
calling $\lambda_{\min}$ and $\lambda_{\max}$ the minimum and maximum
eigenvalues of $A$, respectively: 
\[
\log\lambda_{\min}\leq f\left(A\right)\leq\log\lambda_{\max}
\]
\[
\left|f\left(A+\epsilon I\right)-f\left(A\right)\right|\leq\epsilon\left\Vert A^{-1}\right\Vert _{2}+O\left(\epsilon^{2}\right)
\]
The last inequality in particular shows that any perturbation to $\log\left|A\right|$
will be in the order $O\left(n\right)$, and so that all the interesting
log-determinants in practice will be dominated by some $O\left(n\right)$.


\subsection{Main results}

We first present some general results about the preconditioning of
determinants. Consider $A\in SDD_{n}$ invertible, and some other
matrix $B\in SDD_{n}$ that is close to $A$ in the spectral sense.
All the results of this article stem from observing that: 
\begin{eqnarray*}
\log\left|A\right| & = & \log\left|B\right|+\log\left|B^{-1}A\right|\\
 &  & \log\left|B\right|+\text{Tr}\left(\log\left(B^{-1}A\right)\right)
\end{eqnarray*}
The first section is concerned with estimating the remainder term
$\text{Tr}\left(\log\left(B^{-1}A\right)\right)$ using the Martin
expansion. The exact inverse $B^{-1}$ is usually not available, but
we are given instead a linear operator $C$ that is an $\epsilon-$approximation
of $B^{-1}$, for example using a conjugate gradient method. We show
in Section \ref{sec:Preconditioned-log-determinants} that if the
precision of this approximation is high enough, we can estimate the
remainder with high probability and with a reasonable number of calls
to the operator $C$ (this sentence will be made precise in the rather
technical Theorem \ref{thm:preconditioning-approx}). Using this general
framework, the subsequent Section \ref{sec:Making-the-problem} shows
that spectral sparsifiers make excellent preconditioners that are
close enough to $A$ and so that computing the Martin expansion is
not too expansive. In particular, we build upon the recursive structure
of Spielman-Teng ultra-sparsifiers to obtain our main result:

\begin{theorem}\label{thm:ultra_main}On input $A\in SDD_{n}$ with
$m$ non-zeros, $\eta>0$, the algorithm \texttt{UltraLogDet} returns
a scalar $z$ so that: 
\[
\mathbb{P}\left[\left|z-n^{-1}\log\left|A\right|\right|>\epsilon\right]\leq\eta
\]
and this algorithm completes in expected time $\tilde{O}\left(m\epsilon^{-2}\log^{2}n\log^{2}\left(\frac{\kappa_{A}}{\epsilon}\right)\log\left(\eta^{-1}\right)\right)$.
Moreover, if $\epsilon>\Omega(n^{-1})$, then the running time improves
by a factor $\epsilon$.

\end{theorem}

The rest of the article is structured as follows. In the next section,
we present some results about estimating the log-determinant from
a truncated expansion. These results will justify the use of \emph{preconditioners
}to compute the determinant of a matrix. The techniques developed
by Spielman et al. work on the Laplacians of weighted graphs. Section
3 introduces some new concepts to expand the notion of determinants
to Laplacian matrices, and presents a few straightforward results
in the relations between graph Laplacians and SDD matrices. Section
3.2 will use these new concepts to introduce a first family of preconditioners
based on low-stretch spanning trees. Finally, Section 3.3 contains
the proof of our main result, an algorithm to compute determinants
in near-linear time. 
