%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,english]{siamltex}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{float}
\usepackage{amstext}
\usepackage{amssymb}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\newcommand\eqref[1]{(\ref{#1})}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\include{amsmath}

\makeatother

\usepackage{babel}
  \providecommand{\corollaryname}{Corollary}
  \providecommand{\definitionname}{Definition}
  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
\providecommand{\theoremname}{Theorem}

\begin{document}

\title{Computing the log-determinant of symmetric, diagonally dominant matrices
in near-linear time\\
WORKING DRAFT}


\author{Timothy Hunter, Alexandre Bayen}

\maketitle

\section{Introduction}

We consider the problem of computing the determinant of symmetric,
diagonally dominant (SDD) matrices, i.e. real symmetric matrices $A$
for which:
\[
A_{ii}\geq\sum_{j\neq i}\left|A_{ij}\right|
\]
The set of all such matrices of size $n\times n$ is denoted $SDD_{n}$,
and the set of all symmetric real matrices is called $\mathcal{S}_{n}$.
Call $m$ the number of non-zero entries in $A$. We are interested
in computing the determinant of sparse matrices, i.e. matrices for
which $m\ll n^{2}$.

The best algorithm known for computing the determinant of general
matrices, the Cholesky factorization, runs in a cubic complexity $O\left(n^{3}\right)$.
Computing the factorization can be sped up for a few specific patterns
such as trees, but no algorithm has been shown to work in a generic
way for $SDD_{n}$, let alone general symmetric matrices. We present
an algorithm that returns an approximation of the logarithm of the
determinant in time quasi-linear with the number of non-zero entries
of $A$. More specifically, we show that our algorithm, \texttt{UltraLogDet},
computes an $\epsilon$-approximation of the logarithm of the determinant
in expected time%
\footnote{We use the notation $\tilde{O}$ to hide a factor at most $\left(\log\log n\right)^{8}$%
}:
\[
\tilde{\mathcal{O}}\left(m\left(\log n\right)^{7}\epsilon^{-2}\log\left(\frac{n\kappa_{A}}{\epsilon}\right)\right)
\]


where $\kappa_{A}$ is the condition number of $A$. This algorithm
builds upon the work of Spielmann and Teng on \emph{ultra-sparsifiers}
\cite{Spielman2009a}, and it critically exploits the recent improvements
from Koutis, Miller and Peng \cite{Koutis2010}. This is to our knowledge
the first algorithm that presents a nearly linear complexity which
depends neither on the condition number of $A$ (except through a
log-term) nor on a specific pattern for the non-zero coefficients
of $A$.

The high complexity of the algorithm transpires through the large
exponent of $\log n$. However, our algorithm will directly benefit
from any improvement on ultra-sparsifiers. Given the considerable
practical importance of such preconditioners, we expect some fast
improvements in this area. Also, the bulk of the work is performed
in a Monte Carlo procedure that is straightforward to parallelize.
Furthermore, we also present simpler, non-optimal algorithms that
compute upper and lower bounds of the logarithm of the determinant,
and that may be of more immediate practical interest.

\textbf{Background} There are two approaches in numerical linear algebra
to compute approximately a determinant (or the log of the determinant):
by performing a (partial) Cholesky factorization of $A$, or by considering
the trace of some power series.

As mentioned above, the Cholesky factorization performs a decomposition
of the form: $A=PLDL^{T}P^{T}$ with $P$ a permutation matrix, $L$
a low-triangular matrix with $1$ on the diagonal and $D$ a diagonal
matrix of non-negative coefficients. Then the log-determinant of $A$
is simply%
\footnote{We will use the $\left|\cdot\right|$ operator to denote the determinant,
it will be clear from the context that it is different from the absolute
value.%
}:
\[
\log\left|A\right|=\sum_{i}\log D_{ii}
\]
The complexity of dense Cholesky factorization for dense matrices
is $O\left(n^{3}\right)$. Unfortunately, Cholesky factorization usually
does not gain much from the knowledge of the sparsity pattern due
to the \emph{fill-in problem} (see \cite{meurant1999computer}, section
3.2). There is one case, though, for which Cholesky factorization
is efficient: if the sparsity pattern of $A$ is a tree, then performing
Cholesky factorization takes $O\left(n\right)$ time, and the matrix
$L$ is a banded matrix \cite{liu1990eliminationtrees}. If the sparsity
pattern of $A$ is not a tree, however, this advantageous decomposition
does not hold anymore.

When the matrix $A$ is close to the identity, more precisely when
the spectral radius of $M=A-I$ is less than~$1$, one can use the
remarkable Martin expansion of the log-determinant \cite{martin1992approximations}:
\begin{equation}
\log\left|A\right|=\text{Tr}\left(\log A\right)\label{eq:martin-expansion}
\end{equation}
 where $\log A$ is the matrix logarithm defined by the series expansion:
\begin{equation}
\log A=\sum_{i=0}^{\infty}\frac{\left(-1\right)^{i}}{i+1}M^{i}\label{eq:matrix-log}
\end{equation}
The determinant can then be computed by a sum of traces of the power
of $M$, and the rate of convergence of this series is driven by the
spectral radius $M$. This line of reasoning has lead to looking for
decompositions of $A$ of the form $A=U+V$ with the determinant of
$U$ being easier to compute and $U^{-1}V+I$ having a small spectral
radius. Then $\log\left|A\right|=\log\left|U\right|+\log\left|U^{-1}V+I\right|$.
The most common decomposition $U,V$ is in terms of block diagonal
and off-diagonal terms, which can then use Hadamard inequalities on
the determinant to bound the error \cite{Ipsen2006}. Diagonal blocks
also have the advantage of having determinants easy to compute. However,
this approach requires some strong assumptions on the condition number
of $A$, which may not hold in practice.

The trace approach is driven by \emph{spectral properties }(the condition
number) while the Cholesky approach is driven by \emph{graphical }properties\emph{
}(the non-zero pattern)\emph{. }We\emph{ }propose to combine these
two approaches by decomposing the problem with one component that
is close to a tree (and is more amenable to Cholesky methods), and
one component that has a bounded condition number. Our solution is
to use a \emph{spectral sparsifier} introduced by Spielman in \cite{Spielman2008}.

The problem of estimating determinants has important applications
in spatial data analysis, statistical physics and statistics. For
example, a number of quantum system properties can be inferred by
computing determinants of large graphs (determinant quantum Monte
Carlo). This is why computing estimates of the log-determinant has
been an active problem in physics and statistics. In particular, the
Martin expansion presented in Equation \ref{eq:martin-expansion}
can be combined with sampling method to estimate the trace of a matrix
series (\cite{Zhang2008},\cite{McCourt2008},\cite{Zhang2007}).
Another different line of research has worked on bounds on the values
of the determinant itself. This is deeply connected to simplifying
statistical models using variational methods. Such a relaxation using
a message-passing technique is presented in \cite{Wainwright2006}.
Our method is close in spirit to Reuksen's work \cite{Reusken2002}
by the use of a preconditioner. However, Reuksen considers preconditioners
based on a clever approximation of the Cholesky decomposition, and
its interaction with the eigenvalues of the complete matrix. Using
simpler methods based on sampling, we are able to carefully control
the spectrum of the reminder, which in turn leads to strong convergence
guarantees. 

The rest of the article is structured as follows. In the next section,
we present some results about estimating the log-determinant from
a truncated expansion. These results will justify the use of \emph{preconditioners
}to compute the determinant of a matrix. The techniques developed
by Spielman et al. work on the Laplacians of weighted graphs. Section
3 introduces some new concepts to expand the notion of determinants
to Laplacian matrices, and presents a few straightforward results
in the relations between graph Laplacians and SDD matrices. Section
4 will use these new concepts to introduce a first family of preconditioners
based on low-stretch spanning trees. Finally, Section 5 contains the
proof of our main result, an algorithm to compute determinants in
near-linear time.

\begin{algorithm}
\begin{raggedright}
Algorithm \textbf{UltraLogDet}($A$,$\epsilon$):
\par\end{raggedright}

\begin{raggedright}
If $A$ is of a small size (<100), directly compute $\text{ld}\left(A\right)$
with a dense Cholesky factorization.
\par\end{raggedright}

\begin{raggedright}
Compute $B=$\textbf{IncrementalSparsify($A$)}
\par\end{raggedright}

\begin{raggedright}
Compute $D,A'=$\textbf{PartialCholesky($B$)}
\par\end{raggedright}

\begin{raggedright}
Compute $s=$\textbf{PreconditionedLogDetMonteCarlo($B,A,\epsilon$)}
\par\end{raggedright}

\begin{raggedright}
Return $s+\log\left|D\right|+$\textbf{UltraLogDet($A'$,$\epsilon$)}
\par\end{raggedright}

\caption{The main algorithm\label{alg:The-main-algorithm}}
\end{algorithm}


\begin{algorithm}
Algorithm \textbf{PreconditionedLogDetMonteCarlo}($B$,$A$,$\epsilon$,$p$,$l$):

$y\leftarrow0$

for $j$ from $1$ to $p$:

~~Sample $\mathbf{u}\sim\mathcal{N}\left(\mathbf{0},I\right)$

~~$\mathbf{v}\leftarrow\mathbf{u}$

~~$z\leftarrow0$

~~for $k$ from $1$ to $l$:

~~~~$\mathbf{v}\leftarrow B^{-1}A\mathbf{v}$

~~~~$z\leftarrow z+k^{-1}\mathbf{v}^{T}\mathbf{u}$

~~$y\leftarrow y-p^{-1}z$

Return $y$

\caption{\textbf{PreconditionedLogDetMonteCarlo}\label{alg:SampleLogDet}}
\end{algorithm}



\section{Results for log-determinants}

We begin by considering a simple sampling algorithm to compute log-determinants,
presented first in \cite{Barry1999}. We will first present some error
bounds on this algorithm that expand on bounds previously presented
in \cite{Bai1996} and \cite{Barry1999}.

Consider a real symmetric matrix $S\in\mathcal{S}_{n}^{+}$ such that
its spectral radius is less than $1$: $0\preceq S\preceq\left(1-\delta\right)I$
for some $\delta\in\left(0,1\right)$. We want to compute $\log\left|I-S\right|$
up to precision $\epsilon$ and with high probability. From the Martin
expansion:
\begin{equation}
\log\left|I-S\right|=-\mbox{Tr}\left(\sum_{k=1}^{\infty}\frac{1}{k}S^{k}\right)\label{eq:martin}
\end{equation}


This series of traces can be estimated by Monte Carlo sampling, up
to precision $\epsilon$ with high probability. In order to bound
the errors, we will bound the large deviation errors using Hoeffding
inequality. We use a modified version of Proposition 4.2 from \cite{Bai1996}:
\begin{lemma}
\label{lem:hoeffding-trace}Consider $H\in\mathcal{S}_{n}$ lower-
and upper-bounded by $\lambda_{\text{min}}$ and $\lambda_{\text{max}}$:
$\lambda_{\min}I\preceq H\preceq\lambda_{\max}I$. Consider $p$ vectors
sampled from the standard Normal distribution: $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
for $i=1\cdots p$. Then for all $\nu>0$:
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}^{T}-\mbox{Tr}\left(H\right)\right|\geq\frac{\nu}{p}\right]\leq2\exp\left(-\frac{2\nu^{2}}{p\left(\lambda_{\max}-\lambda_{\min}\right)^{2}}\right)
\]
\end{lemma}
\begin{proof}
This is a simple adaptation from \cite{Bai1996}.
\end{proof}
The previous lemma shows that if the eigenspectrum of a matrix is
bounded, we can obtain a Hoeffding bound on the error done by sampling
the trace. Furthermore, the convergence of the series \ref{eq:martin}
is also determned by the extremal eigenvalues of $S$. If we truncate
the series (\ref{eq:martin}), we can bound the truncation error using
the extremal eigenvalues. We formalize this intuition in the following
theorem, which is adapted from the main Theorem in \cite{Barry1999}.
While that main Theorem in \cite{Barry1999} only considered a confidence
interval based on the covariance properties of Gaussian distribution,
we generalize this result to more general Hoeffding bounds.
\begin{theorem}
\label{thm:det-sampling-theorem}Consider $S\in\mathcal{S}_{n}^{+}$
with $0\preceq S\preceq\left(1-\delta\right)I$ for some $\delta\in\left(0,1\right)$.
Call $y$ the quantity to estimate: 
\[
y=\log\left|I-S\right|
\]
and consider $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
for $i=1\cdots p$. Call $\hat{y}_{p,l}$ an estimator of the truncated
series of $l$ elements computed by sampling the trace using $p$
samples:
\[
\hat{y}_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\mathbf{u}_{j}^{T}S^{k}\mathbf{u}_{j}
\]
By choosing $p\geq4\epsilon^{-2}\left(\log\delta\right)^{2}\log n$
and $l\geq2\delta^{-1}\log\left(\frac{n}{\delta\epsilon}\right)$,
the error probability is small:
\[
\mathbb{P}\left[\left|y-\hat{y}_{p,l}\right|\geq\epsilon\right]\leq\frac{1}{n}
\]
\end{theorem}
\begin{proof}
The proof of this theorem follows the proof of the Main Theorem in
\cite{Barry1999} with some slight modifications. Using triangular
inequality:
\[
\left|y-\hat{y}_{p,l}\right|\leq\left|\mathbb{E}\left[\hat{y}_{p,l}\right]-\hat{y}_{p,l}\right|+\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right|
\]


Since $S$ is upper-bounded, we have for all $k\in\mathbb{N}$:
\[
\left|\mbox{Tr}\left(S^{k}\right)\right|\leq n\left(1-\delta\right)^{k}
\]
Using again triangle inequality, we can bound the error with respect
to the expected value: 
\begin{eqnarray*}
\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right| & = & \left|\sum_{i=l+1}^{\infty}\frac{\left(-1\right)^{i}}{i}\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \sum_{i=l+1}^{\infty}\frac{1}{i}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{l+1}\sum_{i=l+1}^{\infty}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{n}{l+1}\sum_{i=l+1}^{\infty}\left(1-\delta\right)^{k}\\
 & \leq & \frac{n}{l+1}\frac{\left(1-\delta\right)^{l+1}}{\delta}\\
 & \leq & n\frac{\left(1-\delta\right)^{l+1}}{\delta}
\end{eqnarray*}


Hence, for a choice of $l\geq\delta^{-1}\left(\log\delta^{-1}+\log\epsilon^{-1}+\log n+\log2\right)$,
the latter part is less than $\epsilon/2$. We now bound the first
part using Lemma \ref{lem:hoeffding-trace}. Call $H$ the truncated
series:
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}
\]
The lowest eigenvalue of the truncated series can be lower-bounded
in terms of $\delta$:
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}\succeq-\sum_{i=1}^{m}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I=\left(\log\delta\right)I
\]
We can now invoke Lemma \ref{lem:hoeffding-trace} to conclude. Indeed,
call $\nu=2^{-1}\epsilon p$, then $\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}^{T}-\mbox{Tr}\left(H\right)\right|\geq\frac{\epsilon}{2}\right]\leq2\exp\left(-\frac{p\epsilon^{2}}{2\left(\log\delta\right)^{2}}\right)$
and $2\exp\left(-\frac{p\epsilon^{2}}{2\left(\log\delta\right)^{2}}\right)\leq n^{-1}$
is equivalent to $-\frac{p\epsilon^{2}}{2\left(\log\delta\right)^{2}}\leq-\log2-\log n\leq-2\log n$,
and this latter inequality is verified for any $p\geq4\epsilon^{-2}\left(\log\delta\right)^{2}\log n$.
\end{proof}
We immediately get the following result that justifies the notion
of preconditioners for determinants. The corresponding algorithm,
which we call \texttt{PreconditionedLogDetMonteCarlo}, is presented
in Algorithm \ref{alg:SampleLogDet}.
\begin{corollary}
\label{cor:preconditioning}If $A$ and $B$ are positive definite
matrices so that $B$ is a $\kappa-$approximation of $A$:
\begin{equation}
A\preceq B\preceq\kappa A\label{eq:A-B-bounds}
\end{equation}
then the algorithm \texttt{PreconditionedLogDetMonteCarlo} computes
an $\epsilon$-approximation of $\log\left|B^{-1}A\right|$ with high
probability, by performing $O\left(\kappa\epsilon^{-2}\left(\log\kappa\right)^{2}\log n\left(\log n+\log\kappa+\log\left(\epsilon^{-1}\right)\right)\right)$
vector inversions from $B$ and vector multiplies from $A$.\end{corollary}
\begin{proof}
We introduce some notations that will prove useful for the rest of
the article:
\[
H=I-B^{-1}A
\]
\[
S=I-B^{-1/2}AB^{-1/2}
\]
with $B^{-1/2}$ the inverse of the square root of the positive-definite
matrix $B$%
\footnote{Given a real PSD matrix $X$, which can be diagonalized: $X=Q\Delta Q^{T}$
with $\Delta$ diagonal, and $\Delta_{ii}\geq0$. Call $Y=Q\sqrt{\Delta}Q^{T}$
the square root of $X$, then $Y^{2}=X$.%
}. The inequality \ref{eq:A-B-bounds} is equivalent to $\kappa^{-1}B\preceq A\preceq B$,
or also: 
\[
\left(1-\kappa^{-1}\right)I\succeq I-B^{-1/2}AB^{-1/2}\succeq0
\]
\[
\left(1-\kappa^{-1}\right)I\succeq S\succeq0
\]


The matrix $S$ is a contraction, and its spectral radius is determined
by $\kappa$. Furthermore, computing the determinant of $B^{-1}A$
is equivalent to computing the determinant of $I-S$: 
\begin{eqnarray*}
\log\left|I-S\right| & = & \log\left|B^{-1/2}AB^{-1/2}\right|\\
 & = & \log\left|A\right|-\log\left|B\right|\\
 & = & \log\left|B^{-1}A\right|\\
 & = & \log\left|I-H\right|
\end{eqnarray*}


and invoking Theorem \ref{thm:det-sampling-theorem} gives us bounds
on the number of calls to matrix-vector multiplies with respect to
$S$. It would seem at this point that computing the inverse square
root of $B$ is required, undermining our effort. However, we can
reorganize the terms in the series expansion to yield only full inverses
of $B$. Indeed, given $l\in\mathbb{N}^{*}$, consider the truncated
series:
\begin{eqnarray*}
y_{l} & = & -\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(H^{i}\right)
\end{eqnarray*}
Hence, the practical computation of the latter sum can be done on
$A^{-1}B$. To conclude, if we compute $p=4\epsilon^{-2}\left(\log\kappa\right)^{2}\log n$
truncated chains of length $l=\kappa\left(2\log n+\log\kappa+\log\left(\epsilon^{-1}\right)\right)$,
we get our result. This requires $lp$ multiplies by $A$ and inversions
by $B$.
\end{proof}
Intuitively, this scheme is useful if $B$ is easy to inverse and
has good spectral properties ($\kappa$ is small). We are going to
present some algorithms for a class of matrices (symmetric, diagonally
dominant matrices) that enjoy such good properties.


\section{Making the problem laplacian\label{sec:Making-the-problem}}

From now on, we consider all matrices to be in $SDD_{n}$. Most techniques
we will be using work on Laplacian matrices instead of SDD matrices.
An SDD matrix is positive semi-definite while a Laplacian matrix is
always singular, since its nullspace is spanned by $\mathbf{1}$.
We generalize the definition of the determinant to handle this technicality.
\begin{definition}
\emph{Pseudo-log-determinant (PLD):} Let $A\in\mathcal{S}^{n+}$ be
a positive semi-definite matrix of which the nullspace is of rank
at most $1$ (i.e., it has at most one zero eigenvalue). The pseudo-log-determinant
is defined by the sum over all the eigenvalues except the last one:
\[
\text{ld}\left(A\right)=\sum_{i=2}^{n}\log\left(\lambda_{i}\right)
\]
where $\lambda_{i}$ are the eigenvalues of $A$ sorted in increasing
order.
\end{definition}
The matrix logarithm does not converge for matrices that are not invertible,
so we need to extend its definition. 
\begin{definition}
\emph{Pseudo-matrix logarithm.} Consider $A\in\mathcal{S}_{n}^{+}$so
that $0\preceq A\prec2$. The pseudo-logarithm of a matrix is defined
by the projection of the matrix logarithm onto the span of $A$:
\[
\log^{+}A=\sum_{k}\frac{1}{k}P\left(I-A\right)^{k}P^{T}
\]
with $P$ an orthogonal projector onto $\text{span}\left(A\right)$.
\end{definition}
The interest of the PLD lies in the connection between SDD matrices
and their Laplacian. Recall that a Laplacian matrix can be constructed
from any SDD matrix by adding a ``ground'' node (see Gremban's thesis
\cite{Gremban1996}, Chapter 4).
\begin{definition}
\emph{Augmented Laplacian of an SDD matrix:} Let $A\in SDD_{n}$.
Let $L,D$ be the decomposition of $A$ into $L$ a Laplacian matrix
and $D$ a (non-negative) diagonal matrix such that $A=L+D$. The
augmented matrix of $A$ is:
\[
L_{A}=\left(\begin{array}{cc}
A & -\mathbf{d}\\
-\mathbf{d}^{T} & h
\end{array}\right)
\]
with $\mathbf{d}\in\mathbb{R}^{n}$: $\mathbf{d}_{i}=D_{ii}$ and
$h=\sum_{i}D_{ii}$.
\end{definition}
From now on, given any $A\in SDD_{n}$, we will implicitly define
a Laplacian $L_{A}$ associated to it, and call the graph of A the
weighted undirected graph characterized by $L_{A}$. The PLD enjoys
some of the usual properties of the log-determinant. The following
proposition shows the interest of the PLD in the study of the Laplacian
of a graph.
\begin{proposition}
\textup{\label{pro:pld-properties}Let $A\in SDD_{n}$, and $L_{A}$
its augmented Laplacian. Let $P=\left(I_{n}\,\mathbf{0}\right)$ be
a projection matrix over the first $n$ columns of $L_{A}$. Then:}\end{proposition}
\begin{itemize}
\item If $\lambda$ is an eigenvalue of $A$ with associated eigenvector
$x$, it is also an eigenvalue of $PL_{A}P^{T}$ with the same eigenvector
\item $\text{ld}\left(L_{A}\right)=\log\left|A\right|$
\item Let $A,B\in SDD_{n}$. $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(L_{A}\right)+\text{ld}\left(L_{B}\right)$
and $\text{ld}\left(L_{B}^{+}L_{A}\right)=\text{ld}\left(L_{A}\right)-\text{ld}\left(L_{B}\right)=-\text{ld}\left(L_{A}^{+}L_{B}\right)$
with $A^{+}$the pseudo inverse of $A$.
\item Let $A\in SDD_{n}$ with $\left\Vert I-A\right\Vert <1$ then $\mbox{ld}\left(L_{A}\right)=-\mbox{Tr}\left(\log^{+}L_{A}\right)$
with $\log^{+}L_{A}$ the pseudo-logarithm of $L_{A}$
\end{itemize}
Note in particular that the PLD is well defined over connected graphs.
From now on, we will consider connected graphs only. If graphs have
several unconnected components, the log-determinant of the corresponding
SDD is the sum of the PLDs of the Laplacians of each component.
\begin{lemma}
The pseudo-log-determinant of a connected graph is well-defined.\end{lemma}
\begin{proof}
A connected graph $G$ has non-zero conductance $\Phi_{G}$ and using
Cheeger's inequality (see Theorem 4.1 in \cite{Spielman2010a}) $\lambda_{2}\geq\Phi_{G}^{2}>0$.
Hence $\lambda_{i}\geq\lambda_{2}>0$ for all $i\geq2$ and $\text{ld}\left(G\right)\in\mathbb{R}$.
\end{proof}
From now on, we will focus on computing the PLD of Laplacians. Note
that the kernel of all Laplacian matrices of connected graphs is precisely
$\mathbb{R}\mathbf{1}_{n}$, so we can use the PLD with pseudo inverses
as if we were using the log determinant with SDD matrices: Consider
a graph $G=\left(V,E,w\right)$, identified with its Laplacian matrix
$L_{G}$. Consider a weighted subgraph $H=\left(V,\tilde{E},\tilde{w}\right)$
of $G$, for which it is easier to compute the determinant, and that
closely approximates $G$ in the spectral sense. From Proposition
\ref{pro:pld-properties}:
\[
\text{ld}\left(G\right)=\text{ld}\left(H\right)+\text{ld}\left(H^{+}G\right)
\]


We will see how to adapt Spielman and Teng's remarkable work on \emph{ultra-sparsifiers}
to produce good preconditioners $H$ for the determinant. In particular,
once a good preconditioners is available for $G$, we present an algorithm
that computes an upper bound and a lower bound of the PLD \emph{for
free}. We will improve this results to produce an algorithm that computes
an $\epsilon$-approximation with high probability.


\section{A first preconditioner\label{sec:A-first-preconditioner}}

We now present a first preconditioner that is not optimal, but that
will motivate our results for stronger preconditioners: a tree that
spans the graph $G$. Every graph has a low-stretch spanning tree,
as discovered by Alon et al. \cite{Alon1995}. The bound of Alon et
al. was then improved by Abraham et al. \cite{Abraham2008}. We restate
their main result.
\begin{lemma}
Consider $G$ a weighted graph. There exists a tree $T$ that is a
subgraph of $G$ so that:
\[
G\preceq T\preceq\kappa G
\]
 with $\kappa=Cm\log n\log\log n\left(\log\log\log n\right)^{3}$
for some constant $C>0$.\end{lemma}
\begin{proof}
This follows directly from \cite{Abraham2008}. $T$ is a subgraph
of $G$ (with the same weights on the edges), so $T\preceq G$. Furthermore,
we have $T\preceq\text{st}_{G}\left(T\right)G$ using a result of
Boman and Hendrickson \cite{Boman2004} cited by Spielman in \cite{Spielman2010}.
\end{proof}
Trees enjoy a lot of convenient properties for Gaussian elimination.
The Cholesky factorization of a tree can be computed in linear time,
and furthermore this factorization has a linear number of non-zero
elements \cite{Spielman2009a}. This factorization can be expressed
as:
\[
T=PLDL^{T}P^{T}
\]
 where $P$ is a permutation matrix, $L$ is a lower-triangular matrix
with the diagonal being all ones, and $D$ a diagonal matrix in which
all the elements but the last one are positive, the last element being
$0$. These well-known facts about trees are presented in \cite{Spielman2009a}.
Once the Cholesky factorization of the tree is performed, the log-determinant
of the original graph is an immediate by-product:
\[
\mbox{ld}\left(T\right)=\sum_{i=1}^{n-1}\log D_{ii}
\]
 Furthermore, computing $T^{+}x$ also takes $O\left(n\right)$ computations
by forward-backward substitution. Applying Corollary \ref{cor:preconditioning}
gives immediately:
\begin{theorem}
\label{thm:PLD-tree}Let $G$ be a graph with $n$ vertices and $m$
edges. Its PLD can be computed up to a precision of $\epsilon$ and
with high probability in time $\tilde{O}\left(mn\left(\log m\right)^{3}\left(\log n\right)^{2}\frac{\log\epsilon^{-1}}{\epsilon^{2}}\right)$.\end{theorem}
\begin{proof}
We assume $G$ is connected, hence $m\geq n$ and $\log\kappa=\tilde{\mathcal{O}}\left(\log m\right)$.
\end{proof}
This bound may be of independent interest since it requires relatively
little machinery to compute, and it is an improvement already for
graphs with small vertex degree ($m=\mathcal{O}\left(n^{1+o\left(1\right)}\right)$)
over the Cholesky factorization of $G$.

How good is the estimate provided by the tree $T$? Intuitively, this
depends on how well the tree $T$ approximates the graph $G$. This
notion of quality of approximation can be formalized by the notion
of \emph{stretch}. 
\begin{definition}
Stretch of a graph. Consider $G$ a weighted graph, and $T$ a spanning
tree of $G$. The stretch of each edge $e$ from $G$ is defined as
$\text{st}_{T}\left(e\right)=\omega_{e}\left(\sum_{f\in P}\frac{1}{\omega_{f}}\right)$
where $\omega_{e}$ is the weight of edge $e$ in $G$, and $P$ is
the unique path between the endpoints of $e$ in the tree $T$. The
stretch of the graph is the sum of the stretches of each edge: 
\[
\text{st}_{T}\left(G\right)=\sum_{e\in G}\text{st}_{T}\left(e\right)
\]

\end{definition}
The stretch can be obtained as a by-product of the computation of
low-stretch spanning trees, of which the construction can be done
in $O\left(m\log n+n\log^{2}n\right)$ \cite{Abraham2008}. Knowing
the stretch gives upper and lower bounds on the value of the log-determinant:

\begin{equation}
\text{ld}\left(T\right)+\left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)\geq\text{ld}\left(G\right)\geq\text{ld}\left(T\right)+\log\left(\text{st}_{T}\left(G\right)-n+2\right)\label{eq:encadrement}
\end{equation}

\begin{proof}
This is an application of Jensen's inequality on $\text{ld}\left(T^{+}G\right)$.
We have $\text{ld}G=\text{ld}T+\text{ld}\left(T^{+}G\right)$ and
$\text{ld}\left(T^{+}G\right)=\text{ld}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)=\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right)$
with $\sqrt{T}$ the matrix square root of $T$. From Lemma \ref{lem:Jensen-inequality-matrix-logarithm},
we have 
\begin{eqnarray*}
\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right) & \leq & \left(n-1\right)\log\left(\frac{\text{Tr}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{Tr}\left(T^{+}G\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)
\end{eqnarray*}


The lower bound is slightly more involved. Call $\lambda_{i}$ the
positive eigenvalues of $\sqrt{T^{+}}G\sqrt{T^{+}}$ and $\sigma=\text{st}_{T}\left(G\right)$.
We have $1\leq\lambda_{i}\leq\sigma$ because of the inequality$T\preceq G\preceq\text{st}_{T}\left(G\right)T$.
There are precisely $n-1$ positive eigenvalues $\lambda_{i}$, and
we have: $\text{ld}\left(T^{+}G\right)=\sum_{i}\log\lambda_{i}$.
One can show that $\sum_{i}\log\lambda_{i}\geq\log\left(\sigma-n+2\right)$
by considering the problem of minimizing $\sum_{i}\log x_{i}$ under
the constraints $\sum_{i}x_{i}=\sigma$ and $x_{i}\geq1$. This bound
is tight.
\end{proof}
Intuitively since the stretch of a tree is bounded my $O\left(m\log n\right)$,
it means that a low-stretch spanning tree provides an approximation
of a log-determinant up to a factor of $O\left(n\log n\right)$. This
could be of interest for nearly-cut graphs, with large condition numbers.


\section{Incremental sparsifiers\label{sec:Incremental-sparsifiers}}

We can do better and achieve near-linear time by using ultra-sparsifiers.
The main insight of our result is that the class preconditioners presented
by Spielman and Teng are based on incomplete Cholesky factorization,
and hence have a determinant that is relatively easy to compute, and
that furthermore they are excellent spectral preconditioners, so the
procedure \texttt{PreconditionedLogDetMonteCarlo} is efficient to
apply. We reintroduce some concepts presented in \cite{Koutis2010}
to present a self-contained result.

The central idea is to sample $O\left(n\right)$ edges from the graph
$A$, to form a subgraph $B$ that is close to a tree (hence it is
easy to compute some partial Cholesky factorization), yet it is close
to the original $A$ is the spectral sense ($A\preceq B\preceq\kappa A$),
thanks to the additional edges. The partial Cholesky factorization
is computed using the \texttt{GreedyElimination} algorithm presented
in \cite{Koutis2010}. This section is based on Section 4 of \cite{Spielman2009a}. 

Consider a Laplacian matrix $G$. There exists an algorithm that computes
the partial Cholesky factorization:
\[
B=PLCL^{T}P^{T}
\]
 where:
\begin{itemize}
\item $P$ is a permutation matrix
\item $L$ is a non-singular, low triangular matrix of the form
\[
L=\left(\begin{array}{cc}
L_{1,1} & 0\\
L_{2,1} & I_{n_{1}}
\end{array}\right)
\]
with the diagonal of $L_{1,1}$ being all ones.
\item $C$ has the form 
\[
C=\left(\begin{array}{cc}
D_{n-n_{1}} & 0\\
0 & A_{1}
\end{array}\right)
\]
and every row and column of $A_{1}$ has at least 3 non-zero coefficients.
Furthermore, $A_{1}$ is itself Laplacian and:
\[
\text{ld}\left(B\right)=\sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)
\]

\end{itemize}
Thus, the PLD of the original Laplacian $A$ is:
\begin{eqnarray*}
\text{ld}\left(A\right) & = & \text{ld}\left(B\right)+\text{ld}\left(B^{+}A\right)\\
 & = & \sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)+\text{ld}\left(B^{+}A\right)
\end{eqnarray*}
We are left with solving a smaller problem $A_{1}$, and approximate
the value of $\text{ld}\left(B^{+}A\right)$ using the algorithm \texttt{SampleLogDet}.
ST preconditioners are appealing for this task: they guarantee that
$A_{1}$ is substantially smaller than $A$, so the recursion completes
in $O\left(\log\log n\right)$ steps. Furthermore, computing the vector
product $B^{+}Ax$ is itself efficient (in can be done in near-linear
time), so the sampling algorithm can be run is reasonable time. We
formalize the notion of chain of preconditioners by reintroducing
some material from \cite{Koutis2010}.
\begin{definition}
Definition 7.1 from \cite{Koutis2010}. $\kappa\left(n\right)$-good
chain. $\mathcal{C}=\left\{ A_{1}=A,B_{1},A_{2},\dots A_{d}\right\} $
a chain of graphs with $n_{i}$ and $m_{i}$ the number of vertices
and edges of $A_{i}$. $\mathcal{C}$ is $\kappa\left(n\right)$-good
for $A$ if:\end{definition}
\begin{enumerate}
\item $A_{i}\preceq B_{i}\preceq\kappa\left(n_{i}\right)A_{i}$
\item $A_{i+1}$= GreedyElimination$\left(B_{i}\right)$
\item $m_{i}/m_{i+1}\geq c_{r}\sqrt{\kappa\left(n_{i}\right)}$ for some
constant $c_{r}$.
\end{enumerate}
Good chains exist (Lemma 7.3 from \cite{Koutis2010}):
\begin{lemma}
\label{lem:good-chain}Given a graph $A$, \texttt{BuildChain}$\left(A,p\right)$
from \cite{Koutis2010} produces a $\tilde{O}\left(\log^{4}n\right)$-good
chain for $A$ with probability $1-p$. The algorithm runs in time
\[
\tilde{O}\left(\left(m\log n+n\log^{2}n\right)\log\left(1/p\right)\right)
\]

\end{lemma}
These chains furthermore can be used a good preconditioners for conjugate
gradient and lead to near-linear algorithms for approximate inversion
(Lemma 7.2 from \cite{Koutis2010})
\begin{lemma}
Given a $\kappa\left(n\right)$-good chain for $A$, a vector $x$
such that $\left\Vert x-L_{A}^{+}b\right\Vert _{A}<\epsilon\left\Vert L_{A}^{+}b\right\Vert _{A}$
can be computed in $O\left(m_{d}^{3}m_{1}\sqrt{\kappa\left(n_{1}\right)}\log\left(1/\epsilon\right)\right)$
\end{lemma}
It should now become clear how we can combine a $\kappa\left(n\right)$-good
chain with the Algorithm \texttt{PreconditionedLogDetMonteCarlo}.
We start by building a chain. The partial Cholesky factorizations
provide an upper bound to $\mbox{ld}\left(A\right)$. We then refine
this upper bound by running \texttt{PreconditionedLogDetMonteCarlo}
at each state of the chain to approximate $\mbox{ld}\left(B_{i}^{+}A_{i}\right)$
with high probability. The complete algorithm is presented in Algorithm
\ref{alg:The-main-algorithm}.

We have three sources of errors introduced in our estimate:
\begin{itemize}
\item The truncation of the series of $\log\left(B^{+}A\right)$
\item The sampling error induced by having a finite number of samples when
approximating the trace of $\log\left(B^{+}A\right)$
\item The approximate inversion by $B$
\end{itemize}
The following lemmas show that we can reasonably control the inversion
error by running the ST-solver at a higher precision $O\left(\kappa\left(n_{1}\right)^{-3}\kappa\left(A_{1}\right)^{-1}\epsilon\right)$.
In particular, the dependency on the condition number of $A_{1}$
stems from the fact that the convergence guarantees offered by the
ST-solver are relative to the matrix norm $\left\Vert \right\Vert _{A}$.
The ST-solvers present some convergence guarantees using the matrix
norm induced by $A$ instead of the regular Euclidian norm. The following
lemmas deal with this technicality. 


\subsubsection*{Proofs of convergence}

We need to prove that the error introduced by approximating the solution
of the preconditioner does not have too great an impact on the accuracy
of the solution. We use the same notations as above with $B$ a $\kappa-$approximation
of $A$, and we call:

\[
A\preceq B\preceq\kappa A
\]
\[
S=I-B^{-1/2}AB^{-1/2}
\]


\[
R=I-B^{-1}A
\]


\[
\varphi=\kappa^{-1}
\]

\begin{lemma}
The matrix norm of the elements is bounded and less than $1$:
\begin{eqnarray*}
\left\Vert S\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert _{B} & \leq & \left(1-\varphi\right)^{2}
\end{eqnarray*}
\end{lemma}
\begin{proof}
Recall the definition of the matrix norm:$\left\Vert S\right\Vert =\max_{x^{T}x\leq1}\sqrt{x^{T}Sx}$.
Since we know that $S\preceq\left(1-\varphi\right)I$, we get the
first inequality. 

The second inequality is a consequence of Proposition 3.3 from \cite{Spielman2009a}:
$A$ and $B$ have the same nullspace and we have the LMI $A\preceq B\preceq\kappa A$,
which implies that the eigenvalues of $B^{-1}A$ lie between $\kappa^{-1}=\varphi$
and $1$. This implies that the eigenvalues of $I-B^{-1}A$ are between
$1-\varphi$ and $0$.

Recall the definition of the matrix norm induced by the $B$-norm
over $\mathbb{R}^{n}$:
\begin{eqnarray*}
\left\Vert R\right\Vert _{B} & = & \max_{x\neq0}\frac{\left\Vert Rx\right\Vert _{B}}{\left\Vert x\right\Vert _{B}}\\
 & = & \max_{\left\Vert x\right\Vert _{B}^{2}\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{x^{T}Bx\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{y^{T}y\leq1}\sqrt{y^{T}B^{-1/2}R^{T}BRB^{-1/2}y}
\end{eqnarray*}
and the latter expression simplifies: 
\begin{eqnarray*}
B^{-1/2}R^{T}BRB^{-1/2} & = & B^{-1/2}\left(I-AB^{-1}\right)B\left(I-B^{-1}A\right)B^{-1/2}\\
 & = & \left(I-B^{-1/2}AB^{-1/2}\right)\left(I-B^{-1/2}AB^{-1/2}\right)\\
 & = & S^{2}
\end{eqnarray*}
so we get:
\[
\left\Vert R\right\Vert _{B}=\left\Vert S^{2}\right\Vert \leq\left\Vert S\right\Vert ^{2}\leq\left(1-\varphi\right)^{2}
\]

\end{proof}
The approximation of the log-determinant is performed by computing
sequences $\left(R^{k}x\right)_{k}$. These chains are computed approximately
by repeated applications of the $R$ operator on the previous element
of the chain, starting from a Gaussian white noise element $x_{0}\sim\mathcal{N}\left(0,\mathbf{I}\right)$.
We formalize the notion of an approximate chain. 
\begin{definition}
\emph{Approximate power sequence. }Given a linear operator $R$, a
start point $x^{\left(0\right)}\in\mathbb{R}^{n}$, and a positive-definite
matrix $B$, we define an $\epsilon-$approximate power sequence as
a sequence that does not deviate too much from the power sequence:
\[
\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}
\]


We now prove the following result that is quite intuitive: if the
operator $R$ is a contraction and if the relative error $\epsilon$
is not too great, the sum of all the errors on the chain is bounded.\end{definition}
\begin{lemma}
Given the previous hypothesis, and assuming furthermore that $\left\Vert R\right\Vert _{B}\leq1-\eta$
and that $2\epsilon\leq\eta\leq1/2$, the total error is bounded by
$\mathcal{O}\left(\eta^{-3}\epsilon\right)$:
\[
\sum_{k=0}^{\infty}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]
\end{lemma}
\begin{proof}
Call $\omega_{k}=\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}$
and $\theta_{k}=\left\Vert Rx^{\left(k\right)}\right\Vert _{B}$.
We are going to bound the rate of convergence of these two series.
We have first using triangular inequality on the $B$ norm and then
the definition of the induced matrix norm.
\begin{eqnarray*}
\theta_{k} & \leq & \left\Vert Rx^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & = & \omega_{k}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}
We now bound the error on the $\omega_{k}$ sequence:
\begin{eqnarray*}
\omega_{k+1} & = & \left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}+Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \left\Vert Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}+\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\\
 & \leq & \left\Vert R\right\Vert _{B}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}\\
 & = & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\theta_{k}\\
 & \leq & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\left(\omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}\right)\\
 & \leq & \left[\left(1-\eta\right)^{2}+\epsilon\right]\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}


Note that the condition $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$
is equivalent to $\epsilon\leq\eta-\eta^{2}$. We can assume without
loss of generality that $0\leq\eta\leq1/2$. Under this condition,
$\eta/2\leq\eta-\eta^{2}$. So the condition $2\epsilon\leq\eta$
implies $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$ which leads
to:
\[
\omega_{k+1}\leq\left(1-\eta\right)\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]


By induction, one obtains:
\[
\omega_{k}\leq\omega_{1}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]
and since $\omega_{1}=\left\Vert x^{\left(1\right)}-Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert R\right\Vert _{B}\left\Vert x^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}$,
we have a final bound that depends on $\epsilon$:
\[
\omega_{k}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]


This is the sum of two geometric series, which give the bound:
\[
\sum_{k}\omega_{k}\leq\epsilon\eta^{-1}\left[1+\frac{1}{\eta^{2}\left(1-\eta\right)^{2}}\right]\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]


Since $\eta\leq1/2$, it implies $\left(1-\eta\right)^{-2}\leq4$
and $1\leq\eta^{-2}$, so we can further simplify:
\[
\sum_{k}\omega_{k}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]

\end{proof}
We can use the bound on the norm of $A$ to compute bound the error
with a preconditioner:
\begin{proposition}
\label{prop:estimate-truncated-series}Given a $\kappa\left(n\right)$-good
chain for $A$, a start vector $x$, one can compute an $\epsilon-$estimate
of the truncated series: 
\[
\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B_{1}^{-1}A\right)^{i}xx^{T}\right)
\]
in time $\mathcal{O}\left(lm\sqrt{\kappa\left(n_{1}\right)}\left(\log\epsilon^{-1}+\log\kappa\left(n_{1}\right)+\log\kappa\left(B\right)\right)\right)$.\end{proposition}
\begin{proof}
Consider an $\epsilon-$approximate power sequence $\left(x^{\left(k\right)}\right)_{k}$
with respect to the operator $I-B^{-1}A$, and $\hat{z}$ the truncated
sequence: 
\[
\hat{z}=\sum_{k=1}^{l}\frac{1}{k}\left(x^{\left(0\right)}\right)^{T}x^{\left(k\right)}
\]
This sequence is an approximation of the exact sequence $z$:
\[
z=\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B^{-1}A\right)^{i}x^{\left(0\right)}\left(x^{\left(0\right)}\right)^{T}\right)
\]
We now bound the error between the two sequences:
\[
\left|\hat{z}-z\right|\leq\sum_{k=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|
\]
Using the Cauchy-Schwartz inequality, we obtain:
\[
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{k}x_{0}-x_{k}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}
\]
so that we can bound the deviation:
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{k=1}^{l}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}\leq4\epsilon\kappa^{3}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$. Now,
we can call the ST-solver routine with $\tilde{\epsilon}=\kappa^{-3}\kappa\left(B\right)^{-1}\epsilon$,
the cost of each call to the operator $R$ being then $\mathcal{O}\left(m\sqrt{\kappa}\left(\log\epsilon^{-1}+\log\kappa+\log\kappa\left(B\right)\right)\right)$. 
\end{proof}
We now have all the elements required to prove our main theorem. Now
we can use the previous proposition to bound the error done during
the estimation part. We have already found some bounds for the estimates
of the log-determinant of the preconditioners, and some probability
estimates for the error bounds on the trace series.
\begin{theorem}
\label{thm:main}Consider $A$ a symmetric, diagonally dominant matrix
of size $n\times n$ with $m$ non-zero entries, and $0<\epsilon<\log^{-5}n$.
One can compute $y\in\mathbb{R}$ so that $\mathbb{P}\left(\left|y-\log\left|A\right|\right|>\epsilon\right)\leq\frac{1}{n}$
in expected time $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right)$. \end{theorem}
\begin{proof}
Consider a $\tilde{O}\left(\log^{4}n\right)$-chain \ref{lem:good-chain}
for the Laplacian of $A$. The cost of constructing such a chain is
negligible against the sampling step. From Corollary \ref{cor:preconditioning},
for each level of the chain, we compute $p=\tilde{O}\left(\epsilon^{-2}\log n\right)$
approximate truncated chains of length $l=\tilde{O}\left(\log^{4}n\left(\log n+\log\left(\epsilon^{-1}\right)\right)\right)$.
From Proposition \ref{prop:estimate-truncated-series}, the cost of
running each chain is $\tilde{O}\left(lm\log^{2}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right)$.
Thus the time to approximate the residue PLD at level $i$ is bounded
by $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right).$
Finally, since $m_{i}$ decreases faster than geometrically, the number
$d$ of steps in the chain is $\tilde{O}\left(1\right)$.
\end{proof}

\section*{Comments}

A first approximation upper-bound of the log-determinant follows immediately
from the computation of the $\kappa-$good chain. We presented in
Section \ref{sec:A-first-preconditioner} a first analysis to bound
the value of the residue PLD. This analysis was done using trees as
preconditioners, it can be carried on on more general preconditioners
by introducing a generalization of the stretch over a subgraph (see
the Appendix, section \ref{sub:Stretch-graph}). Since the bulk of
the computations are performed in estimating the residue PLD, it would
be interesting to see if this could be bypassed using better bounds
based on the stretch.

When looking at each step of the analysis, one can see that the $\epsilon^{-2}$
factor comes from the approximation of the trace of a matrix by sampling.
This seems to be a fundamental limitation of this method and it is
shared with other algorithms that rely on random projections. This
result is absolute. It would be interesting to see if the analysis
could be tightened to present a relative bound that does not depend
on the condition number of $A$. Also, even if this algorithm presents
a linear bound, it requires a fairly advanced machinery (ST solvers)
that may limit its practicality. Some heuristic implementation, for
example based on algebraic multi-grid methods, could be a first step
in this direction.

The authors are much indebted to Satish Rao and Jim Demmel for suggesting
the original idea and their helpful comments on the draft of this
article.


\section*{Appendix}
\begin{lemma}
\label{lem:Jensen-inequality-matrix-logarithm}Jensen inequality for
the matrix logarithm. Given $A\in\mathcal{S}_{n}^{+},$ $0\prec A\prec2$,
the following inequalities hold:
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)\geq\frac{1}{n}\text{Tr}\left(\log A\right)
\]
\[
\text{ld}\left(\frac{\text{Tr}\left(A\right)}{n-1}\right)\geq\frac{1}{n-1}\text{Tr}\left(\log A\right)
\]
with $\log A=\sum_{k\geq1}\frac{1}{k}\left(I-A\right)^{k}$\end{lemma}
\begin{proof}
Consider the diagonalization of $A$: $A=P\Delta P^{T}$ with $\Delta$
a diagonal (positive) matrix, and $P$ an orthogonal matrix. Then
\[
\log A=\sum_{k\geq1}\frac{1}{k}\left(PP^{T}-P\Delta P^{T}\right)^{k}=\sum_{k\geq1}\frac{1}{k}\left[P\left(I-\Delta\right)P^{T}\right]^{k}=P\left[\sum_{k\geq1}\frac{1}{k}\left(I-\Delta\right)^{k}\right]P^{T}=P\Gamma P^{T}
\]
 with $\Gamma$ a diagonal matrix that verifies $\Gamma_{ii}=\log\Delta_{ii}$.
We can then conclude using the concavity of the logarithm over the
reals: 
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)=\log\left(\frac{\sum_{i}\Delta_{ii}}{n}\right)\geq\frac{1}{n}\sum_{i}\log\Delta_{ii}=\frac{1}{n}\text{Tr}\left(\Gamma\right)=\frac{1}{n}\text{Tr}\left(\log A\right)
\]


The same reasoning holds for the pseudo-log-determinant while considering
all but one eigenvalue\end{proof}
\begin{lemma}
Power relation for the matrix log: Given $A\in\mathcal{S}_{n}^{+},$
$0\prec A\prec2$, and $k\in\mathbb{N}$, then $\log\left(A^{k}\right)=k\log A$
\end{lemma}

\subsection{Stretch of a graph\label{sub:Stretch-graph}}

We introduce here a refinement on an upper bound that is a byproduct
of using the algorithm XXX.
\begin{definition}
We define the \textbf{stretch of a graph} $G=\left(V,E,\omega\right)$
with respect to a subgraph $H=\left(V,\tilde{E},\tilde{\omega}\right)\subset G$
by the weighted sum of effective resistances of edges $e\in E$ with
respect to the graph $H$:
\[
\text{st}_{H}\left(G\right)=\sum_{e\in E}\omega_{e}\text{eff}_{H}\left(e\right)
\]

\end{definition}
This is a generalization of the notion of stretch defined by Alon,
Karp, Peleg and West in .... We can use this definition to generalize
theorem 2.1 in \cite{Spielman2009b}
\begin{theorem}
Let $G=\left(V,E,\omega\right)$ be a connected graph and let $H=\left(V,\tilde{E},\tilde{\omega}\right)$
be a connected subgraph of $G$. Let $L_{G}$ and $L_{H}$ be the
Laplacian matrices of $G$ and $H$ respectively. Then:
\[
\text{Tr}\left(L_{H}^{+}L_{G}\right)=\text{st}_{H}\left(G\right)
\]
where $L_{H}^{+}$ is the pseudo inverse of $L_{H}$.\end{theorem}
\begin{proof}
The proof is nearly identical to that of \cite{Spielman2009b}, except
for the last line:

\begin{eqnarray*}
\text{Tr}\left(L_{H}^{+}L_{G}\right) & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(L_{\left(u,v\right)}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(\left(\psi_{u}-\psi_{v}\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\left(\psi_{u}-\psi_{v}\right)
\end{eqnarray*}
and the latter term is the effective resistance between $u$ and $v$
in the graph $H$:
\begin{eqnarray*}
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{eff}_{H}\left(u,v\right)\\
 & = & \text{st}_{H}\left(G\right)
\end{eqnarray*}

\end{proof}
From a practical perspective, the graph stretch can be computed in
$\tilde{O}\left(m\log n/\epsilon^{2}\right)$
\begin{proposition}
There exists an algorithm that computes \textup{an $\epsilon$-approximation
of $\text{st}_{H}\left(G\right)$ in} $\tilde{O}\left(m\log n/\epsilon^{2}\right)$
\end{proposition}


\bibliographystyle{plain}
\bibliography{2012_sparse_info,siam}

\end{document}
