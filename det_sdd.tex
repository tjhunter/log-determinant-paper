%% LyX 2.0.5.1 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside]{article}
% \usepackage[T1]{fontenc}
% \usepackage[latin9]{inputenc}
% \usepackage{float}
% \usepackage{amstext}
% \usepackage{amssymb}


\usepackage{jmlr2e}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2000}{1-48}{4/00}{10/00}{Marina Meil\u{a} and Michael I. Jordan}

% Short headings should be running head and authors last names

\ShortHeadings{Computing the log-determinant of symmetric, diagonally dominant matrices
in near-linear time}{Hunter, {El Alaoui} and Bayen}
\firstpageno{1}


\usepackage[]{algorithm2e}
% \usepackage{float}
\usepackage{amstext}
\usepackage{amssymb}

% \makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
% \floatstyle{ruled}
% \newfloat{algorithm}{tbp}{loa}
% \providecommand{\algorithmname}{Algorithm}
% \floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% \newcommand\eqref[1]{(\ref{#1})}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\include{amsmath}

% \makeatother

% \usepackage{babel}
%   \providecommand{\corollaryname}{Corollary}
%   \providecommand{\definitionname}{Definition}
%   \providecommand{\lemmaname}{Lemma}
%   \providecommand{\propositionname}{Proposition}
% \providecommand{\theoremname}{Theorem}

\begin{document}

\title{Computing the log-determinant of symmetric, diagonally dominant matrices
in near-linear time\\
WORKING DRAFT}

\author{\name Timothy Hunter \email tjhunter@eecs.berkeley.edu \\
       \name Ahmed {El Alaoui} \email elalaoui@eecs.berkeley.edu \\
       \name Alexandre M. Bayen \email bayen@berkeley.edu \\
       \addr Department of Electrical Engineering and Computer Sciences\\
       University of California\\
       Berkeley, CA 94720-1776, USA}


% \author{Timothy Hunter, Alexandre Bayen}

\editor{Editor}

\maketitle

\input{detsdd_1introduction}
\input{detsdd_2preconditionedlogdet}
\input{detsdd_3laplacian_problem}

\section{A first preconditioner\label{sec:A-first-preconditioner}}

While the results in this section are not the main claims of this paper, we
hope they will provide some intuition.

We now present a first preconditioner that is not optimal, but that
will motivate our results for stronger preconditioners: a tree that
spans the graph $G$. Every graph has a low-stretch spanning tree,
as discovered by Alon et al. \cite{Alon1995}. The bound of Alon et
al. was then improved by Abraham et al. \cite{Abraham2008}. We restate
their main result.
\begin{lemma}
Consider $G$ a weighted graph. There exists a tree $T$ that is a
subgraph of $G$ so that:
\[
G\preceq T\preceq\kappa G
\]
 with $\kappa=Cm\log n\log\log n\left(\log\log\log n\right)^{3}$
for some constant $C>0$.\end{lemma}
\begin{proof}
This follows directly from \cite{Abraham2008}. $T$ is a subgraph
of $G$ (with the same weights on the edges), so $T\preceq G$. Furthermore,
we have $T\preceq\text{st}_{G}\left(T\right)G$ using a result of
Boman and Hendrickson \cite{Boman2004} cited by Spielman in \cite{Spielman2010}.
\end{proof}
Trees enjoy a lot of convenient properties for Gaussian elimination.
The Cholesky factorization of a tree can be computed in linear time,
and furthermore this factorization has a linear number of non-zero
elements \cite{Spielman2009a}. This factorization can be expressed
as:
\[
L_{T}=PLDL^{T}P^{T}
\]
 where $P$ is a permutation matrix, $L$ is a lower-triangular matrix
with the diagonal being all ones, and $D$ a diagonal matrix in which
all the elements but the last one are positive, the last element being
$0$. These well-known facts about trees are presented in \cite{Spielman2009a}.
Once the Cholesky factorization of the tree is performed, the log-determinant
of the original graph is an immediate by-product:
\[
\log\left|T\right|=\sum_{i=1}^{n-1}\log D_{ii}
\]
 Furthermore, computing $L_{T}^{+}x$ also takes $O\left(n\right)$
computations by forward-backward substitution. Applying Corollary
\ref{cor:preconditioning} gives immediately:
\begin{theorem}
\label{thm:PLD-tree}Let $G$ be a graph with $n$ vertices and $m$
edges. Its PLD can be computed up to a precision of $\epsilon$ and
with high probability in time $\tilde{O}\left(mn\left(\log m\right)^{3}\left(\log n\right)^{2}\frac{\log\epsilon^{-1}}{\epsilon^{2}}\right)$.\end{theorem}
\begin{proof}
We assume $G$ is connected, hence $m\geq n$ and $\log\kappa=\tilde{\mathcal{O}}\left(\log m\right)$.
\end{proof}
This bound may be of independent interest since it requires relatively
little machinery to compute, and it is an improvement already for
graphs with small vertex degree ($m=\mathcal{O}\left(n^{1+o\left(1\right)}\right)$)
over the Cholesky factorization of $G$.

How good is the estimate provided by the tree $T$? Intuitively, this
depends on how well the tree $T$ approximates the graph $G$. This
notion of quality of approximation can be formalized by the notion
of \emph{stretch}. 
\begin{definition}
Stretch of a graph. Consider $G$ a weighted graph, and $T$ a spanning
tree of $G$. The stretch of each edge $e$ from $G$ is defined as
$\text{st}_{T}\left(e\right)=\omega_{e}\left(\sum_{f\in P}\frac{1}{\omega_{f}}\right)$
where $\omega_{e}$ is the weight of edge $e$ in $G$, and $P$ is
the unique path between the endpoints of $e$ in the tree $T$. The
stretch of the graph is the sum of the stretches of each edge: 
\[
\text{st}_{T}\left(G\right)=\sum_{e\in G}\text{st}_{T}\left(e\right)
\]

\end{definition}
The stretch can be obtained as a by-product of the computation of
low-stretch spanning trees, of which the construction can be done
in $O\left(m\log n+n\log^{2}n\right)$ \cite{Abraham2008}. Knowing
the stretch gives upper and lower bounds on the value of the log-determinant.
\begin{proposition}
Stretch bounds on the tree PLD:
\end{proposition}
\begin{equation}
\log\left|T\right|+\left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)\geq\log\left|G\right|\geq\log\left|T\right|+\log\left(\text{st}_{T}\left(G\right)-n+2\right)\label{eq:encadrement}
\end{equation}

\begin{proof}
This is an application of Jensen's inequality on $\text{ld}\left(T^{+}G\right)$.
We have $\text{ld}G=\text{ld}T+\text{ld}\left(T^{+}G\right)$ and
$\text{ld}\left(T^{+}G\right)=\text{ld}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)=\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right)$
with $\sqrt{T}$ the matrix square root of $T$. From Lemma \ref{lem:Jensen-inequality-matrix-logarithm},
we have 
\begin{eqnarray*}
\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right) & \leq & \left(n-1\right)\log\left(\frac{\text{Tr}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{Tr}\left(T^{+}G\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)
\end{eqnarray*}


The lower bound is slightly more involved. Call $\lambda_{i}$ the
positive eigenvalues of $\sqrt{T^{+}}G\sqrt{T^{+}}$ and $\sigma=\text{st}_{T}\left(G\right)$.
We have $1\leq\lambda_{i}\leq\sigma$ because of the inequality$T\preceq G\preceq\text{st}_{T}\left(G\right)T$.
There are precisely $n-1$ positive eigenvalues $\lambda_{i}$, and
we have: $\text{ld}\left(T^{+}G\right)=\sum_{i}\log\lambda_{i}$.
One can show that $\sum_{i}\log\lambda_{i}\geq\log\left(\sigma-n+2\right)$
by considering the problem of minimizing $\sum_{i}\log x_{i}$ under
the constraints $\sum_{i}x_{i}=\sigma$ and $x_{i}\geq1$. This bound
is tight.
\end{proof}
Intuitively since the stretch of a tree is bounded my $O\left(m\log n\right)$,
it means that a low-stretch spanning tree provides an approximation
of a log-determinant up to a factor of $O\left(n\log n\right)$. This
could be of interest for nearly-cut graphs, with large condition numbers.


\section{Incremental sparsifiers\label{sec:Incremental-sparsifiers}}

We can do better and achieve near-linear time by using ultra-sparsifiers.
The main insight of our result is that the class preconditioners presented
by Spielman and Teng are based on incomplete Cholesky factorization,
and hence have a determinant that is relatively easy to compute, and
that furthermore they are excellent spectral preconditioners, so the
procedure \texttt{PreconditionedLogDetMonteCarlo} is efficient to
apply. We reintroduce some concepts presented in \cite{Koutis2010}
to present a self-contained result.

The central idea is to sample $O\left(n\right)$ edges from the graph
$A$, to form a subgraph $B$ that is close to a tree (hence it is
easy to compute some partial Cholesky factorization), yet it is close
to the original $A$ is the spectral sense ($A\preceq B\preceq\kappa A$),
thanks to the additional edges. The partial Cholesky factorization
is computed using the \texttt{GreedyElimination} algorithm presented
in \cite{Koutis2010}. 
In order for this section to be self-contained, we repeat the main results
of Section 4 in \cite{Spielman2009a}. 

Consider a Laplacian matrix $G$. There exists an algorithm that computes
the partial Cholesky factorization:
\[
B=PLCL^{T}P^{T}
\]
 where:
\begin{itemize}
\item $P$ is a permutation matrix
\item $L$ is a non-singular, low triangular matrix of the form
\[
L=\left(\begin{array}{cc}
L_{1,1} & 0\\
L_{2,1} & I_{n_{1}}
\end{array}\right)
\]
with the diagonal of $L_{1,1}$ being all ones.
\item $C$ has the form 
\[
C=\left(\begin{array}{cc}
D_{n-n_{1}} & 0\\
0 & A_{1}
\end{array}\right)
\]
and every row and column of $A_{1}$ has at least 3 non-zero coefficients.
Furthermore, $A_{1}$ is itself Laplacian and:
\[
\text{ld}\left(B\right)=\sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)
\]

\end{itemize}
The exact algorithm is detailled that achieves this factorization is presented in \cite{Koutis2010}.
Using this factorization, the PLD of the original Laplacian $A$ is:
\begin{eqnarray*}
\text{ld}\left(A\right) & = & \text{ld}\left(B\right)+\text{ld}\left(B^{+}A\right)\\
 & = & \sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)+\text{ld}\left(B^{+}A\right)
\end{eqnarray*}
Thus, we are left with solving a smaller problem $A_{1}$, and approximate
the value of $\text{ld}\left(B^{+}A\right)$ using the algorithm \texttt{SampleLogDet}.
ST preconditioners are appealing for this task: they guarantee that
$A_{1}$ is substantially smaller than $A$, so the recursion completes
in $O\left(\log\log n\right)$ steps. Furthermore, computing the vector
product $B^{+}Ax$ is itself efficient (in can be done in near-linear
time), so the sampling algorithm can be run is reasonable time. We
formalize the notion of chain of preconditioners by reintroducing
some material from \cite{Koutis2010}.
\begin{definition}
Definition 7.1 from \cite{Koutis2010}. $\kappa\left(n\right)$-good
chain. $\mathcal{C}=\left\{ A_{1}=A,B_{1},A_{2},\dots A_{d}\right\} $
a chain of graphs with $n_{i}$ and $m_{i}$ the number of vertices
and edges of $A_{i}$. $\mathcal{C}$ is $\kappa\left(n\right)$-good
for $A$ if:\end{definition}
\begin{enumerate}
\item $A_{i}\preceq B_{i}\preceq\kappa\left(n_{i}\right)A_{i}$
\item $A_{i+1}$= GreedyElimination$\left(B_{i}\right)$
\item $m_{i}/m_{i+1}\geq c_{r}\sqrt{\kappa\left(n_{i}\right)}$ for some
constant $c_{r}$.
\end{enumerate}
Good chains exist (Lemma 7.3 from \cite{Koutis2010}):
\begin{lemma}
\label{lem:good-chain}
(Lemma 7.3 from \cite{Koutis2010}) Given a graph $A$, \texttt{BuildChain}$\left(A,p\right)$
from \cite{Koutis2010} produces a $\tilde{O}\left(\log^{4}n\right)$-good
chain for $A$ with probability $1-p$. The algorithm runs in time
\[
\tilde{O}\left(\left(m\log n+n\log^{2}n\right)\log\left(1/p\right)\right)
\]

\end{lemma}
These chains furthermore can be used a good preconditioners for conjugate
gradient and lead to near-linear algorithms for approximate inversion
(Lemma 7.2 from \cite{Koutis2010})
\begin{lemma}
(Lemma 7.2 from \cite{Koutis2010}) 
Given a $\kappa\left(n\right)$-good chain for $A$, a vector $x$
such that $\left\Vert x-L_{A}^{+}b\right\Vert _{A}<\epsilon\left\Vert L_{A}^{+}b\right\Vert _{A}$
can be computed in $O\left(m_{d}^{3}m_{1}\sqrt{\kappa\left(n_{1}\right)}\log\left(1/\epsilon\right)\right)$
\end{lemma}
It should now become clear how we can combine a $\kappa\left(n\right)$-good
chain with the Algorithm \texttt{PreconditionedLogDetMonteCarlo}.
We start by building a chain. The partial Cholesky factorizations
provide an upper bound to $\mbox{ld}\left(A\right)$. We then refine
this upper bound by running \texttt{PreconditionedLogDetMonteCarlo}
at each state of the chain to approximate $\mbox{ld}\left(B_{i}^{+}A_{i}\right)$
with high probability. The complete algorithm is presented in Algorithm
\ref{alg:The-main-algorithm}.

We have three sources of errors introduced in our estimate:
\begin{itemize}
\item The truncation of the series of $\text{ld}\left(B^{+}A\right)$
\item The sampling error induced by having a finite number of samples when
approximating the trace of $\text{ld}\left(B^{+}A\right)$
\item The approximate inversion by $B$
\end{itemize}
The following lemmas show that we can reasonably control the inversion
error by running the ST-solver at a higher precision $O\left(\kappa\left(n_{1}\right)^{-3}\kappa\left(A_{1}\right)^{-1}\epsilon\right)$.
The ST-solvers have some convergence guarantees using the matrix norm
induced by $A$ instead of the regular Euclidian norm. The following
lemmas deal with this technicality. 
\begin{lemma}
Consider $M\in\mathcal{S}_{n}^{+}$ with $\text{Tr}\left(M\right)=n$
for all $i$. Then $\log\left|M\right|\leq0$\end{lemma}
\begin{proof}
This is a simple consequence of the concavity of the logarithm. Call
$\lambda_{i}$ the eigenvalues of $M$, then $\log\left|M\right|=\sum_{i}\log\lambda_{i}$,
and by concavity we have$n^{-1}\sum_{i}\log\lambda_{i}\leq\log\left(n^{-1}\sum_{i}\lambda_{i}\right)=\log\left(n^{-1}\text{Tr}\left(M\right)\right)=0$.
\end{proof}

We can use the bound on the norm of $A$ to compute bound the error
with a preconditioner:
\begin{proposition}
\label{prop:estimate-truncated-series}Given a $\kappa\left(n\right)$-good
chain for $A$, a start vector $x$, one can compute an $\epsilon-$estimate
of the truncated series: 
\[
\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B_{1}^{-1}A\right)^{i}xx^{T}\right)
\]
in time $\mathcal{O}\left(lm\sqrt{\kappa\left(n_{1}\right)}\left(\log\epsilon^{-1}+\log\kappa\left(n_{1}\right)+\log\kappa\left(B\right)\right)\right)$.\end{proposition}
\begin{proof}
Consider an $\epsilon-$approximate power sequence $\left(x^{\left(k\right)}\right)_{k}$
with respect to the operator $I-B^{-1}A$, and $\hat{z}$ the truncated
sequence: 
\[
\hat{z}=\sum_{k=1}^{l}\frac{1}{k}\left(x^{\left(0\right)}\right)^{T}x^{\left(k\right)}
\]
This sequence is an approximation of the exact sequence $z$:
\[
z=\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B^{-1}A\right)^{i}x^{\left(0\right)}\left(x^{\left(0\right)}\right)^{T}\right)
\]
We now bound the error between the two sequences:
\[
\left|\hat{z}-z\right|\leq\sum_{k=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|
\]
Using the Cauchy-Schwartz inequality, we obtain:
\[
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{k}x_{0}-x_{k}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}
\]
so that we can bound the deviation:
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{k=1}^{l}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}\leq4\epsilon\kappa^{3}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$. Now,
we can call the ST-solver routine with $\tilde{\epsilon}=\kappa^{-3}\kappa\left(B\right)^{-1}\epsilon$,
the cost of each call to the operator $R$ being then $\mathcal{O}\left(m\sqrt{\kappa}\left(\log\epsilon^{-1}+\log\kappa+\log\kappa\left(B\right)\right)\right)$. 
\end{proof}
We now have all the elements required to prove our main theorem. Now
we can use the previous proposition to bound the error done during
the estimation part. We have already found some bounds for the estimates
of the log-determinant of the preconditioners, and some probability
estimates for the error bounds on the trace series.
\begin{theorem}
\label{thm:main}Consider $A$ a symmetric, diagonally dominant matrix
of size $n\times n$ with $m$ non-zero entries, and $0<\epsilon<\log^{-5}n$.
One can compute $y\in\mathbb{R}$ so that $\mathbb{P}\left(\left|y-\log\left|A\right|\right|>\epsilon\right)\leq\frac{1}{n}$
in expected time $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\left(\frac{n\kappa\left(A\right)}{\epsilon}\right)\right)\right)$. \end{theorem}
\begin{proof}
Consider a $\tilde{O}\left(\log^{4}n\right)$-chain \ref{lem:good-chain}
for the Laplacian of $A$. The cost of constructing such a chain is
negligible against the sampling step. From Corollary \ref{cor:preconditioning},
for each level of the chain, we compute $p=\tilde{O}\left(\epsilon^{-2}\log n\right)$
approximate truncated chains of length $l=\tilde{O}\left(\log^{4}n\left(\log n+\log\left(\epsilon^{-1}\right)\right)\right)$.
From Proposition \ref{prop:estimate-truncated-series}, the cost of
running each chain is $\tilde{O}\left(lm\log^{2}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right)$.
Thus the time to approximate the residue PLD at level $i$ is bounded
by $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right).$
Finally, since $m_{i}$ decreases faster than geometrically, the number
$d$ of steps in the chain is $\tilde{O}\left(1\right)$.
\end{proof}

\input{detsdd_6bounds}


\section*{Comments}

A first approximation upper-bound of the log-determinant follows immediately
from the computation of the $\kappa-$good chain. We presented in
Section \ref{sec:A-first-preconditioner} a first analysis to bound
the value of the residue PLD. This analysis was done using trees as
preconditioners, it can be carried on on more general preconditioners
by introducing a generalization of the stretch over a subgraph (see
the Appendix, section \ref{sub:Stretch-graph}). Since the bulk of
the computations are performed in estimating the residue PLD, it would
be interesting to see if this could be bypassed using better bounds
based on the stretch.

When looking at each step of the analysis, one can see that the $\epsilon^{-2}$
factor comes from the approximation of the trace of a matrix by sampling.
This seems to be a fundamental limitation of this method and it is
shared with other algorithms that rely on random projections. This
result is absolute. It would be interesting to see if the analysis
could be tightened to present a relative bound that does not depend
on the condition number of $A$. Also, even if this algorithm presents
a linear bound, it requires a fairly advanced machinery (ST solvers)
that may limit its practicality. Some heuristic implementation, for
example based on algebraic multi-grid methods, could be a first step
in this direction.

The authors are much indebted to Satish Rao and Jim Demmel for suggesting
the original idea and their helpful comments on the draft of this
article.


\input{detsdd_8appendixa}


\subsection{Pseudo-log-determinant and pseudo-matrix-logarithm}

This section provides the proofs related to the PLD. These proofs
are elementary and do not provide much insight. In this section, we
call$\mathcal{D}_{n}^{+}$the set of all non-zero diagonalizable matrices
in $\mathbb{R}^{n\times n}$ with non-negative eigenvalues.
\begin{lemma}
Given two non-empty orthornormal families $P$ and $Q$ that span
the same space, $P^{T}Q$ is invertible and we have:
\[
\left(P^{T}Q\right)^{-1}=Q^{T}P
\]
\end{lemma}
\begin{proof}
Move the proof here.\end{proof}
\begin{lemma}
Given $A\in\mathcal{D}_{n}^{+}$ and $P$ an orthonormal basis for
$\text{span}\left(A\right)$, we have the relation:
\[
\text{ld\ensuremath{\left(A\right)}}=\log\left|P^{T}AP\right|
\]
\end{lemma}
\begin{proof}
Since $A$ is diagonalizable, consider its decomposition $A=Q\Delta Q^{T}$
with $\Delta$ a diagonal matrix with positive entries on the diagonal.
Note that $\Delta$ exists since $A\neq0$. By definition of the PLD,
$\text{ld}\left(A\right)=\sum_{i}\log\Delta_{i}$. Since $\mathbb{R}^{n}=\text{span}\left(Q\right)\oplus\text{Ker}\left(A\right)=\text{span}\left(P\right)\oplus\text{Ker}\left(A\right)$,
there exists an invertible matrix $B$ such that $Q=PB$. Using the
fact that $P$ is an orthonormal family, we get 
\[
P^{T}Q=P^{T}PB=B
\]
 Furthermore, $B$ is invertible, so $QB^{-1}=P$. Using the fact
that the eigenvalues make an orthonormal family, we get: $B^{-1}=Q^{T}P$,
hence:
\[
\left(P^{T}Q\right)^{-1}=Q^{T}P
\]


Now we wan conclude:
\[
\left|P^{T}AP\right|=\left|P^{T}Q\Delta Q^{T}P\right|=\left|P^{T}Q\right|\left|\Delta\right|\left|Q^{T}P\right|=\left|\Delta\right|
\]
Hence $\log\left|P^{T}AP\right|=\sum_{i}\log\Delta_{i}=\text{ld}\left(A\right)$.\end{proof}
\begin{lemma}
Given $A\succ0,\, B\succ0$, then $AB$ is diagonalizable and $AB\succ0$.
Same thing for $A\succeq0$ and $B\succeq0$.\end{lemma}
\begin{proof}
The proof of this lemma is in {[}cite Spielman{]}.\end{proof}
\begin{lemma}
Given $A,B\in\mathcal{D}_{n}^{+}$ so that $\text{Ker}\left(A\right)=\text{Ker}\left(B\right)$,
then $C=AB\in\mathcal{D}_{n}^{+}$ and $\text{Ker}\left(C\right)=\text{Ker}\left(A\right)$.\end{lemma}
\begin{proof}
Consider the diagonalization of $A$ and $B$: $A=Q_{A}\Delta_{A}Q_{A}^{T}$
and $B=Q_{B}\Delta_{B}Q_{B}^{T}$, and call $D=Q_{A}^{T}Q_{B}$. Since
$\Delta_{A}D\Delta_{B}$ is invertible, $\text{Ker}\left(C\right)=\text{Ker}\left(A\right)$.
Furthermore, consider a non-null eigenvalue $u=Q_{A}s$. Then $s$
verifies the equation:
\[
\Delta_{A}D\Delta_{B}D^{-1}s=\lambda s
\]
So all the eigenvalues of $AB$ are that of $\Delta_{A}D\Delta_{B}D^{-1}$,
and they are positive from the lemma above.\end{proof}
\begin{lemma}
\label{lem:pld-sum}Consider $A,B\in\mathcal{D}_{n}^{+}$with $\text{Ker}\left(A\right)=\text{Ker}\left(B\right)$,
then:
\[
\text{ld}\left(AB\right)=\text{ld}\left(A\right)+\text{ld}\left(B\right)
\]
\end{lemma}
\begin{proof}
From the lemma above, the product $AB$ is in $\mathcal{D}_{n}^{+}$and
the term $\text{ld}\left(AB\right)$ is well defined. Consider the
diagonalization of $A$ and $B$: $A=Q_{A}\Delta_{A}Q_{A}^{T}$ and
$B=Q_{B}\Delta_{B}Q_{B}^{T}$. Then 
\begin{eqnarray*}
\text{ld}\left(AB\right) & = & \log\left|Q_{A}^{T}ABQ_{A}\right|\\
 & = & \log\left|\Delta_{A}Q_{A}^{T}Q_{B}\Delta_{B}\left(Q_{A}^{T}Q_{B}\right)^{-1}\right|\\
 & = & \log\left|\Delta_{A}\right|+\log\left|\Delta_{B}\right|\\
 & = & \text{ld}\left(A\right)+\text{ld}\left(B\right)
\end{eqnarray*}

\end{proof}

\subsection{Other inequalities}
\begin{lemma}
\label{lem:Jensen-inequality-matrix-logarithm}Jensen inequality for
the matrix logarithm. Given $A\in\mathcal{S}_{n}^{+},$ $0\prec A\prec2$,
the following inequalities hold:
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)\geq\frac{1}{n}\text{Tr}\left(\log A\right)
\]
\[
\text{ld}\left(\frac{\text{Tr}\left(A\right)}{n-1}\right)\geq\frac{1}{n-1}\text{Tr}\left(\log A\right)
\]
with $\log A=\sum_{k\geq1}\frac{1}{k}\left(I-A\right)^{k}$\end{lemma}
\begin{proof}
Consider the diagonalization of $A$: $A=P\Delta P^{T}$ with $\Delta$
a diagonal (positive) matrix, and $P$ an orthogonal matrix. Then
\[
\log A=\sum_{k\geq1}\frac{1}{k}\left(PP^{T}-P\Delta P^{T}\right)^{k}=\sum_{k\geq1}\frac{1}{k}\left[P\left(I-\Delta\right)P^{T}\right]^{k}=P\left[\sum_{k\geq1}\frac{1}{k}\left(I-\Delta\right)^{k}\right]P^{T}=P\Gamma P^{T}
\]
 with $\Gamma$ a diagonal matrix that verifies $\Gamma_{ii}=\log\Delta_{ii}$.
We can then conclude using the concavity of the logarithm over the
reals: 
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)=\log\left(\frac{\sum_{i}\Delta_{ii}}{n}\right)\geq\frac{1}{n}\sum_{i}\log\Delta_{ii}=\frac{1}{n}\text{Tr}\left(\Gamma\right)=\frac{1}{n}\text{Tr}\left(\log A\right)
\]


The same reasoning holds for the pseudo-log-determinant while considering
all but one eigenvalue\end{proof}
\begin{lemma}
Power relation for the matrix log: Given $A\in\mathcal{S}_{n}^{+},$
$0\prec A\prec2$, and $k\in\mathbb{N}$, then $\log\left(A^{k}\right)=k\log A$
\end{lemma}

\subsection{Stretch of a graph\label{sub:Stretch-graph}}

We introduce here a refinement on an upper bound that is a byproduct
of using the algorithm XXX.
\begin{definition}
We define the \textbf{stretch of a graph} $G=\left(V,E,\omega\right)$
with respect to a subgraph $H=\left(V,\tilde{E},\tilde{\omega}\right)\subset G$
by the weighted sum of effective resistances of edges $e\in E$ with
respect to the graph $H$:
\[
\text{st}_{H}\left(G\right)=\sum_{e\in E}\omega_{e}\text{eff}_{H}\left(e\right)
\]

\end{definition}
This is a generalization of the notion of stretch defined by Alon,
Karp, Peleg and West in .... We can use this definition to generalize
theorem 2.1 in \cite{Spielman2009b}
\begin{theorem}
Let $G=\left(V,E,\omega\right)$ be a connected graph and let $H=\left(V,\tilde{E},\tilde{\omega}\right)$
be a connected subgraph of $G$. Let $L_{G}$ and $L_{H}$ be the
Laplacian matrices of $G$ and $H$ respectively. Then:
\[
\text{Tr}\left(L_{H}^{+}L_{G}\right)=\text{st}_{H}\left(G\right)
\]
where $L_{H}^{+}$ is the pseudo inverse of $L_{H}$.\end{theorem}
\begin{proof}
The proof is nearly identical to that of \cite{Spielman2009b}, except
for the last line:

\begin{eqnarray*}
\text{Tr}\left(L_{H}^{+}L_{G}\right) & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(L_{\left(u,v\right)}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(\left(\psi_{u}-\psi_{v}\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\left(\psi_{u}-\psi_{v}\right)
\end{eqnarray*}
and the latter term is the effective resistance between $u$ and $v$
in the graph $H$:
\begin{eqnarray*}
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{eff}_{H}\left(u,v\right)\\
 & = & \text{st}_{H}\left(G\right)
\end{eqnarray*}

\end{proof}
From a practical perspective, the graph stretch can be computed in
$\tilde{O}\left(m\log n/\epsilon^{2}\right)$
\begin{proposition}
There exists an algorithm that computes \textup{an $\epsilon$-approximation
of $\text{st}_{H}\left(G\right)$ in} $\tilde{O}\left(m\log n/\epsilon^{2}\right)$\end{proposition}
\begin{proof}
This is the second main result from Spielman and Srivastana (XXX cite).
\end{proof}


% \bibliographystyle{plain}
\bibliography{2012_sparse_info}
% \bibliography{2012_sparse_info,siam}

\end{document}
