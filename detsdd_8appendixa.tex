
\section*{Appendix A: Proofs of Section 2}


\subsection{Proof of Theorem \ref{thm:det-sampling-theorem}}

\label{sub:det-sampling-proof}
\begin{proof}
The proof of this theorem follows the proof of the Main Theorem in
\cite{Barry1999} with some slight modifications. Using triangular
inequality: 
\[
\left|y-\hat{y}_{p,l}\right|\leq\left|\mathbb{E}\left[\hat{y}_{p,l}\right]-\hat{y}_{p,l}\right|+\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right|
\]


Since $S$ is upper-bounded by $\left(1-\delta\right)I$, we have
for all $k\in\mathbb{N}$: 
\[
\left|\mbox{Tr}\left(S^{k}\right)\right|\leq n\left(1-\delta\right)^{k}
\]
We have $\mathbb{E}\left[\hat{y}_{p,l}\right]=-\sum_{i=1}^{l}i^{-1}S^{i}$
and $y=-\sum_{i=1}^{\infty}i^{-1}S^{i}$. Using again triangle inequality,
we can bound the error with respect to the expected value: 
\begin{eqnarray*}
\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right| & = & n^{-1}\left|\sum_{i=l+1}^{\infty}\frac{1}{i}\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & n^{-1}\sum_{i=l+1}^{\infty}\frac{1}{i}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{n\left(l+1\right)}\sum_{i=l+1}^{\infty}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{l+1}\sum_{i=l+1}^{\infty}\left(1-\delta\right)^{k}\\
 & \leq & \frac{1}{l+1}\frac{\left(1-\delta\right)^{l+1}}{\delta}\\
 & \leq & \frac{\left(1-\delta\right)^{l+1}}{\delta}
\end{eqnarray*}


And since $\delta\le-\log\left(1-\delta\right)$, for a choice of
$l\geq\delta^{-1}\log\left(\frac{2}{\epsilon\delta}\right)$, the
latter part is less than $\epsilon/2$. We now bound the first part
using Lemma \ref{lem:bernstein-trace}. Call $H$ the truncated series:
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}
\]
This truncated series is upper-bounded by $0$ ($H$ is negative,
semi-definite). The lowest eigenvalue of the truncated series can
be lower-bounded in terms of $\delta$: 
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}\succeq-\sum_{i=1}^{m}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I=\left(\log\delta\right)I
\]
We can now invoke Lemma \ref{lem:bernstein-trace} to conclude: 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\left(\mathbf{u}_{i}^{T}\mathbf{u}_{i}\right)^{-1}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}-n^{-1}\mbox{Tr}\left(H\right)\right|\geq\frac{\epsilon}{2}\right]\leq2\exp\left(-\frac{p\epsilon^{2}}{16n^{-1}\left(\log\left(1/\delta\right)\right)^{2}+4\log\left(1/\delta\right)\epsilon/3}\right)
\]
Thus, any choice of 
\[
p\geq16\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(2/\eta\right)\log^{2}\left(\delta^{-1}\right)\geq\log\left(2/\eta\right)\epsilon^{-2}\left(16n^{-1}\log^{2}\left(\delta^{-1}\right)+\frac{4}{3}\epsilon\log\left(\delta^{-1}\right)\right)
\]
satisfies the inequality:$2\exp\left(-\frac{p\epsilon^{2}}{16n^{-1}\left(\log\left(1/\delta\right)\right)^{2}+4\log\left(1/\delta\right)\epsilon/3}\right)\leq\eta$.
\end{proof}

\subsection{Proof of Corollary \ref{cor:preconditioning}}
\begin{proof}
We introduce some notations that will prove useful for the rest of
the article: 
\[
H=I-B^{-1}A
\]
\[
S=I-B^{-1/2}AB^{-1/2}
\]
with $B^{-1/2}$ the inverse of the square root%
\footnote{Given a real PSD matrix $X$, which can be diagonalized: $X=Q\Delta Q^{T}$
with $\Delta$ diagonal, and $\Delta_{ii}\geq0$. Call $Y=Q\sqrt{\Delta}Q^{T}$
the square root of $X$, then $Y^{2}=X$.%
} of the positive-definite matrix $B$. The inequality (\ref{eq:A-B-bounds})
is equivalent to $\kappa^{-1}B\preceq A\preceq B$, or also: 
\[
\left(1-\kappa^{-1}\right)I\succeq I-B^{-1/2}AB^{-1/2}\succeq0
\]
\begin{equation}
\left(1-\kappa^{-1}\right)I\succeq S\succeq0\label{eq:s-encadrement}
\end{equation}


The matrix $S$ is a contraction, and its spectral radius is determined
by $\kappa$. Furthermore, computing the determinant of $B^{-1}A$
is equivalent to computing the determinant of $I-S$: 
\begin{eqnarray*}
\log\left|I-S\right| & = & \log\left|B^{-1/2}AB^{-1/2}\right|\\
 & = & \log\left|A\right|-\log\left|B\right|\\
 & = & \log\left|B^{-1}A\right|\\
 & = & \log\left|I-H\right|
\end{eqnarray*}


and invoking Theorem \ref{thm:det-sampling-theorem} gives us bounds
on the number of calls to matrix-vector multiplies with respect to
$S$. It would seem at this point that computing the inverse square
root of $B$ is required, undermining our effort. However, we can
reorganize the terms in the series expansion to yield only full inverses
of $B$. Indeed, given $l\in\mathbb{N}^{*}$, consider the truncated
series: 
\begin{eqnarray*}
y_{l} & = & -\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(-1\right)^{j}\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(-1\right)^{j}\mbox{Tr}\left(\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(-1\right)^{j}\mbox{Tr}\left(\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(-1\right)^{j}\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(H^{i}\right)
\end{eqnarray*}
Hence, the practical computation of the latter sum can be done on
$A^{-1}B$. To conclude, if we compute $p=16\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(2/\eta\right)\log^{2}\left(\kappa\right)$
truncated chains of length $l=\kappa\log\left(\frac{2\kappa}{\epsilon}\right)$,
we get our result. This requires $lp$ multiplications by $A$ and
inversions by $B$. 
\end{proof}

\subsection{Proof of Theorem \ref{thm:preconditioning-approx}}

We prove here the main result of Section \ref{sec:Preconditioned-log-determinants}.
In the following, $A$ and $B$ are positive-definite matrices in
$\mathcal{S}_{n}$, and $B$ is a $\kappa-$approximation of $A$
($A\preceq B\preceq\kappa A$). The following notations will prove
useful:

\begin{equation}
S=I-B^{-1/2}AB^{-1/2}\label{eq:S-def}
\end{equation}


\begin{equation}
R=I-B^{-1}A\label{eq:R-def}
\end{equation}


\[
\varphi=\kappa^{-1}
\]


Recall the definition of the matrix norm. Given $M\in\mathcal{S}_{n}^{+}$,
$\left\Vert M\right\Vert _{B}=\max_{x\neq0}\sqrt{\frac{x^{T}Mx}{x^{T}Bx}}$
\begin{lemma}
\label{lem:S-R-contractions} $S$ and $R$ are contractions for the
Euclidian and $B-$norms: 
\begin{eqnarray*}
\left\Vert S\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert _{B} & \leq & \left(1-\varphi\right)^{2}
\end{eqnarray*}
\end{lemma}
\begin{proof}
Recall the definition of the matrix norm: $\left\Vert S\right\Vert =\max_{x^{T}x\leq1}\sqrt{x^{T}Sx}$.
Since we know from Equation (\ref{eq:s-encadrement}) that $S\preceq\left(1-\varphi\right)I$,
we get the first inequality.

The second inequality is a consequence of Proposition 3.3 from \cite{Spielman2009a}:
$A$ and $B$ have the same nullspace and we have the linear matrix
inequality $A\preceq B\preceq\kappa A$, which implies that the eigenvalues
of $B^{-1}A$ lie between $\kappa^{-1}=\varphi$ and $1$. This implies
that the eigenvalues of $I-B^{-1}A$ are between $0$ and $1-\varphi$.

Recall the definition of the matrix norm induced by the $B$-norm
over $\mathbb{R}^{n}$: 
\begin{eqnarray*}
\left\Vert R\right\Vert _{B} & = & \max_{x\neq0}\frac{\left\Vert Rx\right\Vert _{B}}{\left\Vert x\right\Vert _{B}}\\
 & = & \max_{\left\Vert x\right\Vert _{B}^{2}\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{x^{T}Bx\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{y^{T}y\leq1}\sqrt{y^{T}B^{-1/2}R^{T}BRB^{-1/2}y}
\end{eqnarray*}
and the latter expression simplifies: 
\begin{eqnarray*}
B^{-1/2}R^{T}BRB^{-1/2} & = & B^{-1/2}\left(I-AB^{-1}\right)B\left(I-B^{-1}A\right)B^{-1/2}\\
 & = & \left(I-B^{-1/2}AB^{-1/2}\right)\left(I-B^{-1/2}AB^{-1/2}\right)\\
 & = & S^{2}
\end{eqnarray*}
so we get: 
\[
\left\Vert R\right\Vert _{B}=\left\Vert S^{2}\right\Vert \leq\left\Vert S\right\Vert ^{2}\leq\left(1-\varphi\right)^{2}
\]

\end{proof}
The approximation of the log-determinant is performed by computing
sequences of power series $\left(R^{k}x\right)_{k}$. These chains
are computed approximately by repeated applications of the $R$ operator
on the previous element of the chain, starting from a random variable
$x_{0}$. We formalize the notion of an approximate chain.
\begin{definition}
\emph{Approximate power sequence. }Given a linear operator $H$, a
start point $x^{\left(0\right)}\in\mathbb{R}^{n}$, and a positive-definite
matrix $D$, we define an $\epsilon-$approximate power sequence as
a sequence that does not deviate too much from the power sequence:
\[
\left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}\right\Vert _{D}\leq\epsilon\left\Vert Hx^{\left(k\right)}\right\Vert _{D}
\]

\end{definition}
We now prove the following result that is quite intuitive: if the
operator $H$ is a contraction and if the relative error $\epsilon$
is not too great, the sum of all the errors on the chain is bounded.
\begin{lemma}
\label{lem:nu-sequence-bound}Let $H$ be a linear operator and $D$
a norm over the space of that linear operator. Assume that the operator
$H$ is a contraction under this norm ($\left\Vert H\right\Vert _{D}<1$)
and consider $\rho\in\left(0,1\right)$ so that $\left\Vert H\right\Vert _{D}\leq\left(1-\rho\right)^{2}$.
Consider $\left(x^{\left(k\right)}\right)_{k}$ a $\nu-$approximate
power sequence for the operator $H$ and the norm $D$. If $\rho\leq1/2$
and $\nu\leq\rho/2$, the total error is bounded: 
\[
\sum_{k=0}^{\infty}\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}\leq4\rho^{-2}\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]
\end{lemma}
\begin{proof}
Call $\omega_{k}=\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}$
and $\theta_{k}=\left\Vert Hx^{\left(k\right)}\right\Vert _{D}$.
We are going to bound the rate of convergence of these two series.
We have first using triangular inequality on the $D$ norm and then
the definition of the induced matrix norm. 
\begin{eqnarray*}
\theta_{k} & \leq & \left\Vert Hx^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}+\left\Vert H^{k}x^{\left(0\right)}\right\Vert _{D}\\
 & = & \omega_{k}+\left\Vert H^{k}x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \omega_{k}+\left\Vert H\right\Vert _{D}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\end{eqnarray*}
We now bound the error on the $\omega_{k}$ sequence: 
\begin{eqnarray*}
\omega_{k+1} & = & \left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}+Hx^{\left(k\right)}-H^{k+1}x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \left\Vert Hx^{\left(k\right)}-H^{k+1}x^{\left(0\right)}\right\Vert _{D}+\left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}\right\Vert _{D}\\
 & \leq & \left\Vert H\right\Vert _{D}\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}+\nu\left\Vert Hx^{\left(k\right)}\right\Vert _{D}\\
 & = & \left\Vert H\right\Vert _{D}\omega_{k}+\nu\theta_{k}\\
 & \leq & \left\Vert H\right\Vert _{D}\omega_{k}+\nu\left(\omega_{k}+\left\Vert H\right\Vert _{D}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}\right)\\
 & \leq & \left[\left\Vert H\right\Vert _{D}+\nu\right]\omega_{k}+\nu\left\Vert H\right\Vert _{D}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\end{eqnarray*}
The assumption $\rho\leq1-\sqrt{\left\Vert H\right\Vert _{D}}$ is
equivalent to $\left\Vert H\right\Vert _{D}\leq\left(1-\rho\right)^{2}$,
so the last inequality implies: 
\[
\omega_{k+1}\leq\left[\left(1-\rho\right)^{2}+\nu\right]\omega_{k}+\nu\left(1-\rho\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]


Note that the inequality $\left(1-\rho\right)^{2}+\nu\leq1-\rho$
is equivalent to $\nu\leq\rho-\rho^{2}$. Using the hypothesis, this
implies: 
\begin{equation}
\omega_{k+1}\leq\left(1-\rho\right)\omega_{k}+\nu\left(1-\rho\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{D}\label{eq:anon1}
\end{equation}


We show by induction that: 
\[
\forall k,\omega_{k}\leq\frac{\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}}{1-\sqrt{1-\rho}}\left(\sqrt{1-\rho}\right)^{k-1}
\]


Note first that 
\begin{align*}
\omega_{1} & =\left\Vert x^{\left(1\right)}-Hx^{\left(0\right)}\right\Vert _{D}\\
 & \leq\nu\left\Vert Hx^{\left(0\right)}\right\Vert _{D}\\
 & \leq\nu\left\Vert H\right\Vert _{D}\left\Vert x^{\left(0\right)}\right\Vert _{D}\\
 & \leq\nu\left(1-\rho\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{D}\\
 & \leq\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}
\end{align*}
So this relation is verified for $k=1$. Now, assuming it is true
for $k$, we use Equation (\ref{eq:anon1}) to see that: 
\begin{eqnarray*}
\omega_{k} & \leq & \left(1-\rho\right)\omega_{k}+\nu\left(1-\rho\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \left(1-\rho\right)\omega_{k}+\nu\left(\sqrt{1-\rho}\right)^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \nu\left\Vert x^{\left(0\right)}\right\Vert _{D}\left[\frac{\left(1-\rho\right)}{1-\sqrt{1-\rho}}\left(\sqrt{1-\rho}\right)^{k-1}+\left(\sqrt{1-\rho}\right)^{k}\right]\\
 & = & \nu\left\Vert x^{\left(0\right)}\right\Vert _{D}\left(\sqrt{1-\rho}\right)^{k}\left[\frac{\sqrt{1-\rho}}{1-\sqrt{1-\rho}}+1\right]\\
 & = & \frac{\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}}{1-\sqrt{1-\rho}}\left(\sqrt{1-\rho}\right)^{k}
\end{eqnarray*}
which is the the property for $k+1$. Using this property, we can
sum all the errors by a geometric series (note that $\omega_{0}=0$).
\[
\sum_{k=1}^{\infty}\omega_{k}\leq\frac{\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}}{1-\sqrt{1-\rho}}\sum_{k=0}^{\infty}\left(\sqrt{1-\rho}\right)^{k}=\frac{\nu\left\Vert x^{\left(0\right)}\right\Vert _{D}}{\left(1-\sqrt{1-\rho}\right)^{2}}
\]


Finally, note that for $\rho\in\left(0,1/2\right)$, the inequality
$\nu\leq\rho/2$ implies $\nu\leq\rho-\rho^{2}$. Furthermore, by
concavity of the square root function, we have $\sqrt{1-\rho}\leq1-\rho/2$
for $\rho\leq1$. Thus, $\left(1-\sqrt{1-\rho}\right)^{2}\geq\rho^{2}/4$
and we get our result.
\end{proof}
We can use the bound on the norm of $A$ to compute bound the error
with a preconditioner:
\begin{lemma}
\label{lem:partial-sequence-approximate}Consider $A,B$ with the
same hypothesis as above, $x_{0}\in\mathbb{R}^{n}$, and the additional
hypothesis $\nu\in\left(0,\frac{1}{2\kappa}\right)$ and $\kappa\ge2$,
and $\left(x_{u}\right)_{u}$ an $\nu-$approximate power sequence
for the operator $R$ with start vector $x_{0}$. Then: 
\[
\left|\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}R^{i}x_{0}-\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}x_{i}\right|\leq4\nu\kappa^{2}\sqrt{\kappa\left(B\right)}\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$.\end{lemma}
\begin{proof}
Call $\hat{z}$ the truncated sequence: 
\[
\hat{z}=\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}x_{i}
\]
This sequence is an approximation of the exact sequence $z$: 
\[
z=\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}R^{i}x_{0}
\]
We now bound the error between the two sequences: 
\begin{equation}
\left|\hat{z}-z\right|\leq\sum_{i=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{i}x_{0}-x_{i}\right)\right|\leq\sum_{i=1}^{l}\left|x_{0}^{T}\left(R^{i}x_{0}-x_{i}\right)\right|\leq\sum_{i=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{i}x_{0}-x_{i}\right)\right|\label{eq:anon}
\end{equation}
Using the Cauchy-Schwartz inequality, we obtain: 
\begin{equation}
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{i}x_{0}-x_{i}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{i}x_{0}-x_{i}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}\label{eq:anon-1}
\end{equation}
From Lemma (\ref{lem:S-R-contractions}), we have $\left\Vert R\right\Vert _{B}\leq\left(1-\varphi\right)^{2}$,
and from the hypothesis, we have $\nu\in\left(0,\varphi/2\right)$
and $\varphi\le1/2$, so we can bound the deviation using the bound
from Lemma \ref{lem:nu-sequence-bound}: 
\begin{equation}
\sum_{i=1}^{l}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}\leq\sum_{i=1}^{\infty}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}\leq4\varphi^{-2}\nu\left\Vert x_{0}\right\Vert _{B}=4\kappa^{2}\nu\left\Vert x_{0}\right\Vert _{B}\label{eq:anon-2}
\end{equation}
Combining Equations (\ref{eq:anon}), (\ref{eq:anon-1}) and (\ref{eq:anon-2}),
we get: 
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{i=1}^{l}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}\leq4\nu\kappa^{2}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}
\]
Finally, it is more convenient to consider the Euclidian norm for
the norm of $x_{0}$. Call $\lambda_{\text{max }}$ and $\lambda_{\text{min }}$
the extremal eigenvalues of the positive semidefinite matrix $B$.
By definition of the matrix norm: $\left\Vert x_{0}\right\Vert _{B}=\sqrt{x_{0}^{T}Bx_{0}}\leq\sqrt{\lambda_{\text{max}}}\left\Vert x_{0}\right\Vert $
and $\left\Vert B^{-1}x_{0}\right\Vert _{B}=\sqrt{x_{0}^{T}B^{-1}x_{0}}\leq\sqrt{\lambda_{\text{min}}^{-1}}\left\Vert x_{0}\right\Vert $
so we get: 
\[
\left|\hat{z}-z\right|\leq4\nu\kappa^{2}\sqrt{\kappa\left(B\right)}\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$. 
\end{proof}
We now have all the elements required for the proof of Theorem \ref{thm:preconditioning-approx}.
\begin{proof}
Consider $\mathbf{u}_{j}\sim\mathcal{N}\left(0,I_{n}\right)$ for
$j=1\cdots p$, and $x_{i,j}=\begin{cases}
\mathbf{u}_{j}/\left\Vert \mathbf{u}_{j}\right\Vert  & \,\, i=0\\
x_{i-1,j}-C\left(Ax_{i-1,j}\right) & \,\, i>0
\end{cases}$

Call 
\[
z_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{i=1}^{l}\frac{1}{i}\left(x_{0,j}\right)^{T}x_{i,j}
\]
\[
\hat{y}_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\left(x_{0,j}\right)^{T}S^{k}x_{0,j}
\]


By construction, $\left(x_{i,j}\right)_{i}$ is an $\nu-$approximate
chain for the operator $R$. Applying Lemma \ref{lem:partial-sequence-approximate}
to the operator $R$ under the norm $B$, we get: 
\begin{eqnarray*}
\left|z_{p,l}-\hat{y}_{p,l}\right| & \leq & 4\nu\kappa^{2}\sqrt{\kappa\left(B\right)}\left[\frac{1}{p}\sum_{j=1}^{p}\left\Vert x_{0,j}\right\Vert ^{2}\right]=4\nu\kappa^{2}\sqrt{\kappa\left(B\right)}
\end{eqnarray*}


since $\left\Vert x_{0,j}\right\Vert ^{2}=1$, which gives us a deterministic
bound. Consider $\nu\leq\min\left(\frac{\epsilon}{8\kappa^{2}\sqrt{\kappa\left(B\right)}},\frac{1}{2\kappa}\right)$.
Then $\left|z_{p,l}-\hat{y}_{p,l}\right|\leq\epsilon/2$. Furthermore:
\[
\left|z_{p,l}-y\right|\leq\left|z_{p,l}-\hat{y}_{p,l}\right|+\left|y-\hat{y}_{p,l}\right|
\]
and $\mathbb{P}\left[\left|y-\hat{y}_{p,l}\right|\geq\epsilon/2\right]\leq\eta$
for a choice of $p\geq16\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(2/\eta\right)\log^{2}\left(\delta^{-1}\right)$
and $l\geq4\kappa\log\left(\frac{n}{\delta\epsilon}\right)$. Hence,
we get our bound result of 
\[
pl=64\kappa\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(2/\eta\right)\log^{2}\left(\delta^{-1}\right)\log\left(\frac{n}{\delta\epsilon}\right)
\]
\end{proof}

