
\section*{Appendix A: Proofs of Section 2}


\subsection{Proof of Corollary\ref{cor:preconditioning}}

\begin{proof} We introduce some notations that will prove useful
for the rest of the article: 
\[
H=I-B^{-1}A
\]
\[
S=I-B^{-1/2}AB^{-1/2}
\]
with $B^{-1/2}$ the inverse of the square root of the positive-definite
matrix $B$%
\footnote{Given a real PSD matrix $X$, which can be diagonalized: $X=Q\Delta Q^{T}$
with $\Delta$ diagonal, and $\Delta_{ii}\geq0$. Call $Y=Q\sqrt{\Delta}Q^{T}$
the square root of $X$, then $Y^{2}=X$.%
}. The inequality \ref{eq:A-B-bounds} is equivalent to $\kappa^{-1}B\preceq A\preceq B$,
or also: 
\[
\left(1-\kappa^{-1}\right)I\succeq I-B^{-1/2}AB^{-1/2}\succeq0
\]
\[
\left(1-\kappa^{-1}\right)I\succeq S\succeq0
\]


The matrix $S$ is a contraction, and its spectral radius is determined
by $\kappa$. Furthermore, computing the determinant of $B^{-1}A$
is equivalent to computing the determinant of $I-S$: 
\begin{eqnarray*}
\log\left|I-S\right| & = & \log\left|B^{-1/2}AB^{-1/2}\right|\\
 & = & \log\left|A\right|-\log\left|B\right|\\
 & = & \log\left|B^{-1}A\right|\\
 & = & \log\left|I-H\right|
\end{eqnarray*}


and invoking Theorem \ref{thm:det-sampling-theorem} gives us bounds
on the number of calls to matrix-vector multiplies with respect to
$S$. It would seem at this point that computing the inverse square
root of $B$ is required, undermining our effort. However, we can
reorganize the terms in the series expansion to yield only full inverses
of $B$. Indeed, given $l\in\mathbb{N}^{*}$, consider the truncated
series: 
\begin{eqnarray*}
y_{l} & = & -\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(H^{i}\right)
\end{eqnarray*}
Hence, the practical computation of the latter sum can be done on
$A^{-1}B$. To conclude, if we compute $p=4\epsilon^{-2}\left(\log\kappa\right)^{2}\log n$
truncated chains of length $l=\kappa\left(2\log n+\log\kappa+\log\left(\epsilon^{-1}\right)\right)$,
we get our result. This requires $lp$ multiplies by $A$ and inversions
by $B$. \end{proof} 


\subsection{Proof of Theorem \ref{thm:preconditioning-approx}}

We prove here the main result of Section \ref{sec:Preconditioned-log-determinants}.
In the following, $A$ and $B$ are positive-definite matrices in
$\mathcal{S}_{n}$, and $B$ is a $\kappa-$approximation of $A$
($A\preceq B\preceq\kappa A$). The following notations will prove
useful:

\begin{equation}
S=I-B^{-1/2}AB^{-1/2}\label{eq:S-def}
\end{equation}


\begin{equation}
R=I-B^{-1}A\label{eq:R-def}
\end{equation}


\[
\varphi=\kappa^{-1}
\]


\begin{lemma}\label{lem:S-R-contractions} $S$ and $R$ are contractions
for the Euclidian and $B-$norms: 
\begin{eqnarray*}
\left\Vert S\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert _{B} & \leq & \left(1-\varphi\right)^{2}
\end{eqnarray*}
\end{lemma} 

\begin{proof} Recall the definition of the matrix norm:$\left\Vert S\right\Vert =\max_{x^{T}x\leq1}\sqrt{x^{T}Sx}$.
Since we know that $S\preceq\left(1-\varphi\right)I$, we get the
first inequality.

The second inequality is a consequence of Proposition 3.3 from \cite{Spielman2009a}:
$A$ and $B$ have the same nullspace and we have the linear matrix
inequality $A\preceq B\preceq\kappa A$, which implies that the eigenvalues
of $B^{-1}A$ lie between $\kappa^{-1}=\varphi$ and $1$. This implies
that the eigenvalues of $I-B^{-1}A$ are between $1-\varphi$ and
$0$.

Recall the definition of the matrix norm induced by the $B$-norm
over $\mathbb{R}^{n}$: 
\begin{eqnarray*}
\left\Vert R\right\Vert _{B} & = & \max_{x\neq0}\frac{\left\Vert Rx\right\Vert _{B}}{\left\Vert x\right\Vert _{B}}\\
 & = & \max_{\left\Vert x\right\Vert _{B}^{2}\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{x^{T}Bx\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{y^{T}y\leq1}\sqrt{y^{T}B^{-1/2}R^{T}BRB^{-1/2}y}
\end{eqnarray*}
and the latter expression simplifies: 
\begin{eqnarray*}
B^{-1/2}R^{T}BRB^{-1/2} & = & B^{-1/2}\left(I-AB^{-1}\right)B\left(I-B^{-1}A\right)B^{-1/2}\\
 & = & \left(I-B^{-1/2}AB^{-1/2}\right)\left(I-B^{-1/2}AB^{-1/2}\right)\\
 & = & S^{2}
\end{eqnarray*}
so we get: 
\[
\left\Vert R\right\Vert _{B}=\left\Vert S^{2}\right\Vert \leq\left\Vert S\right\Vert ^{2}\leq\left(1-\varphi\right)^{2}
\]


\end{proof} 

The approximation of the log-determinant is performed by computing
sequences of power series $\left(R^{k}x\right)_{k}$. These chains
are computed approximately by repeated applications of the $R$ operator
on the previous element of the chain, starting from a Gaussian white
noise element $x_{0}\sim\mathcal{N}\left(0,\mathbf{I}\right)$. We
formalize the notion of an approximate chain. 

\begin{definition} \emph{Approximate power sequence. }Given a linear
operator $R$, a start point $x^{\left(0\right)}\in\mathbb{R}^{n}$,
and a positive-definite matrix $B$, we define an $\epsilon-$approximate
power sequence as a sequence that does not deviate too much from the
power sequence: 
\[
\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}
\]


\end{definition}

We now prove the following result that is quite intuitive: if the
operator $R$ is a contraction and if the relative error $\epsilon$
is not too great, the sum of all the errors on the chain is bounded. 

\begin{lemma} Given the previous hypothesis, and assuming furthermore
that $\left\Vert R\right\Vert _{B}\leq1-\eta$ and that $2\epsilon\leq\eta\leq1/2$,
the total error is bounded by $\mathcal{O}\left(\eta^{-3}\epsilon\right)$:
\[
\sum_{k=0}^{\infty}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]
\end{lemma}

\begin{proof} Call $\omega_{k}=\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}$
and $\theta_{k}=\left\Vert Rx^{\left(k\right)}\right\Vert _{B}$.
We are going to bound the rate of convergence of these two series.
We have first using triangular inequality on the $B$ norm and then
the definition of the induced matrix norm. 
\begin{eqnarray*}
\theta_{k} & \leq & \left\Vert Rx^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & = & \omega_{k}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}
We now bound the error on the $\omega_{k}$ sequence: 
\begin{eqnarray*}
\omega_{k+1} & = & \left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}+Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \left\Vert Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}+\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\\
 & \leq & \left\Vert R\right\Vert _{B}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}\\
 & = & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\theta_{k}\\
 & \leq & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\left(\omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}\right)\\
 & \leq & \left[\left(1-\eta\right)^{2}+\epsilon\right]\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}


Note that the condition $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$
is equivalent to $\epsilon\leq\eta-\eta^{2}$. We can assume without
loss of generality that $0\leq\eta\leq1/2$. Under this condition,
$\eta/2\leq\eta-\eta^{2}$. So the condition $2\epsilon\leq\eta$
implies $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$ which leads
to: 
\[
\omega_{k+1}\leq\left(1-\eta\right)\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]


By induction, one obtains: 
\[
\omega_{k}\leq\omega_{1}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]
and since $\omega_{1}=\left\Vert x^{\left(1\right)}-Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert R\right\Vert _{B}\left\Vert x^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}$,
we have a final bound that depends on $\epsilon$: 
\[
\omega_{k}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]


This is the sum of two geometric series, which give the bound: 
\[
\sum_{k}\omega_{k}\leq\epsilon\eta^{-1}\left[1+\frac{1}{\eta^{2}\left(1-\eta\right)^{2}}\right]\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]


Since $\eta\leq1/2$, it implies $\left(1-\eta\right)^{-2}\leq4$
and $1\leq\eta^{-2}$, so we can further simplify: 
\[
\sum_{k}\omega_{k}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]


\end{proof}

We can use the bound on the norm of $A$ to compute bound the error
with a preconditioner:

\begin{proposition} \label{prop:estimate-truncated-series}Given
a $\kappa\left(n\right)$-good chain for $A$, a start vector $x$,
one can compute an $\epsilon-$estimate of the truncated series: 
\[
\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B_{1}^{-1}A\right)^{i}xx^{T}\right)
\]
in time $\mathcal{O}\left(lm\sqrt{\kappa\left(n_{1}\right)}\left(\log\epsilon^{-1}+\log\kappa\left(n_{1}\right)+\log\kappa\left(B\right)\right)\right)$.\end{proposition} 

\begin{proof} Consider an $\epsilon-$approximate power sequence
$\left(x^{\left(k\right)}\right)_{k}$ with respect to the operator
$I-B^{-1}A$, and $\hat{z}$ the truncated sequence: 
\[
\hat{z}=\sum_{k=1}^{l}\frac{1}{k}\left(x^{\left(0\right)}\right)^{T}x^{\left(k\right)}
\]
This sequence is an approximation of the exact sequence $z$: 
\[
z=\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B^{-1}A\right)^{i}x^{\left(0\right)}\left(x^{\left(0\right)}\right)^{T}\right)
\]
We now bound the error between the two sequences: 
\[
\left|\hat{z}-z\right|\leq\sum_{k=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|
\]
Using the Cauchy-Schwartz inequality, we obtain: 
\[
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{k}x_{0}-x_{k}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}
\]
so that we can bound the deviation: 
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{k=1}^{l}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}\leq4\epsilon\kappa^{3}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$. Now,
we can call the ST-solver routine with $\tilde{\epsilon}=\kappa^{-3}\kappa\left(B\right)^{-1}\epsilon$,
the cost of each call to the operator $R$ being then $\mathcal{O}\left(m\sqrt{\kappa}\left(\log\epsilon^{-1}+\log\kappa+\log\kappa\left(B\right)\right)\right)$.
\end{proof}
