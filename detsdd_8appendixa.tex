
\section*{Appendix A: Proofs of Section 2}


\subsection{Proof of Theorem \ref{thm:det-sampling-theorem}}

\label{sub:det-sampling-proof}

\begin{proof} The proof of this theorem follows the proof of the
Main Theorem in \cite{Barry1999} with some slight modifications.
Using triangular inequality: 
\[
\left|y-\hat{y}_{p,l}\right|\leq\left|\mathbb{E}\left[\hat{y}_{p,l}\right]-\hat{y}_{p,l}\right|+\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right|
\]


Since $S$ is upper-bounded by $\left(1-\delta\right)I$, we have
for all $k\in\mathbb{N}$: 
\[
\left|\mbox{Tr}\left(S^{k}\right)\right|\leq n\left(1-\delta\right)^{k}
\]
Using again triangle inequality, we can bound the error with respect
to the expected value: 
\begin{eqnarray*}
\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right| & = & n^{-1}\left|\sum_{i=l+1}^{\infty}\frac{\left(-1\right)^{i}}{i}\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & n^{-1}\sum_{i=l+1}^{\infty}\frac{1}{i}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{n\left(l+1\right)}\sum_{i=l+1}^{\infty}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{l+1}\sum_{i=l+1}^{\infty}\left(1-\delta\right)^{k}\\
 & \leq & \frac{1}{l+1}\frac{\left(1-\delta\right)^{l+1}}{\delta}\\
 & \leq & \frac{\left(1-\delta\right)^{l+1}}{\delta}
\end{eqnarray*}


And since $\delta\le-\log\left(1-\delta\right)$, for a choice of
$l\geq\delta^{-1}\log\left(\frac{2}{\epsilon\delta}\right)$, the
latter part is less than $\epsilon/2$. We now bound the first part
using Lemma \ref{lem:bernstein-trace}. Call $H$ the truncated series:
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}
\]
This truncated series is upper-bounded by $0$ ($H$ is negative,
semi-definite). The lowest eigenvalue of the truncated series can
be lower-bounded in terms of $\delta$: 
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}\succeq-\sum_{i=1}^{m}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I=\left(\log\delta\right)I
\]
We can now invoke Lemma \ref{lem:bernstein-trace} to conclude: 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\left(\mathbf{u}_{i}^{T}\mathbf{u}_{i}\right)^{-1}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}-n^{-1}\mbox{Tr}\left(H\right)\right|\geq\frac{\epsilon}{2}\right]\leq\exp\left(-\frac{p\epsilon^{2}}{8n^{-1}\left(\log\left(1/\delta\right)\right)^{2}+4\log\left(1/\delta\right)\epsilon}\right)
\]
Thus, any choice of 
\[
p\geq8\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(\eta^{-1}\right)\log^{2}\left(\delta^{-1}\right)\geq4\log\left(\eta^{-1}\right)\epsilon^{-2}\left(2n^{-1}\log^{2}\left(\delta^{-1}\right)+\epsilon\log\left(\delta^{-1}\right)\right)
\]
satisfies the inequality:$\exp\left(-\frac{p\epsilon^{2}}{8n^{-1}\left(\log\delta\right)^{2}+4\log\left(1/\delta\right)\epsilon}\right)\leq\eta$.

\end{proof}


\subsection{Proof of Corollary\ref{cor:preconditioning}}

\begin{proof} We introduce some notations that will prove useful
for the rest of the article: 
\[
H=I-B^{-1}A
\]
\[
S=I-B^{-1/2}AB^{-1/2}
\]
with $B^{-1/2}$ the inverse of the square root of the positive-definite
matrix $B$%
\footnote{Given a real PSD matrix $X$, which can be diagonalized: $X=Q\Delta Q^{T}$
with $\Delta$ diagonal, and $\Delta_{ii}\geq0$. Call $Y=Q\sqrt{\Delta}Q^{T}$
the square root of $X$, then $Y^{2}=X$.%
}. The inequality \ref{eq:A-B-bounds} is equivalent to $\kappa^{-1}B\preceq A\preceq B$,
or also: 
\[
\left(1-\kappa^{-1}\right)I\succeq I-B^{-1/2}AB^{-1/2}\succeq0
\]
\[
\left(1-\kappa^{-1}\right)I\succeq S\succeq0
\]


The matrix $S$ is a contraction, and its spectral radius is determined
by $\kappa$. Furthermore, computing the determinant of $B^{-1}A$
is equivalent to computing the determinant of $I-S$: 
\begin{eqnarray*}
\log\left|I-S\right| & = & \log\left|B^{-1/2}AB^{-1/2}\right|\\
 & = & \log\left|A\right|-\log\left|B\right|\\
 & = & \log\left|B^{-1}A\right|\\
 & = & \log\left|I-H\right|
\end{eqnarray*}


and invoking Theorem \ref{thm:det-sampling-theorem} gives us bounds
on the number of calls to matrix-vector multiplies with respect to
$S$. It would seem at this point that computing the inverse square
root of $B$ is required, undermining our effort. However, we can
reorganize the terms in the series expansion to yield only full inverses
of $B$. Indeed, given $l\in\mathbb{N}^{*}$, consider the truncated
series: 
\begin{eqnarray*}
y_{l} & = & -\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(H^{i}\right)
\end{eqnarray*}
Hence, the practical computation of the latter sum can be done on
$A^{-1}B$. To conclude, if we compute $p=8\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(n\right)\log^{2}\left(\kappa\right)$
truncated chains of length $l=2\kappa\log\left(\frac{n}{\kappa\epsilon}\right)$,
we get our result. This requires $lp$ multiplies by $A$ and inversions
by $B$. \end{proof} 


\subsection{Proof of Theorem \ref{thm:preconditioning-approx}}

We prove here the main result of Section \ref{sec:Preconditioned-log-determinants}.
In the following, $A$ and $B$ are positive-definite matrices in
$\mathcal{S}_{n}$, and $B$ is a $\kappa-$approximation of $A$
($A\preceq B\preceq\kappa A$). The following notations will prove
useful:

\begin{equation}
S=I-B^{-1/2}AB^{-1/2}\label{eq:S-def}
\end{equation}


\begin{equation}
R=I-B^{-1}A\label{eq:R-def}
\end{equation}


\[
\varphi=\kappa^{-1}
\]


\begin{lemma}\label{lem:S-R-contractions} $S$ and $R$ are contractions
for the Euclidian and $B-$norms: 
\begin{eqnarray*}
\left\Vert S\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert _{B} & \leq & \left(1-\varphi\right)^{2}
\end{eqnarray*}
\end{lemma} 

\begin{proof} Recall the definition of the matrix norm:$\left\Vert S\right\Vert =\max_{x^{T}x\leq1}\sqrt{x^{T}Sx}$.
Since we know that $S\preceq\left(1-\varphi\right)I$, we get the
first inequality.

The second inequality is a consequence of Proposition 3.3 from \cite{Spielman2009a}:
$A$ and $B$ have the same nullspace and we have the linear matrix
inequality $A\preceq B\preceq\kappa A$, which implies that the eigenvalues
of $B^{-1}A$ lie between $\kappa^{-1}=\varphi$ and $1$. This implies
that the eigenvalues of $I-B^{-1}A$ are between $1-\varphi$ and
$0$.

Recall the definition of the matrix norm induced by the $B$-norm
over $\mathbb{R}^{n}$: 
\begin{eqnarray*}
\left\Vert R\right\Vert _{B} & = & \max_{x\neq0}\frac{\left\Vert Rx\right\Vert _{B}}{\left\Vert x\right\Vert _{B}}\\
 & = & \max_{\left\Vert x\right\Vert _{B}^{2}\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{x^{T}Bx\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{y^{T}y\leq1}\sqrt{y^{T}B^{-1/2}R^{T}BRB^{-1/2}y}
\end{eqnarray*}
and the latter expression simplifies: 
\begin{eqnarray*}
B^{-1/2}R^{T}BRB^{-1/2} & = & B^{-1/2}\left(I-AB^{-1}\right)B\left(I-B^{-1}A\right)B^{-1/2}\\
 & = & \left(I-B^{-1/2}AB^{-1/2}\right)\left(I-B^{-1/2}AB^{-1/2}\right)\\
 & = & S^{2}
\end{eqnarray*}
so we get: 
\[
\left\Vert R\right\Vert _{B}=\left\Vert S^{2}\right\Vert \leq\left\Vert S\right\Vert ^{2}\leq\left(1-\varphi\right)^{2}
\]


\end{proof} 

The approximation of the log-determinant is performed by computing
sequences of power series $\left(R^{k}x\right)_{k}$. These chains
are computed approximately by repeated applications of the $R$ operator
on the previous element of the chain, starting from a Gaussian white
noise element $x_{0}\sim\mathcal{N}\left(0,\mathbf{I}\right)$. We
formalize the notion of an approximate chain. 

\begin{definition} \emph{Approximate power sequence. }Given a linear
operator $H$, a start point $x^{\left(0\right)}\in\mathbb{R}^{n}$,
and a positive-definite matrix $D$, we define an $\epsilon-$approximate
power sequence as a sequence that does not deviate too much from the
power sequence: 
\[
\left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}\right\Vert _{D}\leq\epsilon\left\Vert Hx^{\left(k\right)}\right\Vert _{D}
\]


\end{definition}

We now prove the following result that is quite intuitive: if the
operator $H$ is a contraction and if the relative error $\epsilon$
is not too great, the sum of all the errors on the chain is bounded. 

\begin{lemma} Given the previous hypothesis, and assuming furthermore
that $\left\Vert H\right\Vert _{D}\leq1-\eta$ and that $2\epsilon\leq\eta\leq1/2$,
the total error is bounded by $\mathcal{O}\left(\eta^{-3}\epsilon\right)$:
\[
\sum_{k=0}^{\infty}\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]
\end{lemma}

\begin{proof} Call $\omega_{k}=\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}$
and $\theta_{k}=\left\Vert Hx^{\left(k\right)}\right\Vert _{D}$.
We are going to bound the rate of convergence of these two series.
We have first using triangular inequality on the $D$ norm and then
the definition of the induced matrix norm. 
\begin{eqnarray*}
\theta_{k} & \leq & \left\Vert Hx^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}+\left\Vert H^{k}x^{\left(0\right)}\right\Vert _{D}\\
 & = & \omega_{k}+\left\Vert H^{k}x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \omega_{k}+\left\Vert H\right\Vert _{D}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\end{eqnarray*}
We now bound the error on the $\omega_{k}$ sequence: 
\begin{eqnarray*}
\omega_{k+1} & = & \left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}+Hx^{\left(k\right)}-H^{k+1}x^{\left(0\right)}\right\Vert _{D}\\
 & \leq & \left\Vert Hx^{\left(k\right)}-H^{k+1}x^{\left(0\right)}\right\Vert _{D}+\left\Vert x^{\left(k+1\right)}-Hx^{\left(k\right)}\right\Vert _{D}\\
 & \leq & \left\Vert H\right\Vert _{D}\left\Vert x^{\left(k\right)}-H^{k}x^{\left(0\right)}\right\Vert _{D}+\epsilon\left\Vert Hx^{\left(k\right)}\right\Vert _{D}\\
 & = & \left\Vert H\right\Vert _{D}\omega_{k}+\epsilon\theta_{k}\\
 & \leq & \left\Vert H\right\Vert _{D}\omega_{k}+\epsilon\left(\omega_{k}+\left\Vert H\right\Vert _{D}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{D}\right)\\
 & \leq & \left[\left(1-\eta\right)^{2}+\epsilon\right]\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\end{eqnarray*}


Note that the condition $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$
is equivalent to $\epsilon\leq\eta-\eta^{2}$. We can assume without
loss of generality that $0\leq\eta\leq1/2$. Under this condition,
$\eta/2\leq\eta-\eta^{2}$. So the condition $2\epsilon\leq\eta$
implies $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$ which leads
to: 
\[
\omega_{k+1}\leq\left(1-\eta\right)\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]


By induction, one obtains: 
\[
\omega_{k}\leq\omega_{1}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{D}}{\eta}\left(1-\eta\right)^{2k-1}
\]
and since $\omega_{1}=\left\Vert x^{\left(1\right)}-Hx^{\left(0\right)}\right\Vert _{D}\leq\epsilon\left\Vert Hx^{\left(0\right)}\right\Vert _{D}\leq\epsilon\left\Vert H\right\Vert _{D}\left\Vert x^{\left(0\right)}\right\Vert _{D}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{D}$,
we have a final bound that depends on $\epsilon$: 
\[
\omega_{k}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{D}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{D}}{\eta}\left(1-\eta\right)^{2k-1}
\]


This is the sum of two geometric series, which give the bound: 
\[
\sum_{k}\omega_{k}\leq\epsilon\eta^{-1}\left[1+\frac{1}{\eta^{2}\left(1-\eta\right)^{2}}\right]\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]


Since $\eta\leq1/2$, it implies $\left(1-\eta\right)^{-2}\leq4$
and $1\leq\eta^{-2}$, so we can further simplify: 
\[
\sum_{k}\omega_{k}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{D}
\]


\end{proof}

We can use the bound on the norm of $A$ to compute bound the error
with a preconditioner:

\begin{lemma}\label{lem:partial-sequence-approximate}Consider $A,B$
with the same hypothesis as above, $x_{0}\in\mathbb{R}^{n}$, $\epsilon\in\left(0,\varphi/2\right)$
and $\left(x_{u}\right)_{u}$ an $\epsilon-$approximate power sequence
for the operator $R$ with start vector $x_{0}$. Then:
\[
\left|\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}R^{i}x_{0}-\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}x_{i}\right|\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]
 where $\kappa\left(B\right)$ is the condition number of $B$.

\end{lemma}

\begin{proof}

Call $\hat{z}$ the truncated sequence: 
\[
\hat{z}=\sum_{i=1}^{l}\frac{1}{i}\left(x_{0}\right)^{T}x_{i}
\]
This sequence is an approximation of the exact sequence $z$: 
\[
z=\sum_{i=1}^{l}\frac{1}{i}x_{0}^{T}R^{i}x_{0}
\]
We now bound the error between the two sequences: 
\[
\left|\hat{z}-z\right|\leq\sum_{i=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{i}x_{0}-x_{i}\right)\right|\leq\sum_{i=1}^{l}\left|x_{0}^{T}\left(R^{i}x_{0}-x_{i}\right)\right|\leq\sum_{i=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{i}x_{0}-x_{i}\right)\right|
\]
Using the Cauchy-Schwartz inequality, we obtain: 
\[
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{i}x_{0}-x_{i}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{i}x_{0}-x_{i}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}
\]
so that we can bound the deviation: 
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{i=1}^{l}\left\Vert R^{i}x_{0}-x_{i}\right\Vert _{B}\leq4\epsilon\kappa^{3}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]
where $\kappa\left(B\right)$ is the condition number of $B$. \end{proof}

We now have all the elements required for the proof of Theorem \ref{thm:preconditioning-approx}.

\begin{proof}

Consider $\mathbf{u}_{j}\sim\mathcal{N}\left(0,I_{n}\right)$ for
$j=1\cdots p$, and $x_{i,j}=\begin{cases}
\mathbf{u}_{j} & i=0\\
x_{i-1,j}-C\left(Ax_{i-1,j}\right) & i>0
\end{cases}$

Call 
\[
z_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{i=1}^{l}\frac{1}{i}\frac{\left(x_{0,j}\right)^{T}x_{i,j}}{x_{0,j}^{T}x_{0,j}}
\]
\[
\hat{y}_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\frac{\mathbf{u}_{j}^{T}S^{k}\mathbf{u}_{j}}{\mathbf{u}_{j}^{T}\mathbf{u}_{j}}
\]


By construction, $\left(x_{i,j}\right)_{i}$ is an $\epsilon-$approximate
chain for the operator $R$. Applying Lemma \ref{lem:partial-sequence-approximate},
we get:
\begin{eqnarray*}
\left|z_{p,l}-\hat{y}_{p,l}\right| & \leq & 4\epsilon\kappa^{3}\kappa\left(B\right)\left[\frac{1}{p}\sum_{j=1}^{p}\left\Vert \mathbf{u}_{j}\right\Vert ^{2}\right]
\end{eqnarray*}


Call $\beta=4p^{-1}\kappa^{3}\kappa\left(B\right)$While this bound
depends on the (random) value of the squared norms of the start vectors,
it is enough to bound this term with high probability using a Chebyshev
bound. Call $Z=\sum_{j=1}^{p}\left\Vert \mathbf{u}_{j}\right\Vert ^{2}$
and $Y=\epsilon\beta Z$. The random variable $Z$ is a sum of square
norms of $np$ independent scaled, centered Gaussian distributions,
hence it follows a Chi-square distribution with $np$ degrees of freedom.
We have $\mathbb{E}\left[Z\right]=np$ and $\text{Cov}\left[Z\right]=2np$,
so $\mathbb{P}\left[\left|Z-np\right|\geq\mu\right]\leq2\frac{np}{\mu^{2}}$.

Assume $\epsilon\leq\min\left(\frac{1}{4\kappa^{3}\kappa\left(B\right)}\frac{\mu}{n},\frac{1}{4\kappa^{3}\kappa\left(B\right)}\frac{\mu}{n\sqrt{p}}\eta,\frac{1}{2\kappa}\right)$.
Then $\mathbb{E}\left[Y\right]=4p^{-1}\epsilon\kappa^{3}\kappa\left(B\right)np\leq\mu$,
which means that 
\begin{eqnarray*}
\mathbb{P}\left[Y\geq\mu\right] & \leq & \mathbb{P}\left[\left|Y-\mathbb{E}\left[Y\right]\right|\geq\mu\right]\\
 & \leq & \frac{\text{Var}\left(Y\right)}{\mu^{2}}\\
 & = & \epsilon^{2}\frac{\beta^{2}np}{\mu^{2}}
\end{eqnarray*}
and our choice of $\epsilon$ ensures that $\epsilon^{2}\frac{\beta^{2}np}{\mu^{2}}\leq\eta$.
Finally, we can simplify the bound on $\epsilon$ since $\frac{\eta}{\sqrt{p}}\le1$.

Thus, we have $\mathbb{P}\left[\left|z_{p,l}-\hat{y}_{p,l}\right|\geq\mu\right]\leq\mathbb{P}\left[Y\geq\mu\right]\leq\eta$

\end{proof}
