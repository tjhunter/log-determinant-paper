#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass siamltex
\begin_preamble
\include{amsmath}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Computing the log-determinant of symmetric, diagonally dominant matrices
 in near-linear time
\begin_inset Newline newline
\end_inset

WORKING DRAFT
\end_layout

\begin_layout Author
Timothy Hunter, Ahmed El-Alaoui, Alexandre Bayen
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
We consider the problem of computing the determinant of symmetric, diagonally
 dominant (SDD) matrices, i.e.
 real symmetric matrices 
\begin_inset Formula $A$
\end_inset

 for which:
\begin_inset Formula 
\[
A_{ii}\geq\sum_{j\neq i}\left|A_{ij}\right|
\]

\end_inset

The set of all such matrices of size 
\begin_inset Formula $n\times n$
\end_inset

 is denoted 
\begin_inset Formula $SDD_{n}$
\end_inset

, and the set of all symmetric real matrices is called 
\begin_inset Formula $\mathcal{S}_{n}$
\end_inset

.
 Call 
\begin_inset Formula $m$
\end_inset

 the number of non-zero entries in 
\begin_inset Formula $A$
\end_inset

.
 We are interested in computing the determinant of sparse matrices, i.e.
 matrices for which 
\begin_inset Formula $m\ll n^{2}$
\end_inset

.
\end_layout

\begin_layout Standard
The best algorithm known for computing the determinant of general matrices,
 the Cholesky factorization, runs in a cubic complexity 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

.
 Computing the factorization can be sped up for a few specific patterns
 such as trees, but no algorithm has been shown to work in a generic way
 for 
\begin_inset Formula $SDD_{n}$
\end_inset

, let alone general symmetric matrices.
 We present an algorithm that returns an approximation of the logarithm
 of the determinant in time quasi-linear with the number of non-zero entries
 of 
\begin_inset Formula $A$
\end_inset

.
 More specifically, we show that our algorithm, 
\family typewriter
UltraLogDet
\family default
, computes an 
\begin_inset Formula $\epsilon$
\end_inset

-approximation of the logarithm of the determinant in expected time
\begin_inset Foot
status open

\begin_layout Plain Layout
We use the notation 
\begin_inset Formula $\tilde{O}$
\end_inset

 to hide a factor at most 
\begin_inset Formula $\left(\log\log n\right)^{8}$
\end_inset


\end_layout

\end_inset

:
\begin_inset Formula 
\[
\tilde{\mathcal{O}}\left(m\left(\log n\right)^{7}\epsilon^{-2}\log\left(\frac{n\kappa_{A}}{\epsilon}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $\kappa_{A}$
\end_inset

 is the condition number of 
\begin_inset Formula $A$
\end_inset

.
 This algorithm builds upon the work of Spielmann and Teng on 
\emph on
ultra-sparsifiers
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

, and it critically exploits the recent improvements from Koutis, Miller
 and Peng 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

.
 This is to our knowledge the first algorithm that presents a nearly linear
 complexity which depends neither on the condition number of 
\begin_inset Formula $A$
\end_inset

 (except through a log-term) nor on a specific pattern for the non-zero
 coefficients of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
The high complexity of the algorithm transpires through the large exponent
 of 
\begin_inset Formula $\log n$
\end_inset

.
 However, our algorithm will directly benefit from any improvement on ultra-spar
sifiers.
 Given the considerable practical importance of such preconditioners, we
 expect some fast improvements in this area.
 Also, the bulk of the work is performed in a Monte Carlo procedure that
 is straightforward to parallelize.
 Furthermore, we also present simpler, non-optimal algorithms that compute
 upper and lower bounds of the logarithm of the determinant, and that may
 be of more immediate practical interest.
\end_layout

\begin_layout Standard

\series bold
Background
\series default
 There are two approaches in numerical linear algebra to compute approximately
 a determinant (or the log of the determinant): by performing a (partial)
 Cholesky factorization of 
\begin_inset Formula $A$
\end_inset

, or by considering the trace of some power series.
\end_layout

\begin_layout Standard
As mentioned above, the Cholesky factorization performs a decomposition
 of the form: 
\begin_inset Formula $A=PLDL^{T}P^{T}$
\end_inset

 with 
\begin_inset Formula $P$
\end_inset

 a permutation matrix, 
\begin_inset Formula $L$
\end_inset

 a low-triangular matrix with 
\begin_inset Formula $1$
\end_inset

 on the diagonal and 
\begin_inset Formula $D$
\end_inset

 a diagonal matrix of non-negative coefficients.
 Then the log-determinant of 
\begin_inset Formula $A$
\end_inset

 is simply
\begin_inset Foot
status open

\begin_layout Plain Layout
We will use the 
\begin_inset Formula $\left|\cdot\right|$
\end_inset

 operator to denote the determinant, it will be clear from the context that
 it is different from the absolute value.
\end_layout

\end_inset

:
\begin_inset Formula 
\[
\log\left|A\right|=\sum_{i}\log D_{ii}
\]

\end_inset

The complexity of dense Cholesky factorization for dense matrices is 
\begin_inset Formula $O\left(n^{3}\right)$
\end_inset

.
 Unfortunately, Cholesky factorization usually does not gain much from the
 knowledge of the sparsity pattern due to the 
\emph on
fill-in problem
\emph default
 (see 
\begin_inset CommandInset citation
LatexCommand cite
key "meurant1999computer"

\end_inset

, section 3.2).
 There is one case, though, for which Cholesky factorization is efficient:
 if the sparsity pattern of 
\begin_inset Formula $A$
\end_inset

 is a tree, then performing Cholesky factorization takes 
\begin_inset Formula $O\left(n\right)$
\end_inset

 time, and the matrix 
\begin_inset Formula $L$
\end_inset

 is a banded matrix 
\begin_inset CommandInset citation
LatexCommand cite
key "liu1990eliminationtrees"

\end_inset

.
 If the sparsity pattern of 
\begin_inset Formula $A$
\end_inset

 is not a tree, however, this advantageous decomposition does not hold anymore.
\end_layout

\begin_layout Standard
When the matrix 
\begin_inset Formula $A$
\end_inset

 is close to the identity, more precisely when the spectral radius of 
\begin_inset Formula $M=A-I$
\end_inset

 is less than
\begin_inset space ~
\end_inset


\begin_inset Formula $1$
\end_inset

, one can use the remarkable Martin expansion of the log-determinant 
\begin_inset CommandInset citation
LatexCommand cite
key "martin1992approximations"

\end_inset

:
\begin_inset Formula 
\begin{equation}
\log\left|A\right|=\text{Tr}\left(\log A\right)\label{eq:martin-expansion}
\end{equation}

\end_inset

 where 
\begin_inset Formula $\log A$
\end_inset

 is the matrix logarithm defined by the series expansion: 
\begin_inset Formula 
\begin{equation}
\log A=\sum_{i=0}^{\infty}\frac{\left(-1\right)^{i}}{i+1}M^{i}\label{eq:matrix-log}
\end{equation}

\end_inset

The determinant can then be computed by a sum of traces of the power of
 
\begin_inset Formula $M$
\end_inset

, and the rate of convergence of this series is driven by the spectral radius
 
\begin_inset Formula $M$
\end_inset

.
 This line of reasoning has lead to looking for decompositions of 
\begin_inset Formula $A$
\end_inset

 of the form 
\begin_inset Formula $A=U+V$
\end_inset

 with the determinant of 
\begin_inset Formula $U$
\end_inset

 being easier to compute and 
\begin_inset Formula $U^{-1}V+I$
\end_inset

 having a small spectral radius.
 Then 
\begin_inset Formula $\log\left|A\right|=\log\left|U\right|+\log\left|U^{-1}V+I\right|$
\end_inset

.
 The most common decomposition 
\begin_inset Formula $U,V$
\end_inset

 is in terms of block diagonal and off-diagonal terms, which can then use
 Hadamard inequalities on the determinant to bound the error 
\begin_inset CommandInset citation
LatexCommand cite
key "Ipsen2006"

\end_inset

.
 Diagonal blocks also have the advantage of having determinants easy to
 compute.
 However, this approach requires some strong assumptions on the condition
 number of 
\begin_inset Formula $A$
\end_inset

, which may not hold in practice.
\end_layout

\begin_layout Standard
The trace approach is driven by 
\emph on
spectral properties 
\emph default
(the condition number) while the Cholesky approach is driven by 
\emph on
graphical 
\emph default
properties
\emph on
 
\emph default
(the non-zero pattern)
\emph on
.
 
\emph default
We
\emph on
 
\emph default
propose to combine these two approaches by decomposing the problem with
 one component that is close to a tree (and is more amenable to Cholesky
 methods), and one component that has a bounded condition number.
 Our solution is to use a 
\emph on
spectral sparsifier
\emph default
 introduced by Spielman in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2008"

\end_inset

.
\end_layout

\begin_layout Standard
The problem of estimating determinants has important applications in spatial
 data analysis, statistical physics and statistics.
 For example, a number of quantum system properties can be inferred by computing
 determinants of large graphs (determinant quantum Monte Carlo).
 This is why computing estimates of the log-determinant has been an active
 problem in physics and statistics.
 In particular, the Martin expansion presented in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:martin-expansion"

\end_inset

 can be combined with sampling method to estimate the trace of a matrix
 series (
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2008"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "McCourt2008"

\end_inset

,
\begin_inset CommandInset citation
LatexCommand cite
key "Zhang2007"

\end_inset

).
 Another different line of research has worked on bounds on the values of
 the determinant itself.
 This is deeply connected to simplifying statistical models using variational
 methods.
 Such a relaxation using a message-passing technique is presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Wainwright2006"

\end_inset

.
 Our method is close in spirit to Reuksen's work 
\begin_inset CommandInset citation
LatexCommand cite
key "Reusken2002"

\end_inset

 by the use of a preconditioner.
 However, Reuksen considers preconditioners based on a clever approximation
 of the Cholesky decomposition, and its interaction with the eigenvalues
 of the complete matrix is not well understood.
 Using simpler methods based on sampling, we are able to carefully control
 the spectrum of the reminder, which in turn leads to strong convergence
 guarantees.
 
\end_layout

\begin_layout Standard
The rest of the article is structured as follows.
 In the next section, we present some results about estimating the log-determina
nt from a truncated expansion.
 These results will justify the use of 
\emph on
preconditioners 
\emph default
to compute the determinant of a matrix.
 The techniques developed by Spielman et al.
 work on the Laplacians of weighted graphs.
 Section 3 introduces some new concepts to expand the notion of determinants
 to Laplacian matrices, and presents a few straightforward results in the
 relations between graph Laplacians and SDD matrices.
 Section 4 will use these new concepts to introduce a first family of preconditi
oners based on low-stretch spanning trees.
 Finally, Section 5 contains the proof of our main result, an algorithm
 to compute determinants in near-linear time.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\align left
Algorithm 
\series bold
UltraLogDet
\series default
(
\begin_inset Formula $A$
\end_inset

,
\begin_inset Formula $\epsilon$
\end_inset

):
\end_layout

\begin_layout Plain Layout
\align left
If 
\begin_inset Formula $A$
\end_inset

 is of a small size (<100), directly compute 
\begin_inset Formula $\text{ld}\left(A\right)$
\end_inset

 with a dense Cholesky factorization.
\end_layout

\begin_layout Plain Layout
\align left
Compute 
\begin_inset Formula $B=$
\end_inset


\series bold
IncrementalSparsify(
\begin_inset Formula $A$
\end_inset

)
\end_layout

\begin_layout Plain Layout
\align left
Compute 
\begin_inset Formula $D,A'=$
\end_inset


\series bold
PartialCholesky(
\begin_inset Formula $B$
\end_inset

)
\end_layout

\begin_layout Plain Layout
\align left
Compute 
\begin_inset Formula $s=$
\end_inset


\series bold
PreconditionedLogDetMonteCarlo(
\begin_inset Formula $B,A,\epsilon$
\end_inset

)
\end_layout

\begin_layout Plain Layout
\align left
Return 
\begin_inset Formula $s+\log\left|D\right|+$
\end_inset


\series bold
UltraLogDet(
\begin_inset Formula $A'$
\end_inset

,
\begin_inset Formula $\epsilon$
\end_inset

)
\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout
The main algorithm
\begin_inset CommandInset label
LatexCommand label
name "alg:The-main-algorithm"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
Algorithm 
\series bold
PreconditionedLogDetMonteCarlo
\series default
(
\begin_inset Formula $B$
\end_inset

,
\begin_inset Formula $A$
\end_inset

,
\begin_inset Formula $\epsilon$
\end_inset

,
\begin_inset Formula $p$
\end_inset

,
\begin_inset Formula $l$
\end_inset

):
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $y\leftarrow0$
\end_inset


\end_layout

\begin_layout Plain Layout
for 
\begin_inset Formula $j$
\end_inset

 from 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $p$
\end_inset

:
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

Sample 
\begin_inset Formula $\mathbf{u}\sim\mathcal{N}\left(\mathbf{0},I\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $\mathbf{v}\leftarrow\mathbf{u}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $z\leftarrow0$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset

for 
\begin_inset Formula $k$
\end_inset

 from 
\begin_inset Formula $1$
\end_inset

 to 
\begin_inset Formula $l$
\end_inset

:
\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $\mathbf{v}\leftarrow B^{-1}A\mathbf{v}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $z\leftarrow z+k^{-1}\mathbf{v}^{T}\mathbf{u}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset space ~
\end_inset


\begin_inset space ~
\end_inset


\begin_inset Formula $y\leftarrow y-p^{-1}z$
\end_inset


\end_layout

\begin_layout Plain Layout
Return 
\begin_inset Formula $y$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\series bold
PreconditionedLogDetMonteCarlo
\series default

\begin_inset CommandInset label
LatexCommand label
name "alg:SampleLogDet"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Results for log-determinants
\end_layout

\begin_layout Standard
We begin by considering a simple sampling algorithm to compute log-determinants,
 presented first in 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

.
 We will first present some error bounds on this algorithm that expand on
 bounds previously presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Bai1996"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

.
\end_layout

\begin_layout Standard
Consider a real symmetric matrix 
\begin_inset Formula $S\in\mathcal{S}_{n}^{+}$
\end_inset

 such that its spectral radius is less than 
\begin_inset Formula $1$
\end_inset

: 
\begin_inset Formula $0\preceq S\preceq\left(1-\delta\right)I$
\end_inset

 for some 
\begin_inset Formula $\delta\in\left(0,1\right)$
\end_inset

.
 We want to compute 
\begin_inset Formula $\log\left|I-S\right|$
\end_inset

 up to precision 
\begin_inset Formula $\epsilon$
\end_inset

 and with high probability.
 From the Martin expansion:
\begin_inset Formula 
\begin{equation}
\log\left|I-S\right|=-\mbox{Tr}\left(\sum_{k=1}^{\infty}\frac{1}{k}S^{k}\right)\label{eq:martin}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This series of traces can be estimated by Monte Carlo sampling, up to precision
 
\begin_inset Formula $\epsilon$
\end_inset

 with high probability.
 In order to bound the errors, we will bound the large deviation errors
 using Hoeffding inequality.
 We use a modified version of Proposition 4.2 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Bai1996"

\end_inset

:
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:hoeffding-trace"

\end_inset

Consider 
\begin_inset Formula $H\in\mathcal{S}_{n}$
\end_inset

 lower- and upper-bounded by 
\begin_inset Formula $\lambda_{\text{min}}$
\end_inset

 and 
\begin_inset Formula $\lambda_{\text{max}}$
\end_inset

: 
\begin_inset Formula $\lambda_{\min}I\preceq H\preceq\lambda_{\max}I$
\end_inset

.
 Consider 
\begin_inset Formula $p$
\end_inset

 vectors sampled from the standard Normal distribution: 
\begin_inset Formula $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
\end_inset

 for 
\begin_inset Formula $i=1\cdots p$
\end_inset

.
 Then for all 
\begin_inset Formula $\nu>0$
\end_inset

:
\begin_inset Formula 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}^{T}-\mbox{Tr}\left(H\right)\right|\geq\frac{\nu}{p}\right]\leq2\exp\left(-\frac{2\nu^{2}}{p\left(\lambda_{\max}-\lambda_{\min}\right)^{2}}\right)
\]

\end_inset


\end_layout

\begin_layout Proof
This is a simple adaptation from 
\begin_inset CommandInset citation
LatexCommand cite
key "Bai1996"

\end_inset

.
\end_layout

\begin_layout Standard
We use the following Bernstein inequality:
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:bernstein"

\end_inset


\begin_inset Formula $X_{1}\cdots X_{U}$
\end_inset

 are independant random variables with 
\begin_inset Formula $\mathbb{E}\left[X_{u}\right]=0$
\end_inset

, 
\begin_inset Formula $\left|X_{u}\right|<c$
\end_inset

.
 Call 
\begin_inset Formula $\sigma^{2}=\frac{1}{U}\sum_{u}\text{Var}\left(X_{u}\right)$
\end_inset

, then for all 
\begin_inset Formula $\epsilon>0$
\end_inset

:
\begin_inset Formula 
\[
\mathbb{P}\left[\frac{1}{U}\sum_{u}X_{u}\geq\epsilon\right]\leq\exp\left(-\frac{U\epsilon^{2}}{2\sigma^{2}+2c\epsilon/3}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The proof of this result can be found in a lecture note from Peter Bartlett's
 class (stat241B/spring 2003).
 We can adapt some results from 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

 to prove this bound on the deviation from the trace.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:bernstein-trace"

\end_inset

Consider 
\begin_inset Formula $H\in\mathcal{S}_{n}$
\end_inset

, and call 
\begin_inset Formula $\lambda_{i}$
\end_inset

 the eigenvalues of 
\begin_inset Formula $H$
\end_inset

.
 Consider 
\begin_inset Formula $p$
\end_inset

 vectors sampled from the standard Normal distribution: 
\begin_inset Formula $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
\end_inset

 for 
\begin_inset Formula $i=1\cdots p$
\end_inset

.
 Then for all 
\begin_inset Formula $\epsilon>0$
\end_inset

:
\begin_inset Formula 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-\frac{1}{n}\mbox{Tr}\left(H\right)\right|\geq\epsilon\right]\leq\exp\left(-\frac{p\epsilon^{2}}{2n^{-1}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}+\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon}\right)
\]

\end_inset

with 
\begin_inset Formula $\lambda_{\text{max}}=\max\lambda_{i}$
\end_inset

 and 
\begin_inset Formula $\lambda_{\min}=\min\lambda_{i}$
\end_inset

.
\end_layout

\begin_layout Proof
The distribution of 
\begin_inset Formula $\mathbf{u}_{i}$
\end_inset

 is invariant through a rotation, so we can consider 
\begin_inset Formula $H$
\end_inset

 diagonal.
 We assume without loss of generality that 
\begin_inset Formula $H=\text{diag}\left(\lambda_{1}...\lambda_{n}\right)$
\end_inset

.
 Again without loss of generality, we assume that 
\begin_inset Formula $\lambda'_{\max}=\lambda_{\max}-\lambda_{\min}$
\end_inset

 and 
\begin_inset Formula $\lambda'_{\min}=0$
\end_inset

 (by considering 
\begin_inset Formula $H'=H-\lambda_{\min}I$
\end_inset

).
 Call 
\begin_inset Formula $V_{i}=\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-n^{-1}\text{Tr}\left(H\right)$
\end_inset

.
 Using results from 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

, we have: 
\begin_inset Formula $\left|V_{i}\right|\leq\lambda_{\max}-\lambda_{\min}$
\end_inset

, 
\begin_inset Formula $\mathbb{E}\left[V_{i}\right]=0$
\end_inset

 and: 
\begin_inset Formula 
\[
\text{Var}\left(V_{i}\right)=\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Proof
Each of the variables 
\begin_inset Formula $V_{i}$
\end_inset

 is independent, so invoking Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:bernstein"

\end_inset

 gives:
\begin_inset Formula 
\[
\mathbb{P}\left[\frac{1}{p}\sum_{i=1}^{p}V_{i}\geq\epsilon\right]\leq\exp\left(-\frac{p\epsilon^{2}}{2\sigma^{2}+2\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon/3}\right)
\]

\end_inset

with 
\begin_inset Formula $\sigma^{2}=\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}$
\end_inset


\end_layout

\begin_layout Standard
The previous lemma shows that if the eigenspectrum of a matrix is bounded,
 we can obtain a Hoeffding bound on the error done by sampling the trace.
 Furthermore, the convergence of the series 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:martin"

\end_inset

 is also determned by the extremal eigenvalues of 
\begin_inset Formula $S$
\end_inset

.
 If we truncate the series (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:martin"

\end_inset

), we can bound the truncation error using the extremal eigenvalues.
 We formalize this intuition in the following theorem, which is adapted
 from the main Theorem in 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

.
 While that main Theorem in 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

 only considered a confidence interval based on the covariance properties
 of Gaussian distribution, we generalize this result to more general Hoeffding
 bounds.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:det-sampling-theorem"

\end_inset

Consider 
\begin_inset Formula $S\in\mathcal{S}_{n}^{+}$
\end_inset

 with 
\begin_inset Formula $0\preceq S\preceq\left(1-\delta\right)I$
\end_inset

 for some 
\begin_inset Formula $\delta\in\left(0,1\right)$
\end_inset

.
 Call 
\begin_inset Formula $y$
\end_inset

 the quantity to estimate: 
\begin_inset Formula 
\[
y=\log\left|I-S\right|
\]

\end_inset

and consider 
\begin_inset Formula $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
\end_inset

 for 
\begin_inset Formula $i=1\cdots p$
\end_inset

.
 Call 
\begin_inset Formula $\hat{y}_{p,l}$
\end_inset

 an estimator of the truncated series of 
\begin_inset Formula $l$
\end_inset

 elements computed by sampling the trace using 
\begin_inset Formula $p$
\end_inset

 samples:
\begin_inset Formula 
\[
\hat{y}_{p,l}=\frac{n}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\frac{\mathbf{u}_{j}^{T}S^{k}\mathbf{u}_{j}}{\mathbf{u}_{j}^{T}\mathbf{u}_{j}}
\]

\end_inset

By choosing 
\begin_inset Formula $p\geq4\log\left(\eta^{-1}\right)\epsilon^{-2}\left(2n^{-1}\log^{2}\left(\delta^{-1}\right)+\epsilon\log\left(\delta^{-1}\right)\right)$
\end_inset

 and 
\begin_inset Formula $l\geq2\delta^{-1}\log\left(\frac{n}{\delta\epsilon}\right)$
\end_inset

, the error probability is small:
\begin_inset Formula 
\[
\mathbb{P}\left[\left|y-\hat{y}_{p,l}\right|\geq n\epsilon\right]\leq\eta
\]

\end_inset


\end_layout

\begin_layout Proof
The proof of this theorem follows the proof of the Main Theorem in 
\begin_inset CommandInset citation
LatexCommand cite
key "Barry1999"

\end_inset

 with some slight modifications.
 Using triangular inequality:
\begin_inset Formula 
\[
\left|y-\hat{y}_{p,l}\right|\leq\left|\mathbb{E}\left[\hat{y}_{p,l}\right]-\hat{y}_{p,l}\right|+\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right|
\]

\end_inset


\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $S$
\end_inset

 is upper-bounded, we have for all 
\begin_inset Formula $k\in\mathbb{N}$
\end_inset

:
\begin_inset Formula 
\[
\left|\mbox{Tr}\left(S^{k}\right)\right|\leq n\left(1-\delta\right)^{k}
\]

\end_inset

Using again triangle inequality, we can bound the error with respect to
 the expected value: 
\begin_inset Formula 
\begin{eqnarray*}
\left|y-\mathbb{E}\left[\hat{y}_{p,l}\right]\right| & = & \left|\sum_{i=l+1}^{\infty}\frac{\left(-1\right)^{i}}{i}\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \sum_{i=l+1}^{\infty}\frac{1}{i}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{1}{l+1}\sum_{i=l+1}^{\infty}\left|\mbox{Tr}\left(S^{k}\right)\right|\\
 & \leq & \frac{n}{l+1}\sum_{i=l+1}^{\infty}\left(1-\delta\right)^{k}\\
 & \leq & \frac{n}{l+1}\frac{\left(1-\delta\right)^{l+1}}{\delta}\\
 & \leq & n\frac{\left(1-\delta\right)^{l+1}}{\delta}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
Hence, for a choice of 
\begin_inset Formula $l\geq\delta^{-1}\log\left(\frac{2n}{\epsilon\delta}\right)$
\end_inset

, the latter part is less than 
\begin_inset Formula $\epsilon/2$
\end_inset

.
 We now bound the first part using Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:bernstein-trace"

\end_inset

.
 Call 
\begin_inset Formula $H$
\end_inset

 the truncated series:
\begin_inset Formula 
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}
\]

\end_inset

This truncated series is upper-bounded by 
\begin_inset Formula $0$
\end_inset

.
 The lowest eigenvalue of the truncated series can be lower-bounded in terms
 of 
\begin_inset Formula $\delta$
\end_inset

:
\begin_inset Formula 
\[
H=-\sum_{i=1}^{m}\frac{1}{i}S^{i}\succeq-\sum_{i=1}^{m}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I\succeq-\sum_{i=1}^{+\infty}\frac{1}{i}\left(1-\delta\right)^{i}I=\left(\log\delta\right)I
\]

\end_inset

We can now invoke Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:bernstein-trace"

\end_inset

 to conclude: 
\begin_inset Formula 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\left(\mathbf{u}_{i}^{T}\mathbf{u}_{i}\right)^{-1}\mathbf{u}_{i}^{T}H\mathbf{u}_{i}-n^{-1}\mbox{Tr}\left(H\right)\right|\geq\frac{\epsilon}{2}\right]\leq\exp\left(-\frac{p\epsilon^{2}}{8n^{-1}\left(\log\delta\right)^{2}+4\log\left(1/\delta\right)\epsilon}\right)
\]

\end_inset

 and 
\begin_inset Formula $\exp\left(-\frac{p\epsilon^{2}}{8n^{-1}\left(\log\delta\right)^{2}+4\log\left(1/\delta\right)\epsilon}\right)\leq\eta$
\end_inset

.
 This latter inequality is verified for any 
\begin_inset Formula 
\[
p\geq4\log\left(\eta^{-1}\right)\epsilon^{-2}\left(2n^{-1}\log^{2}\left(\delta^{-1}\right)+\epsilon\log\left(\delta^{-1}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We immediately get the following result that justifies the notion of preconditio
ners for determinants.
 The corresponding algorithm, which we call 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
, is presented in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:SampleLogDet"

\end_inset

.
\end_layout

\begin_layout Corollary
\begin_inset CommandInset label
LatexCommand label
name "cor:preconditioning"

\end_inset

If 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are positive definite matrices so that 
\begin_inset Formula $B$
\end_inset

 is a 
\begin_inset Formula $\kappa-$
\end_inset

approximation of 
\begin_inset Formula $A$
\end_inset

:
\begin_inset Formula 
\begin{equation}
A\preceq B\preceq\kappa A\label{eq:A-B-bounds}
\end{equation}

\end_inset

then the algorithm 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
 computes an 
\begin_inset Formula $\epsilon$
\end_inset

-approximation of 
\begin_inset Formula $\frac{1}{n}\log\left|B^{-1}A\right|$
\end_inset

 with high probability, by performing 
\begin_inset Formula $O\left(\kappa\epsilon^{-2}\left(\log\kappa\right)^{2}\log\left(\frac{n^{2}\kappa}{\epsilon}\right)\right)$
\end_inset

 vector inversions from 
\begin_inset Formula $B$
\end_inset

 and vector multiplies from 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Proof
We introduce some notations that will prove useful for the rest of the article:
\begin_inset Formula 
\[
H=I-B^{-1}A
\]

\end_inset


\begin_inset Formula 
\[
S=I-B^{-1/2}AB^{-1/2}
\]

\end_inset

with 
\begin_inset Formula $B^{-1/2}$
\end_inset

 the inverse of the square root of the positive-definite matrix 
\begin_inset Formula $B$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
Given a real PSD matrix 
\begin_inset Formula $X$
\end_inset

, which can be diagonalized: 
\begin_inset Formula $X=Q\Delta Q^{T}$
\end_inset

 with 
\begin_inset Formula $\Delta$
\end_inset

 diagonal, and 
\begin_inset Formula $\Delta_{ii}\geq0$
\end_inset

.
 Call 
\begin_inset Formula $Y=Q\sqrt{\Delta}Q^{T}$
\end_inset

 the square root of 
\begin_inset Formula $X$
\end_inset

, then 
\begin_inset Formula $Y^{2}=X$
\end_inset

.
\end_layout

\end_inset

.
 The inequality 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:A-B-bounds"

\end_inset

 is equivalent to 
\begin_inset Formula $\kappa^{-1}B\preceq A\preceq B$
\end_inset

, or also: 
\begin_inset Formula 
\[
\left(1-\kappa^{-1}\right)I\succeq I-B^{-1/2}AB^{-1/2}\succeq0
\]

\end_inset


\begin_inset Formula 
\[
\left(1-\kappa^{-1}\right)I\succeq S\succeq0
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Proof
The matrix 
\begin_inset Formula $S$
\end_inset

 is a contraction, and its spectral radius is determined by 
\begin_inset Formula $\kappa$
\end_inset

.
 Furthermore, computing the determinant of 
\begin_inset Formula $B^{-1}A$
\end_inset

 is equivalent to computing the determinant of 
\begin_inset Formula $I-S$
\end_inset

: 
\begin_inset Formula 
\begin{eqnarray*}
\log\left|I-S\right| & = & \log\left|B^{-1/2}AB^{-1/2}\right|\\
 & = & \log\left|A\right|-\log\left|B\right|\\
 & = & \log\left|B^{-1}A\right|\\
 & = & \log\left|I-H\right|
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
and invoking Theorem 
\begin_inset CommandInset ref
LatexCommand ref
reference "thm:det-sampling-theorem"

\end_inset

 gives us bounds on the number of calls to matrix-vector multiplies with
 respect to 
\begin_inset Formula $S$
\end_inset

.
 It would seem at this point that computing the inverse square root of 
\begin_inset Formula $B$
\end_inset

 is required, undermining our effort.
 However, we can reorganize the terms in the series expansion to yield only
 full inverses of 
\begin_inset Formula $B$
\end_inset

.
 Indeed, given 
\begin_inset Formula $l\in\mathbb{N}^{*}$
\end_inset

, consider the truncated series:
\begin_inset Formula 
\begin{eqnarray*}
y_{l} & = & -\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(S^{i}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1/2}AB^{-1/2}\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\mbox{Tr}\left(\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(\sum_{j}\left(\begin{array}{c}
j\\
i-j
\end{array}\right)\left(B^{-1}A\right)^{j}\right)\\
 & = & -\sum_{i=1}^{l}\frac{1}{i}\mbox{Tr}\left(H^{i}\right)
\end{eqnarray*}

\end_inset

Hence, the practical computation of the latter sum can be done on 
\begin_inset Formula $A^{-1}B$
\end_inset

.
 To conclude, if we compute 
\begin_inset Formula $p=4\epsilon^{-2}\left(\log\kappa\right)^{2}\log n$
\end_inset

 truncated chains of length 
\begin_inset Formula $l=\kappa\left(2\log n+\log\kappa+\log\left(\epsilon^{-1}\right)\right)$
\end_inset

, we get our result.
 This requires 
\begin_inset Formula $lp$
\end_inset

 multiplies by 
\begin_inset Formula $A$
\end_inset

 and inversions by 
\begin_inset Formula $B$
\end_inset

.
\end_layout

\begin_layout Standard
Intuitively, this scheme is useful if 
\begin_inset Formula $B$
\end_inset

 is easy to inverse and has good spectral properties (
\begin_inset Formula $\kappa$
\end_inset

 is small).
 We are going to present some algorithms for a class of matrices (symmetric,
 diagonally dominant matrices) that enjoy such good properties.
\end_layout

\begin_layout Standard
The results above are valid within a scaling of 
\begin_inset Formula $n^{-1}$
\end_inset

, which makes it fall short of our original goal.
 However, we can get a relative error result
\end_layout

\begin_layout Corollary
If 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 are positive definite matrices so that 
\begin_inset Formula $B$
\end_inset

 is a 
\begin_inset Formula $\kappa-$
\end_inset

approximation of 
\begin_inset Formula $A$
\end_inset

:
\begin_inset Formula 
\[
A\preceq B\preceq\kappa A
\]

\end_inset

then the algorithm 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
 computes a scalar 
\begin_inset Formula $\hat{y}$
\end_inset

 so that
\begin_inset Formula 
\[
\mathbb{P}\left[\left|\right|\right]
\]

\end_inset

, by performing 
\begin_inset Formula $O\left(\kappa\epsilon^{-2}\left(\log\kappa\right)^{2}\log\left(\frac{n^{2}\kappa}{\epsilon}\right)\right)$
\end_inset

 vector inversions from 
\begin_inset Formula $B$
\end_inset

 and vector multiplies from 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Section
Making the problem laplacian
\begin_inset CommandInset label
LatexCommand label
name "sec:Making-the-problem"

\end_inset


\end_layout

\begin_layout Standard
From now on, we consider all matrices to be in 
\begin_inset Formula $SDD_{n}$
\end_inset

.
 Most techniques we will be using work on Laplacian matrices instead of
 SDD matrices.
 An SDD matrix is positive semi-definite while a Laplacian matrix is always
 singular, since its nullspace is spanned by 
\begin_inset Formula $\mathbf{1}$
\end_inset

.
 We generalize the definition of the determinant to handle this technicality.
\end_layout

\begin_layout Definition

\emph on
Pseudo-log-determinant (PLD):
\emph default
 Let 
\begin_inset Formula $A\in\mathcal{S}^{n+}$
\end_inset

 be a non-null positive semi-definite matrix.
 The pseudo-log-determinant is defined by the sum over all the eigenvalues
 except the last one:
\begin_inset Formula 
\[
\text{ld}\left(A\right)=\sum_{\lambda_{i}>0}\log\left(\lambda_{i}\right)
\]

\end_inset

where 
\begin_inset Formula $\lambda_{i}$
\end_inset

 are the eigenvalues of 
\begin_inset Formula $A$
\end_inset

.
\end_layout

\begin_layout Standard
The matrix logarithm does not converge for matrices that are not invertible,
 so we need to extend its definition.
 
\end_layout

\begin_layout Definition

\emph on
Pseudo-matrix logarithm.

\emph default
 Consider 
\begin_inset Formula $A\in\mathcal{S}_{n}^{+}$
\end_inset

so that 
\begin_inset Formula $0\preceq A\prec2$
\end_inset

.
 The pseudo-logarithm of a matrix is defined by the projection of the matrix
 logarithm onto the span of 
\begin_inset Formula $A$
\end_inset

:
\begin_inset Formula 
\[
\log_{P}^{+}\left(A\right)=\sum_{k}\frac{1}{k}P\left(I-A\right)^{k}P^{T}
\]

\end_inset

with 
\begin_inset Formula $P$
\end_inset

 an orthonormal projector onto 
\begin_inset Formula $\text{span}\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Standard
The interest of the PLD lies in the connection between SDD matrices and
 their Laplacian.
 Recall that a Laplacian matrix can be constructed from any SDD matrix by
 adding a 
\begin_inset Quotes eld
\end_inset

ground
\begin_inset Quotes erd
\end_inset

 node (see Gremban's thesis 
\begin_inset CommandInset citation
LatexCommand cite
key "Gremban1996"

\end_inset

, Chapter 4).
\end_layout

\begin_layout Definition

\emph on
Augmented Laplacian of an SDD matrix:
\emph default
 Let 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

.
 Let 
\begin_inset Formula $L,D$
\end_inset

 be the decomposition of 
\begin_inset Formula $A$
\end_inset

 into 
\begin_inset Formula $L$
\end_inset

 a Laplacian matrix and 
\begin_inset Formula $D$
\end_inset

 a (non-negative) diagonal matrix such that 
\begin_inset Formula $A=L+D$
\end_inset

.
 The augmented matrix of 
\begin_inset Formula $A$
\end_inset

 is:
\begin_inset Formula 
\[
L_{A}=\left(\begin{array}{cc}
A & -\mathbf{d}\\
-\mathbf{d}^{T} & h
\end{array}\right)
\]

\end_inset

with 
\begin_inset Formula $\mathbf{d}\in\mathbb{R}^{n}$
\end_inset

: 
\begin_inset Formula $\mathbf{d}_{i}=D_{ii}$
\end_inset

 and 
\begin_inset Formula $h=\sum_{i}D_{ii}$
\end_inset

.
\end_layout

\begin_layout Standard
From now on, given any 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

, we will implicitly define a Laplacian 
\begin_inset Formula $L_{A}$
\end_inset

 associated to it, and call the graph of A the weighted undirected graph
 characterized by 
\begin_inset Formula $L_{A}$
\end_inset

.
 The PLD enjoys some of the usual properties of the log-determinant.
 The following proposition shows the interest of the PLD in the study of
 the Laplacian of a graph.
\end_layout

\begin_layout Proposition

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
\begin_inset CommandInset label
LatexCommand label
name "pro:pld-properties"

\end_inset

Let 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

, and 
\begin_inset Formula $L_{A}$
\end_inset

 its augmented Laplacian.
 Let 
\begin_inset Formula $P=\left(I_{n}\,\mathbf{0}\right)$
\end_inset

 be a projection matrix over the first 
\begin_inset Formula $n$
\end_inset

 columns of 
\begin_inset Formula $L_{A}$
\end_inset

.
 Then:
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $\lambda$
\end_inset

 is an eigenvalue of 
\begin_inset Formula $A$
\end_inset

 with associated eigenvector 
\begin_inset Formula $x$
\end_inset

, it is also an eigenvalue of 
\begin_inset Formula $PL_{A}P^{T}$
\end_inset

 with the same eigenvector
\end_layout

\begin_layout Itemize
\begin_inset Formula $\text{ld}\left(L_{A}\right)=\log\left|A\right|$
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $A,B\in SDD_{n}$
\end_inset

.
 
\begin_inset Formula $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(L_{A}\right)+\text{ld}\left(L_{B}\right)$
\end_inset

 and 
\begin_inset Formula $\text{ld}\left(L_{B}^{+}L_{A}\right)=\text{ld}\left(L_{A}\right)-\text{ld}\left(L_{B}\right)=-\text{ld}\left(L_{A}^{+}L_{B}\right)$
\end_inset

 with 
\begin_inset Formula $L_{A}^{+}$
\end_inset

the pseudo inverse of 
\begin_inset Formula $L_{A}$
\end_inset

.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $A,B\in SDD_{n}$
\end_inset

, then 
\begin_inset Formula $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(\sqrt{L_{A}}L_{B}\sqrt{L_{A}}\right)$
\end_inset

 and 
\begin_inset Formula $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(\left(\sqrt{L_{A}}\right)^{+}L_{B}\left(\sqrt{L_{A}}\right)^{+}\right)=\text{ld}\left(\sqrt{L_{A}^{+}}L_{B}\sqrt{L_{A}^{+}}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $A\in SDD_{n}$
\end_inset

 with 
\begin_inset Formula $\left\Vert I-A\right\Vert <1$
\end_inset

 then 
\begin_inset Formula $\mbox{ld}\left(L_{A}\right)=-\mbox{Tr}\left(\log_{P}^{+}\left(L_{A}\right)\right)=-\mbox{Tr}\left(\log\left(A\right)\right)$
\end_inset

.
\end_layout

\begin_layout Proof
The proofs of this proposition are technical and are contained in Appendix
 A.
\end_layout

\begin_layout Standard
Note in particular that the PLD is well defined over connected graphs.
 From now on, we will consider connected graphs only.
 If graphs have several unconnected components, the log-determinant of the
 corresponding SDD is the sum of the PLDs of the Laplacians of each component.
\end_layout

\begin_layout Lemma
The pseudo-log-determinant of a connected graph is well-defined.
\end_layout

\begin_layout Proof
A connected graph 
\begin_inset Formula $G$
\end_inset

 has non-zero conductance 
\begin_inset Formula $\Phi_{G}$
\end_inset

 and using Cheeger's inequality (see Theorem 4.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2010a"

\end_inset

) 
\begin_inset Formula $\lambda_{2}\geq\Phi_{G}^{2}>0$
\end_inset

.
 Hence 
\begin_inset Formula $\lambda_{i}\geq\lambda_{2}>0$
\end_inset

 for all 
\begin_inset Formula $i\geq2$
\end_inset

 and 
\begin_inset Formula $\text{ld}\left(G\right)\in\mathbb{R}$
\end_inset

.
\end_layout

\begin_layout Standard
From now on, we will focus on computing the PLD of Laplacians.
 Note that the kernel of all Laplacian matrices of connected graphs is precisely
 
\begin_inset Formula $\mathbb{R}\mathbf{1}_{n}$
\end_inset

, so we can use the PLD with pseudo inverses as if we were using the log
 determinant with SDD matrices: Consider a graph 
\begin_inset Formula $G=\left(V,E,w\right)$
\end_inset

, identified with its Laplacian matrix 
\begin_inset Formula $L_{G}$
\end_inset

.
 Consider a weighted subgraph 
\begin_inset Formula $H=\left(V,\tilde{E},\tilde{w}\right)$
\end_inset

 of 
\begin_inset Formula $G$
\end_inset

, for which it is easier to compute the determinant, and that closely approximat
es 
\begin_inset Formula $G$
\end_inset

 in the spectral sense.
 From Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "pro:pld-properties"

\end_inset

:
\begin_inset Formula 
\[
\log\left|G\right|=\text{ld}\left(L_{G}\right)=\text{ld}\left(L_{H}\right)+\text{ld}\left(L_{H}^{+}L_{G}\right)=\log\left|H\right|+\text{ld}\left(L_{H}^{+}L_{G}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
We will see how to adapt Spielman and Teng's remarkable work on 
\emph on
ultra-sparsifiers
\emph default
 to produce good preconditioners 
\begin_inset Formula $H$
\end_inset

 for the determinant.
 In particular, once a good preconditioners is available for 
\begin_inset Formula $G$
\end_inset

, we present an algorithm that computes an upper bound and a lower bound
 of the PLD 
\emph on
for free
\emph default
.
 We will improve this results to produce an algorithm that computes an 
\begin_inset Formula $\epsilon$
\end_inset

-approximation with high probability.
\end_layout

\begin_layout Section
A first preconditioner
\begin_inset CommandInset label
LatexCommand label
name "sec:A-first-preconditioner"

\end_inset


\end_layout

\begin_layout Standard
We now present a first preconditioner that is not optimal, but that will
 motivate our results for stronger preconditioners: a tree that spans the
 graph 
\begin_inset Formula $G$
\end_inset

.
 Every graph has a low-stretch spanning tree, as discovered by Alon et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Alon1995"

\end_inset

.
 The bound of Alon et al.
 was then improved by Abraham et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Abraham2008"

\end_inset

.
 We restate their main result.
\end_layout

\begin_layout Lemma
Consider 
\begin_inset Formula $G$
\end_inset

 a weighted graph.
 There exists a tree 
\begin_inset Formula $T$
\end_inset

 that is a subgraph of 
\begin_inset Formula $G$
\end_inset

 so that:
\begin_inset Formula 
\[
G\preceq T\preceq\kappa G
\]

\end_inset

 with 
\begin_inset Formula $\kappa=Cm\log n\log\log n\left(\log\log\log n\right)^{3}$
\end_inset

 for some constant 
\begin_inset Formula $C>0$
\end_inset

.
\end_layout

\begin_layout Proof
This follows directly from 
\begin_inset CommandInset citation
LatexCommand cite
key "Abraham2008"

\end_inset

.
 
\begin_inset Formula $T$
\end_inset

 is a subgraph of 
\begin_inset Formula $G$
\end_inset

 (with the same weights on the edges), so 
\begin_inset Formula $T\preceq G$
\end_inset

.
 Furthermore, we have 
\begin_inset Formula $T\preceq\text{st}_{G}\left(T\right)G$
\end_inset

 using a result of Boman and Hendrickson 
\begin_inset CommandInset citation
LatexCommand cite
key "Boman2004"

\end_inset

 cited by Spielman in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2010"

\end_inset

.
\end_layout

\begin_layout Standard
Trees enjoy a lot of convenient properties for Gaussian elimination.
 The Cholesky factorization of a tree can be computed in linear time, and
 furthermore this factorization has a linear number of non-zero elements
 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

.
 This factorization can be expressed as:
\begin_inset Formula 
\[
L_{T}=PLDL^{T}P^{T}
\]

\end_inset

 where 
\begin_inset Formula $P$
\end_inset

 is a permutation matrix, 
\begin_inset Formula $L$
\end_inset

 is a lower-triangular matrix with the diagonal being all ones, and 
\begin_inset Formula $D$
\end_inset

 a diagonal matrix in which all the elements but the last one are positive,
 the last element being 
\begin_inset Formula $0$
\end_inset

.
 These well-known facts about trees are presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

.
 Once the Cholesky factorization of the tree is performed, the log-determinant
 of the original graph is an immediate by-product:
\begin_inset Formula 
\[
\log\left|T\right|=\sum_{i=1}^{n-1}\log D_{ii}
\]

\end_inset

 Furthermore, computing 
\begin_inset Formula $L_{T}^{+}x$
\end_inset

 also takes 
\begin_inset Formula $O\left(n\right)$
\end_inset

 computations by forward-backward substitution.
 Applying Corollary 
\begin_inset CommandInset ref
LatexCommand ref
reference "cor:preconditioning"

\end_inset

 gives immediately:
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:PLD-tree"

\end_inset

Let 
\begin_inset Formula $G$
\end_inset

 be a graph with 
\begin_inset Formula $n$
\end_inset

 vertices and 
\begin_inset Formula $m$
\end_inset

 edges.
 Its PLD can be computed up to a precision of 
\begin_inset Formula $\epsilon$
\end_inset

 and with high probability in time 
\begin_inset Formula $\tilde{O}\left(mn\left(\log m\right)^{3}\left(\log n\right)^{2}\frac{\log\epsilon^{-1}}{\epsilon^{2}}\right)$
\end_inset

.
\end_layout

\begin_layout Proof
We assume 
\begin_inset Formula $G$
\end_inset

 is connected, hence 
\begin_inset Formula $m\geq n$
\end_inset

 and 
\begin_inset Formula $\log\kappa=\tilde{\mathcal{O}}\left(\log m\right)$
\end_inset

.
\end_layout

\begin_layout Standard
This bound may be of independent interest since it requires relatively little
 machinery to compute, and it is an improvement already for graphs with
 small vertex degree (
\begin_inset Formula $m=\mathcal{O}\left(n^{1+o\left(1\right)}\right)$
\end_inset

) over the Cholesky factorization of 
\begin_inset Formula $G$
\end_inset

.
\end_layout

\begin_layout Standard
How good is the estimate provided by the tree 
\begin_inset Formula $T$
\end_inset

? Intuitively, this depends on how well the tree 
\begin_inset Formula $T$
\end_inset

 approximates the graph 
\begin_inset Formula $G$
\end_inset

.
 This notion of quality of approximation can be formalized by the notion
 of 
\emph on
stretch
\emph default
.
 
\end_layout

\begin_layout Definition
Stretch of a graph.
 Consider 
\begin_inset Formula $G$
\end_inset

 a weighted graph, and 
\begin_inset Formula $T$
\end_inset

 a spanning tree of 
\begin_inset Formula $G$
\end_inset

.
 The stretch of each edge 
\begin_inset Formula $e$
\end_inset

 from 
\begin_inset Formula $G$
\end_inset

 is defined as 
\begin_inset Formula $\text{st}_{T}\left(e\right)=\omega_{e}\left(\sum_{f\in P}\frac{1}{\omega_{f}}\right)$
\end_inset

 where 
\begin_inset Formula $\omega_{e}$
\end_inset

 is the weight of edge 
\begin_inset Formula $e$
\end_inset

 in 
\begin_inset Formula $G$
\end_inset

, and 
\begin_inset Formula $P$
\end_inset

 is the unique path between the endpoints of 
\begin_inset Formula $e$
\end_inset

 in the tree 
\begin_inset Formula $T$
\end_inset

.
 The stretch of the graph is the sum of the stretches of each edge: 
\begin_inset Formula 
\[
\text{st}_{T}\left(G\right)=\sum_{e\in G}\text{st}_{T}\left(e\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The stretch can be obtained as a by-product of the computation of low-stretch
 spanning trees, of which the construction can be done in 
\begin_inset Formula $O\left(m\log n+n\log^{2}n\right)$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "Abraham2008"

\end_inset

.
 Knowing the stretch gives upper and lower bounds on the value of the log-determ
inant.
\end_layout

\begin_layout Proposition
Stretch bounds on the tree PLD:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\log\left|T\right|+\left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)\geq\log\left|G\right|\geq\log\left|T\right|+\log\left(\text{st}_{T}\left(G\right)-n+2\right)\label{eq:encadrement}
\end{equation}

\end_inset


\end_layout

\begin_layout Proof
This is an application of Jensen's inequality on 
\begin_inset Formula $\text{ld}\left(T^{+}G\right)$
\end_inset

.
 We have 
\begin_inset Formula $\text{ld}G=\text{ld}T+\text{ld}\left(T^{+}G\right)$
\end_inset

 and 
\begin_inset Formula $\text{ld}\left(T^{+}G\right)=\text{ld}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)=\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right)$
\end_inset

 with 
\begin_inset Formula $\sqrt{T}$
\end_inset

 the matrix square root of 
\begin_inset Formula $T$
\end_inset

.
 From Lemma 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:Jensen-inequality-matrix-logarithm"

\end_inset

, we have 
\begin_inset Formula 
\begin{eqnarray*}
\text{Tr}\left(\log^{+}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)\right) & \leq & \left(n-1\right)\log\left(\frac{\text{Tr}\left(\sqrt{T}^{+}G\sqrt{T}^{+}\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{Tr}\left(T^{+}G\right)}{n-1}\right)\\
 & = & \left(n-1\right)\log\left(\frac{\text{st}_{G}\left(T\right)}{n-1}\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
The lower bound is slightly more involved.
 Call 
\begin_inset Formula $\lambda_{i}$
\end_inset

 the positive eigenvalues of 
\begin_inset Formula $\sqrt{T^{+}}G\sqrt{T^{+}}$
\end_inset

 and 
\begin_inset Formula $\sigma=\text{st}_{T}\left(G\right)$
\end_inset

.
 We have 
\begin_inset Formula $1\leq\lambda_{i}\leq\sigma$
\end_inset

 because of the inequality
\begin_inset Formula $T\preceq G\preceq\text{st}_{T}\left(G\right)T$
\end_inset

.
 There are precisely 
\begin_inset Formula $n-1$
\end_inset

 positive eigenvalues 
\begin_inset Formula $\lambda_{i}$
\end_inset

, and we have: 
\begin_inset Formula $\text{ld}\left(T^{+}G\right)=\sum_{i}\log\lambda_{i}$
\end_inset

.
 One can show that 
\begin_inset Formula $\sum_{i}\log\lambda_{i}\geq\log\left(\sigma-n+2\right)$
\end_inset

 by considering the problem of minimizing 
\begin_inset Formula $\sum_{i}\log x_{i}$
\end_inset

 under the constraints 
\begin_inset Formula $\sum_{i}x_{i}=\sigma$
\end_inset

 and 
\begin_inset Formula $x_{i}\geq1$
\end_inset

.
 This bound is tight.
\end_layout

\begin_layout Standard
Intuitively since the stretch of a tree is bounded my 
\begin_inset Formula $O\left(m\log n\right)$
\end_inset

, it means that a low-stretch spanning tree provides an approximation of
 a log-determinant up to a factor of 
\begin_inset Formula $O\left(n\log n\right)$
\end_inset

.
 This could be of interest for nearly-cut graphs, with large condition numbers.
\end_layout

\begin_layout Section
Incremental sparsifiers
\begin_inset CommandInset label
LatexCommand label
name "sec:Incremental-sparsifiers"

\end_inset


\end_layout

\begin_layout Standard
We can do better and achieve near-linear time by using ultra-sparsifiers.
 The main insight of our result is that the class preconditioners presented
 by Spielman and Teng are based on incomplete Cholesky factorization, and
 hence have a determinant that is relatively easy to compute, and that furthermo
re they are excellent spectral preconditioners, so the procedure 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
 is efficient to apply.
 We reintroduce some concepts presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

 to present a self-contained result.
\end_layout

\begin_layout Standard
The central idea is to sample 
\begin_inset Formula $O\left(n\right)$
\end_inset

 edges from the graph 
\begin_inset Formula $A$
\end_inset

, to form a subgraph 
\begin_inset Formula $B$
\end_inset

 that is close to a tree (hence it is easy to compute some partial Cholesky
 factorization), yet it is close to the original 
\begin_inset Formula $A$
\end_inset

 is the spectral sense (
\begin_inset Formula $A\preceq B\preceq\kappa A$
\end_inset

), thanks to the additional edges.
 The partial Cholesky factorization is computed using the 
\family typewriter
GreedyElimination
\family default
 algorithm presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

.
 This section is based on Section 4 of 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

.
 
\end_layout

\begin_layout Standard
Consider a Laplacian matrix 
\begin_inset Formula $G$
\end_inset

.
 There exists an algorithm that computes the partial Cholesky factorization:
\begin_inset Formula 
\[
B=PLCL^{T}P^{T}
\]

\end_inset

 where:
\end_layout

\begin_layout Itemize
\begin_inset Formula $P$
\end_inset

 is a permutation matrix
\end_layout

\begin_layout Itemize
\begin_inset Formula $L$
\end_inset

 is a non-singular, low triangular matrix of the form
\begin_inset Formula 
\[
L=\left(\begin{array}{cc}
L_{1,1} & 0\\
L_{2,1} & I_{n_{1}}
\end{array}\right)
\]

\end_inset

with the diagonal of 
\begin_inset Formula $L_{1,1}$
\end_inset

 being all ones.
\end_layout

\begin_layout Itemize
\begin_inset Formula $C$
\end_inset

 has the form 
\begin_inset Formula 
\[
C=\left(\begin{array}{cc}
D_{n-n_{1}} & 0\\
0 & A_{1}
\end{array}\right)
\]

\end_inset

and every row and column of 
\begin_inset Formula $A_{1}$
\end_inset

 has at least 3 non-zero coefficients.
 Furthermore, 
\begin_inset Formula $A_{1}$
\end_inset

 is itself Laplacian and:
\begin_inset Formula 
\[
\text{ld}\left(B\right)=\sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Thus, the PLD of the original Laplacian 
\begin_inset Formula $A$
\end_inset

 is:
\begin_inset Formula 
\begin{eqnarray*}
\text{ld}\left(A\right) & = & \text{ld}\left(B\right)+\text{ld}\left(B^{+}A\right)\\
 & = & \sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)+\text{ld}\left(B^{+}A\right)
\end{eqnarray*}

\end_inset

We are left with solving a smaller problem 
\begin_inset Formula $A_{1}$
\end_inset

, and approximate the value of 
\begin_inset Formula $\text{ld}\left(B^{+}A\right)$
\end_inset

 using the algorithm 
\family typewriter
SampleLogDet
\family default
.
 ST preconditioners are appealing for this task: they guarantee that 
\begin_inset Formula $A_{1}$
\end_inset

 is substantially smaller than 
\begin_inset Formula $A$
\end_inset

, so the recursion completes in 
\begin_inset Formula $O\left(\log\log n\right)$
\end_inset

 steps.
 Furthermore, computing the vector product 
\begin_inset Formula $B^{+}Ax$
\end_inset

 is itself efficient (in can be done in near-linear time), so the sampling
 algorithm can be run is reasonable time.
 We formalize the notion of chain of preconditioners by reintroducing some
 material from 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

.
\end_layout

\begin_layout Definition
Definition 7.1 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

.
 
\begin_inset Formula $\kappa\left(n\right)$
\end_inset

-good chain.
 
\begin_inset Formula $\mathcal{C}=\left\{ A_{1}=A,B_{1},A_{2},\dots A_{d}\right\} $
\end_inset

 a chain of graphs with 
\begin_inset Formula $n_{i}$
\end_inset

 and 
\begin_inset Formula $m_{i}$
\end_inset

 the number of vertices and edges of 
\begin_inset Formula $A_{i}$
\end_inset

.
 
\begin_inset Formula $\mathcal{C}$
\end_inset

 is 
\begin_inset Formula $\kappa\left(n\right)$
\end_inset

-good for 
\begin_inset Formula $A$
\end_inset

 if:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $A_{i}\preceq B_{i}\preceq\kappa\left(n_{i}\right)A_{i}$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $A_{i+1}$
\end_inset

= GreedyElimination
\begin_inset Formula $\left(B_{i}\right)$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $m_{i}/m_{i+1}\geq c_{r}\sqrt{\kappa\left(n_{i}\right)}$
\end_inset

 for some constant 
\begin_inset Formula $c_{r}$
\end_inset

.
\end_layout

\begin_layout Standard
Good chains exist (Lemma 7.3 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

):
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:good-chain"

\end_inset

Given a graph 
\begin_inset Formula $A$
\end_inset

, 
\family typewriter
BuildChain
\family default

\begin_inset Formula $\left(A,p\right)$
\end_inset

 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

 produces a 
\begin_inset Formula $\tilde{O}\left(\log^{4}n\right)$
\end_inset

-good chain for 
\begin_inset Formula $A$
\end_inset

 with probability 
\begin_inset Formula $1-p$
\end_inset

.
 The algorithm runs in time
\begin_inset Formula 
\[
\tilde{O}\left(\left(m\log n+n\log^{2}n\right)\log\left(1/p\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
These chains furthermore can be used a good preconditioners for conjugate
 gradient and lead to near-linear algorithms for approximate inversion (Lemma
 7.2 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Koutis2010"

\end_inset

)
\end_layout

\begin_layout Lemma
Given a 
\begin_inset Formula $\kappa\left(n\right)$
\end_inset

-good chain for 
\begin_inset Formula $A$
\end_inset

, a vector 
\begin_inset Formula $x$
\end_inset

 such that 
\begin_inset Formula $\left\Vert x-L_{A}^{+}b\right\Vert _{A}<\epsilon\left\Vert L_{A}^{+}b\right\Vert _{A}$
\end_inset

 can be computed in 
\begin_inset Formula $O\left(m_{d}^{3}m_{1}\sqrt{\kappa\left(n_{1}\right)}\log\left(1/\epsilon\right)\right)$
\end_inset


\end_layout

\begin_layout Standard
It should now become clear how we can combine a 
\begin_inset Formula $\kappa\left(n\right)$
\end_inset

-good chain with the Algorithm 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
.
 We start by building a chain.
 The partial Cholesky factorizations provide an upper bound to 
\begin_inset Formula $\mbox{ld}\left(A\right)$
\end_inset

.
 We then refine this upper bound by running 
\family typewriter
PreconditionedLogDetMonteCarlo
\family default
 at each state of the chain to approximate 
\begin_inset Formula $\mbox{ld}\left(B_{i}^{+}A_{i}\right)$
\end_inset

 with high probability.
 The complete algorithm is presented in Algorithm 
\begin_inset CommandInset ref
LatexCommand ref
reference "alg:The-main-algorithm"

\end_inset

.
\end_layout

\begin_layout Standard
We have three sources of errors introduced in our estimate:
\end_layout

\begin_layout Itemize
The truncation of the series of 
\begin_inset Formula $\text{ld}\left(B^{+}A\right)$
\end_inset


\end_layout

\begin_layout Itemize
The sampling error induced by having a finite number of samples when approximati
ng the trace of 
\begin_inset Formula $\text{ld}\left(B^{+}A\right)$
\end_inset


\end_layout

\begin_layout Itemize
The approximate inversion by 
\begin_inset Formula $B$
\end_inset


\end_layout

\begin_layout Standard
The following lemmas show that we can reasonably control the inversion error
 by running the ST-solver at a higher precision 
\begin_inset Formula $O\left(\kappa\left(n_{1}\right)^{-3}\kappa\left(A_{1}\right)^{-1}\epsilon\right)$
\end_inset

.
 The ST-solvers have some convergence guarantees using the matrix norm induced
 by 
\begin_inset Formula $A$
\end_inset

 instead of the regular Euclidian norm.
 The following lemmas deal with this technicality.
 
\end_layout

\begin_layout Lemma
Consider 
\begin_inset Formula $M\in\mathcal{S}_{n}^{+}$
\end_inset

 with 
\begin_inset Formula $\text{Tr}\left(M\right)=n$
\end_inset

 for all 
\begin_inset Formula $i$
\end_inset

.
 Then 
\begin_inset Formula $\log\left|M\right|\leq0$
\end_inset


\end_layout

\begin_layout Proof
This is a simple consequence of the concavity of the logarithm.
 Call 
\begin_inset Formula $\lambda_{i}$
\end_inset

 the eigenvalues of 
\begin_inset Formula $M$
\end_inset

, then 
\begin_inset Formula $\log\left|M\right|=\sum_{i}\log\lambda_{i}$
\end_inset

, and by concavity we have
\begin_inset Formula $n^{-1}\sum_{i}\log\lambda_{i}\leq\log\left(n^{-1}\sum_{i}\lambda_{i}\right)=\log\left(n^{-1}\text{Tr}\left(M\right)\right)=0$
\end_inset

.
\end_layout

\begin_layout Subsubsection*
Proofs of convergence
\end_layout

\begin_layout Standard
We need to prove that the error introduced by approximating the solution
 of the preconditioner does not have too great an impact on the accuracy
 of the solution.
 We use the same notations as above with 
\begin_inset Formula $B$
\end_inset

 a 
\begin_inset Formula $\kappa-$
\end_inset

approximation of 
\begin_inset Formula $A$
\end_inset

 (
\begin_inset Formula $A\preceq B\preceq\kappa A$
\end_inset

), and we call:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S=I-B^{-1/2}AB^{-1/2}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
R=I-B^{-1}A
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\varphi=\kappa^{-1}
\]

\end_inset


\end_layout

\begin_layout Lemma
\begin_inset Formula $S$
\end_inset

 and 
\begin_inset Formula $R$
\end_inset

 are contractions for the Euclidian and 
\begin_inset Formula $B-$
\end_inset

norms:
\begin_inset Formula 
\begin{eqnarray*}
\left\Vert S\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert  & \leq & 1-\varphi\\
\left\Vert R\right\Vert _{B} & \leq & \left(1-\varphi\right)^{2}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
Recall the definition of the matrix norm:
\begin_inset Formula $\left\Vert S\right\Vert =\max_{x^{T}x\leq1}\sqrt{x^{T}Sx}$
\end_inset

.
 Since we know that 
\begin_inset Formula $S\preceq\left(1-\varphi\right)I$
\end_inset

, we get the first inequality.
 
\end_layout

\begin_layout Proof
The second inequality is a consequence of Proposition 3.3 from 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009a"

\end_inset

: 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

 have the same nullspace and we have the LMI 
\begin_inset Formula $A\preceq B\preceq\kappa A$
\end_inset

, which implies that the eigenvalues of 
\begin_inset Formula $B^{-1}A$
\end_inset

 lie between 
\begin_inset Formula $\kappa^{-1}=\varphi$
\end_inset

 and 
\begin_inset Formula $1$
\end_inset

.
 This implies that the eigenvalues of 
\begin_inset Formula $I-B^{-1}A$
\end_inset

 are between 
\begin_inset Formula $1-\varphi$
\end_inset

 and 
\begin_inset Formula $0$
\end_inset

.
\end_layout

\begin_layout Proof
Recall the definition of the matrix norm induced by the 
\begin_inset Formula $B$
\end_inset

-norm over 
\begin_inset Formula $\mathbb{R}^{n}$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
\left\Vert R\right\Vert _{B} & = & \max_{x\neq0}\frac{\left\Vert Rx\right\Vert _{B}}{\left\Vert x\right\Vert _{B}}\\
 & = & \max_{\left\Vert x\right\Vert _{B}^{2}\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{x^{T}Bx\leq1}\sqrt{x^{T}R^{T}BRx}\\
 & = & \max_{y^{T}y\leq1}\sqrt{y^{T}B^{-1/2}R^{T}BRB^{-1/2}y}
\end{eqnarray*}

\end_inset

and the latter expression simplifies: 
\begin_inset Formula 
\begin{eqnarray*}
B^{-1/2}R^{T}BRB^{-1/2} & = & B^{-1/2}\left(I-AB^{-1}\right)B\left(I-B^{-1}A\right)B^{-1/2}\\
 & = & \left(I-B^{-1/2}AB^{-1/2}\right)\left(I-B^{-1/2}AB^{-1/2}\right)\\
 & = & S^{2}
\end{eqnarray*}

\end_inset

so we get:
\begin_inset Formula 
\[
\left\Vert R\right\Vert _{B}=\left\Vert S^{2}\right\Vert \leq\left\Vert S\right\Vert ^{2}\leq\left(1-\varphi\right)^{2}
\]

\end_inset


\end_layout

\begin_layout Standard
The approximation of the log-determinant is performed by computing sequences
 
\begin_inset Formula $\left(R^{k}x\right)_{k}$
\end_inset

.
 These chains are computed approximately by repeated applications of the
 
\begin_inset Formula $R$
\end_inset

 operator on the previous element of the chain, starting from a Gaussian
 white noise element 
\begin_inset Formula $x_{0}\sim\mathcal{N}\left(0,\mathbf{I}\right)$
\end_inset

.
 We formalize the notion of an approximate chain.
 
\end_layout

\begin_layout Definition

\emph on
Approximate power sequence.
 
\emph default
Given a linear operator 
\begin_inset Formula $R$
\end_inset

, a start point 
\begin_inset Formula $x^{\left(0\right)}\in\mathbb{R}^{n}$
\end_inset

, and a positive-definite matrix 
\begin_inset Formula $B$
\end_inset

, we define an 
\begin_inset Formula $\epsilon-$
\end_inset

approximate power sequence as a sequence that does not deviate too much
 from the power sequence:
\begin_inset Formula 
\[
\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}
\]

\end_inset


\end_layout

\begin_layout Definition
We now prove the following result that is quite intuitive: if the operator
 
\begin_inset Formula $R$
\end_inset

 is a contraction and if the relative error 
\begin_inset Formula $\epsilon$
\end_inset

 is not too great, the sum of all the errors on the chain is bounded.
\end_layout

\begin_layout Lemma
Given the previous hypothesis, and assuming furthermore that 
\begin_inset Formula $\left\Vert R\right\Vert _{B}\leq1-\eta$
\end_inset

 and that 
\begin_inset Formula $2\epsilon\leq\eta\leq1/2$
\end_inset

, the total error is bounded by 
\begin_inset Formula $\mathcal{O}\left(\eta^{-3}\epsilon\right)$
\end_inset

:
\begin_inset Formula 
\[
\sum_{k=0}^{\infty}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]

\end_inset


\end_layout

\begin_layout Proof
Call 
\begin_inset Formula $\omega_{k}=\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}$
\end_inset

 and 
\begin_inset Formula $\theta_{k}=\left\Vert Rx^{\left(k\right)}\right\Vert _{B}$
\end_inset

.
 We are going to bound the rate of convergence of these two series.
 We have first using triangular inequality on the 
\begin_inset Formula $B$
\end_inset

 norm and then the definition of the induced matrix norm.
\begin_inset Formula 
\begin{eqnarray*}
\theta_{k} & \leq & \left\Vert Rx^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & = & \omega_{k}+\left\Vert R^{k}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}

\end_inset

We now bound the error on the 
\begin_inset Formula $\omega_{k}$
\end_inset

 sequence:
\begin_inset Formula 
\begin{eqnarray*}
\omega_{k+1} & = & \left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}+Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}\\
 & \leq & \left\Vert Rx^{\left(k\right)}-R^{k+1}x^{\left(0\right)}\right\Vert _{B}+\left\Vert x^{\left(k+1\right)}-Rx^{\left(k\right)}\right\Vert _{B}\\
 & \leq & \left\Vert R\right\Vert _{B}\left\Vert x^{\left(k\right)}-R^{k}x^{\left(0\right)}\right\Vert _{B}+\epsilon\left\Vert Rx^{\left(k\right)}\right\Vert _{B}\\
 & = & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\theta_{k}\\
 & \leq & \left\Vert R\right\Vert _{B}\omega_{k}+\epsilon\left(\omega_{k}+\left\Vert R\right\Vert _{B}^{k}\left\Vert x^{\left(0\right)}\right\Vert _{B}\right)\\
 & \leq & \left[\left(1-\eta\right)^{2}+\epsilon\right]\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Proof
Note that the condition 
\begin_inset Formula $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$
\end_inset

 is equivalent to 
\begin_inset Formula $\epsilon\leq\eta-\eta^{2}$
\end_inset

.
 We can assume without loss of generality that 
\begin_inset Formula $0\leq\eta\leq1/2$
\end_inset

.
 Under this condition, 
\begin_inset Formula $\eta/2\leq\eta-\eta^{2}$
\end_inset

.
 So the condition 
\begin_inset Formula $2\epsilon\leq\eta$
\end_inset

 implies 
\begin_inset Formula $\left(1-\eta\right)^{2}+\epsilon\leq1-\eta$
\end_inset

 which leads to:
\begin_inset Formula 
\[
\omega_{k+1}\leq\left(1-\eta\right)\omega_{k}+\epsilon\left(1-\eta\right)^{2k}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]

\end_inset


\end_layout

\begin_layout Proof
By induction, one obtains:
\begin_inset Formula 
\[
\omega_{k}\leq\omega_{1}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]

\end_inset

and since 
\begin_inset Formula $\omega_{1}=\left\Vert x^{\left(1\right)}-Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert Rx^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left\Vert R\right\Vert _{B}\left\Vert x^{\left(0\right)}\right\Vert _{B}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}$
\end_inset

, we have a final bound that depends on 
\begin_inset Formula $\epsilon$
\end_inset

:
\begin_inset Formula 
\[
\omega_{k}\leq\epsilon\left(1-\eta\right)^{2}\left\Vert x^{\left(0\right)}\right\Vert _{B}\left(1-\eta\right)^{k}+\frac{\epsilon\left\Vert x^{\left(0\right)}\right\Vert _{B}}{\eta}\left(1-\eta\right)^{2k-1}
\]

\end_inset


\end_layout

\begin_layout Proof
This is the sum of two geometric series, which give the bound:
\begin_inset Formula 
\[
\sum_{k}\omega_{k}\leq\epsilon\eta^{-1}\left[1+\frac{1}{\eta^{2}\left(1-\eta\right)^{2}}\right]\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]

\end_inset


\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $\eta\leq1/2$
\end_inset

, it implies 
\begin_inset Formula $\left(1-\eta\right)^{-2}\leq4$
\end_inset

 and 
\begin_inset Formula $1\leq\eta^{-2}$
\end_inset

, so we can further simplify:
\begin_inset Formula 
\[
\sum_{k}\omega_{k}\leq4\epsilon\eta^{-3}\left\Vert x^{\left(0\right)}\right\Vert _{B}
\]

\end_inset


\end_layout

\begin_layout Standard
We can use the bound on the norm of 
\begin_inset Formula $A$
\end_inset

 to compute bound the error with a preconditioner:
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:estimate-truncated-series"

\end_inset

Given a 
\begin_inset Formula $\kappa\left(n\right)$
\end_inset

-good chain for 
\begin_inset Formula $A$
\end_inset

, a start vector 
\begin_inset Formula $x$
\end_inset

, one can compute an 
\begin_inset Formula $\epsilon-$
\end_inset

estimate of the truncated series: 
\begin_inset Formula 
\[
\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B_{1}^{-1}A\right)^{i}xx^{T}\right)
\]

\end_inset

in time 
\begin_inset Formula $\mathcal{O}\left(lm\sqrt{\kappa\left(n_{1}\right)}\left(\log\epsilon^{-1}+\log\kappa\left(n_{1}\right)+\log\kappa\left(B\right)\right)\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Consider an 
\begin_inset Formula $\epsilon-$
\end_inset

approximate power sequence 
\begin_inset Formula $\left(x^{\left(k\right)}\right)_{k}$
\end_inset

 with respect to the operator 
\begin_inset Formula $I-B^{-1}A$
\end_inset

, and 
\begin_inset Formula $\hat{z}$
\end_inset

 the truncated sequence: 
\begin_inset Formula 
\[
\hat{z}=\sum_{k=1}^{l}\frac{1}{k}\left(x^{\left(0\right)}\right)^{T}x^{\left(k\right)}
\]

\end_inset

This sequence is an approximation of the exact sequence 
\begin_inset Formula $z$
\end_inset

:
\begin_inset Formula 
\[
z=\mbox{Tr}\left(\sum_{i=1}^{l}\frac{1}{i}\left(I-B^{-1}A\right)^{i}x^{\left(0\right)}\left(x^{\left(0\right)}\right)^{T}\right)
\]

\end_inset

We now bound the error between the two sequences:
\begin_inset Formula 
\[
\left|\hat{z}-z\right|\leq\sum_{k=1}^{l}\frac{1}{i}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|x_{0}^{T}\left(R^{k}x_{0}-x_{k}\right)\right|\leq\sum_{k=1}^{l}\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|
\]

\end_inset

Using the Cauchy-Schwartz inequality, we obtain:
\begin_inset Formula 
\[
\left|\left(B^{-1}x_{0}\right)^{T}B\left(R^{k}x_{0}-x_{k}\right)\right|=\left|\left\langle B^{-1}x_{0},R^{k}x_{0}-x_{k}\right\rangle _{B}\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}
\]

\end_inset

so that we can bound the deviation:
\begin_inset Formula 
\[
\left|\hat{z}-z\right|\leq\left\Vert B^{-1}x_{0}\right\Vert _{B}\sum_{k=1}^{l}\left\Vert R^{k}x_{0}-x_{k}\right\Vert _{B}\leq4\epsilon\kappa^{3}\left\Vert B^{-1}x_{0}\right\Vert _{B}\left\Vert x_{0}\right\Vert _{B}\leq4\epsilon\kappa^{3}\kappa\left(B\right)\left\Vert x_{0}\right\Vert ^{2}
\]

\end_inset

where 
\begin_inset Formula $\kappa\left(B\right)$
\end_inset

 is the condition number of 
\begin_inset Formula $B$
\end_inset

.
 Now, we can call the ST-solver routine with 
\begin_inset Formula $\tilde{\epsilon}=\kappa^{-3}\kappa\left(B\right)^{-1}\epsilon$
\end_inset

, the cost of each call to the operator 
\begin_inset Formula $R$
\end_inset

 being then 
\begin_inset Formula $\mathcal{O}\left(m\sqrt{\kappa}\left(\log\epsilon^{-1}+\log\kappa+\log\kappa\left(B\right)\right)\right)$
\end_inset

.
 
\end_layout

\begin_layout Standard
We now have all the elements required to prove our main theorem.
 Now we can use the previous proposition to bound the error done during
 the estimation part.
 We have already found some bounds for the estimates of the log-determinant
 of the preconditioners, and some probability estimates for the error bounds
 on the trace series.
\end_layout

\begin_layout Theorem
\begin_inset CommandInset label
LatexCommand label
name "thm:main"

\end_inset

Consider 
\begin_inset Formula $A$
\end_inset

 a symmetric, diagonally dominant matrix of size 
\begin_inset Formula $n\times n$
\end_inset

 with 
\begin_inset Formula $m$
\end_inset

 non-zero entries, and 
\begin_inset Formula $0<\epsilon<\log^{-5}n$
\end_inset

.
 One can compute 
\begin_inset Formula $y\in\mathbb{R}$
\end_inset

 so that 
\begin_inset Formula $\mathbb{P}\left(\left|y-\log\left|A\right|\right|>\epsilon\right)\leq\frac{1}{n}$
\end_inset

 in expected time 
\begin_inset Formula $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\left(\frac{n\kappa\left(A\right)}{\epsilon}\right)\right)\right)$
\end_inset

.
 
\end_layout

\begin_layout Proof
Consider a 
\begin_inset Formula $\tilde{O}\left(\log^{4}n\right)$
\end_inset

-chain 
\begin_inset CommandInset ref
LatexCommand ref
reference "lem:good-chain"

\end_inset

 for the Laplacian of 
\begin_inset Formula $A$
\end_inset

.
 The cost of constructing such a chain is negligible against the sampling
 step.
 From Corollary 
\begin_inset CommandInset ref
LatexCommand ref
reference "cor:preconditioning"

\end_inset

, for each level of the chain, we compute 
\begin_inset Formula $p=\tilde{O}\left(\epsilon^{-2}\log n\right)$
\end_inset

 approximate truncated chains of length 
\begin_inset Formula $l=\tilde{O}\left(\log^{4}n\left(\log n+\log\left(\epsilon^{-1}\right)\right)\right)$
\end_inset

.
 From Proposition 
\begin_inset CommandInset ref
LatexCommand ref
reference "prop:estimate-truncated-series"

\end_inset

, the cost of running each chain is 
\begin_inset Formula $\tilde{O}\left(lm\log^{2}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right)$
\end_inset

.
 Thus the time to approximate the residue PLD at level 
\begin_inset Formula $i$
\end_inset

 is bounded by 
\begin_inset Formula $\tilde{O}\left(m\epsilon^{-2}\log^{7}n\left(\log\epsilon^{-1}+\log n+\log\kappa\left(A\right)\right)\right).$
\end_inset

 Finally, since 
\begin_inset Formula $m_{i}$
\end_inset

 decreases faster than geometrically, the number 
\begin_inset Formula $d$
\end_inset

 of steps in the chain is 
\begin_inset Formula $\tilde{O}\left(1\right)$
\end_inset

.
\end_layout

\begin_layout Section*
Comments
\end_layout

\begin_layout Standard
A first approximation upper-bound of the log-determinant follows immediately
 from the computation of the 
\begin_inset Formula $\kappa-$
\end_inset

good chain.
 We presented in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:A-first-preconditioner"

\end_inset

 a first analysis to bound the value of the residue PLD.
 This analysis was done using trees as preconditioners, it can be carried
 on on more general preconditioners by introducing a generalization of the
 stretch over a subgraph (see the Appendix, section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sub:Stretch-graph"

\end_inset

).
 Since the bulk of the computations are performed in estimating the residue
 PLD, it would be interesting to see if this could be bypassed using better
 bounds based on the stretch.
\end_layout

\begin_layout Standard
When looking at each step of the analysis, one can see that the 
\begin_inset Formula $\epsilon^{-2}$
\end_inset

 factor comes from the approximation of the trace of a matrix by sampling.
 This seems to be a fundamental limitation of this method and it is shared
 with other algorithms that rely on random projections.
 This result is absolute.
 It would be interesting to see if the analysis could be tightened to present
 a relative bound that does not depend on the condition number of 
\begin_inset Formula $A$
\end_inset

.
 Also, even if this algorithm presents a linear bound, it requires a fairly
 advanced machinery (ST solvers) that may limit its practicality.
 Some heuristic implementation, for example based on algebraic multi-grid
 methods, could be a first step in this direction.
\end_layout

\begin_layout Standard
The authors are much indebted to Satish Rao and Jim Demmel for suggesting
 the original idea and their helpful comments on the draft of this article.
\end_layout

\begin_layout Section*
Appendix
\end_layout

\begin_layout Subsection
Pseudo-log-determinant and pseudo-matrix-logarithm
\end_layout

\begin_layout Standard
This section provides the proofs related to the PLD.
 These proofs are elementary and do not provide much insight.
 In this section, we call
\begin_inset Formula $\mathcal{D}_{n}^{+}$
\end_inset

the set of all non-zero diagonalizable matrices in 
\begin_inset Formula $\mathbb{R}^{n\times n}$
\end_inset

 with non-negative eigenvalues.
\end_layout

\begin_layout Lemma
Given two non-empty orthornormal families 
\begin_inset Formula $P$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 that span the same space, 
\begin_inset Formula $P^{T}Q$
\end_inset

 is invertible and we have:
\begin_inset Formula 
\[
\left(P^{T}Q\right)^{-1}=Q^{T}P
\]

\end_inset


\end_layout

\begin_layout Proof
Move the proof here.
\end_layout

\begin_layout Lemma
Given 
\begin_inset Formula $A\in\mathcal{D}_{n}^{+}$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

 an orthonormal basis for 
\begin_inset Formula $\text{span}\left(A\right)$
\end_inset

, we have the relation:
\begin_inset Formula 
\[
\text{ld\left(A\right)}=\log\left|P^{T}AP\right|
\]

\end_inset


\end_layout

\begin_layout Proof
Since 
\begin_inset Formula $A$
\end_inset

 is diagonalizable, consider its decomposition 
\begin_inset Formula $A=Q\Delta Q^{T}$
\end_inset

 with 
\begin_inset Formula $\Delta$
\end_inset

 a diagonal matrix with positive entries on the diagonal.
 Note that 
\begin_inset Formula $\Delta$
\end_inset

 exists since 
\begin_inset Formula $A\neq0$
\end_inset

.
 By definition of the PLD, 
\begin_inset Formula $\text{ld}\left(A\right)=\sum_{i}\log\Delta_{i}$
\end_inset

.
 Since 
\begin_inset Formula $\mathbb{R}^{n}=\text{span}\left(Q\right)\oplus\text{Ker}\left(A\right)=\text{span}\left(P\right)\oplus\text{Ker}\left(A\right)$
\end_inset

, there exists an invertible matrix 
\begin_inset Formula $B$
\end_inset

 such that 
\begin_inset Formula $Q=PB$
\end_inset

.
 Using the fact that 
\begin_inset Formula $P$
\end_inset

 is an orthonormal family, we get 
\begin_inset Formula 
\[
P^{T}Q=P^{T}PB=B
\]

\end_inset

 Furthermore, 
\begin_inset Formula $B$
\end_inset

 is invertible, so 
\begin_inset Formula $QB^{-1}=P$
\end_inset

.
 Using the fact that the eigenvalues make an orthonormal family, we get:
 
\begin_inset Formula $B^{-1}=Q^{T}P$
\end_inset

, hence:
\begin_inset Formula 
\[
\left(P^{T}Q\right)^{-1}=Q^{T}P
\]

\end_inset


\end_layout

\begin_layout Proof
Now we wan conclude:
\begin_inset Formula 
\[
\left|P^{T}AP\right|=\left|P^{T}Q\Delta Q^{T}P\right|=\left|P^{T}Q\right|\left|\Delta\right|\left|Q^{T}P\right|=\left|\Delta\right|
\]

\end_inset

Hence 
\begin_inset Formula $\log\left|P^{T}AP\right|=\sum_{i}\log\Delta_{i}=\text{ld}\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Lemma
Given 
\begin_inset Formula $A\succ0,\, B\succ0$
\end_inset

, then 
\begin_inset Formula $AB$
\end_inset

 is diagonalizable and 
\begin_inset Formula $AB\succ0$
\end_inset

.
 Same thing for 
\begin_inset Formula $A\succeq0$
\end_inset

 and 
\begin_inset Formula $B\succeq0$
\end_inset

.
\end_layout

\begin_layout Proof
The proof of this lemma is in [cite Spielman].
\end_layout

\begin_layout Lemma
Given 
\begin_inset Formula $A,B\in\mathcal{D}_{n}^{+}$
\end_inset

 so that 
\begin_inset Formula $\text{Ker}\left(A\right)=\text{Ker}\left(B\right)$
\end_inset

, then 
\begin_inset Formula $C=AB\in\mathcal{D}_{n}^{+}$
\end_inset

 and 
\begin_inset Formula $\text{Ker}\left(C\right)=\text{Ker}\left(A\right)$
\end_inset

.
\end_layout

\begin_layout Proof
Consider the diagonalization of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

: 
\begin_inset Formula $A=Q_{A}\Delta_{A}Q_{A}^{T}$
\end_inset

 and 
\begin_inset Formula $B=Q_{B}\Delta_{B}Q_{B}^{T}$
\end_inset

, and call 
\begin_inset Formula $D=Q_{A}^{T}Q_{B}$
\end_inset

.
 Since 
\begin_inset Formula $\Delta_{A}D\Delta_{B}$
\end_inset

 is invertible, 
\begin_inset Formula $\text{Ker}\left(C\right)=\text{Ker}\left(A\right)$
\end_inset

.
 Furthermore, consider a non-null eigenvalue 
\begin_inset Formula $u=Q_{A}s$
\end_inset

.
 Then 
\begin_inset Formula $s$
\end_inset

 verifies the equation:
\begin_inset Formula 
\[
\Delta_{A}D\Delta_{B}D^{-1}s=\lambda s
\]

\end_inset

So all the eigenvalues of 
\begin_inset Formula $AB$
\end_inset

 are that of 
\begin_inset Formula $\Delta_{A}D\Delta_{B}D^{-1}$
\end_inset

, and they are positive from the lemma above.
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:pld-sum"

\end_inset

Consider 
\begin_inset Formula $A,B\in\mathcal{D}_{n}^{+}$
\end_inset

with 
\begin_inset Formula $\text{Ker}\left(A\right)=\text{Ker}\left(B\right)$
\end_inset

, then:
\begin_inset Formula 
\[
\text{ld}\left(AB\right)=\text{ld}\left(A\right)+\text{ld}\left(B\right)
\]

\end_inset


\end_layout

\begin_layout Proof
From the lemma above, the product 
\begin_inset Formula $AB$
\end_inset

 is in 
\begin_inset Formula $\mathcal{D}_{n}^{+}$
\end_inset

and the term 
\begin_inset Formula $\text{ld}\left(AB\right)$
\end_inset

 is well defined.
 Consider the diagonalization of 
\begin_inset Formula $A$
\end_inset

 and 
\begin_inset Formula $B$
\end_inset

: 
\begin_inset Formula $A=Q_{A}\Delta_{A}Q_{A}^{T}$
\end_inset

 and 
\begin_inset Formula $B=Q_{B}\Delta_{B}Q_{B}^{T}$
\end_inset

.
 Then 
\begin_inset Formula 
\begin{eqnarray*}
\text{ld}\left(AB\right) & = & \log\left|Q_{A}^{T}ABQ_{A}\right|\\
 & = & \log\left|\Delta_{A}Q_{A}^{T}Q_{B}\Delta_{B}\left(Q_{A}^{T}Q_{B}\right)^{-1}\right|\\
 & = & \log\left|\Delta_{A}\right|+\log\left|\Delta_{B}\right|\\
 & = & \text{ld}\left(A\right)+\text{ld}\left(B\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Subsection
Other inequalities
\end_layout

\begin_layout Lemma
\begin_inset CommandInset label
LatexCommand label
name "lem:Jensen-inequality-matrix-logarithm"

\end_inset

Jensen inequality for the matrix logarithm.
 Given 
\begin_inset Formula $A\in\mathcal{S}_{n}^{+},$
\end_inset

 
\begin_inset Formula $0\prec A\prec2$
\end_inset

, the following inequalities hold:
\begin_inset Formula 
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)\geq\frac{1}{n}\text{Tr}\left(\log A\right)
\]

\end_inset


\begin_inset Formula 
\[
\text{ld}\left(\frac{\text{Tr}\left(A\right)}{n-1}\right)\geq\frac{1}{n-1}\text{Tr}\left(\log A\right)
\]

\end_inset

with 
\begin_inset Formula $\log A=\sum_{k\geq1}\frac{1}{k}\left(I-A\right)^{k}$
\end_inset


\end_layout

\begin_layout Proof
Consider the diagonalization of 
\begin_inset Formula $A$
\end_inset

: 
\begin_inset Formula $A=P\Delta P^{T}$
\end_inset

 with 
\begin_inset Formula $\Delta$
\end_inset

 a diagonal (positive) matrix, and 
\begin_inset Formula $P$
\end_inset

 an orthogonal matrix.
 Then 
\begin_inset Formula 
\[
\log A=\sum_{k\geq1}\frac{1}{k}\left(PP^{T}-P\Delta P^{T}\right)^{k}=\sum_{k\geq1}\frac{1}{k}\left[P\left(I-\Delta\right)P^{T}\right]^{k}=P\left[\sum_{k\geq1}\frac{1}{k}\left(I-\Delta\right)^{k}\right]P^{T}=P\Gamma P^{T}
\]

\end_inset

 with 
\begin_inset Formula $\Gamma$
\end_inset

 a diagonal matrix that verifies 
\begin_inset Formula $\Gamma_{ii}=\log\Delta_{ii}$
\end_inset

.
 We can then conclude using the concavity of the logarithm over the reals:
 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula 
\[
\log\left(\frac{\text{Tr}\left(A\right)}{n}\right)=\log\left(\frac{\sum_{i}\Delta_{ii}}{n}\right)\geq\frac{1}{n}\sum_{i}\log\Delta_{ii}=\frac{1}{n}\text{Tr}\left(\Gamma\right)=\frac{1}{n}\text{Tr}\left(\log A\right)
\]

\end_inset


\end_layout

\begin_layout Proof
The same reasoning holds for the pseudo-log-determinant while considering
 all but one eigenvalue
\end_layout

\begin_layout Lemma
Power relation for the matrix log: Given 
\begin_inset Formula $A\in\mathcal{S}_{n}^{+},$
\end_inset

 
\begin_inset Formula $0\prec A\prec2$
\end_inset

, and 
\begin_inset Formula $k\in\mathbb{N}$
\end_inset

, then 
\begin_inset Formula $\log\left(A^{k}\right)=k\log A$
\end_inset


\end_layout

\begin_layout Subsection
Stretch of a graph
\begin_inset CommandInset label
LatexCommand label
name "sub:Stretch-graph"

\end_inset


\end_layout

\begin_layout Standard
We introduce here a refinement on an upper bound that is a byproduct of
 using the algorithm XXX.
\end_layout

\begin_layout Definition
We define the 
\series bold
stretch of a graph
\series default
 
\begin_inset Formula $G=\left(V,E,\omega\right)$
\end_inset

 with respect to a subgraph 
\begin_inset Formula $H=\left(V,\tilde{E},\tilde{\omega}\right)\subset G$
\end_inset

 by the weighted sum of effective resistances of edges 
\begin_inset Formula $e\in E$
\end_inset

 with respect to the graph 
\begin_inset Formula $H$
\end_inset

:
\begin_inset Formula 
\[
\text{st}_{H}\left(G\right)=\sum_{e\in E}\omega_{e}\text{eff}_{H}\left(e\right)
\]

\end_inset


\end_layout

\begin_layout Standard
This is a generalization of the notion of stretch defined by Alon, Karp,
 Peleg and West in ....
 We can use this definition to generalize theorem 2.1 in 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009b"

\end_inset


\end_layout

\begin_layout Theorem
Let 
\begin_inset Formula $G=\left(V,E,\omega\right)$
\end_inset

 be a connected graph and let 
\begin_inset Formula $H=\left(V,\tilde{E},\tilde{\omega}\right)$
\end_inset

 be a connected subgraph of 
\begin_inset Formula $G$
\end_inset

.
 Let 
\begin_inset Formula $L_{G}$
\end_inset

 and 
\begin_inset Formula $L_{H}$
\end_inset

 be the Laplacian matrices of 
\begin_inset Formula $G$
\end_inset

 and 
\begin_inset Formula $H$
\end_inset

 respectively.
 Then:
\begin_inset Formula 
\[
\text{Tr}\left(L_{H}^{+}L_{G}\right)=\text{st}_{H}\left(G\right)
\]

\end_inset

where 
\begin_inset Formula $L_{H}^{+}$
\end_inset

 is the pseudo inverse of 
\begin_inset Formula $L_{H}$
\end_inset

.
\end_layout

\begin_layout Proof
The proof is nearly identical to that of 
\begin_inset CommandInset citation
LatexCommand cite
key "Spielman2009b"

\end_inset

, except for the last line:
\end_layout

\begin_layout Proof
\begin_inset Formula 
\begin{eqnarray*}
\text{Tr}\left(L_{H}^{+}L_{G}\right) & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(L_{\left(u,v\right)}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{Tr}\left(\left(\psi_{u}-\psi_{v}\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\right)\\
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\left(\psi_{u}-\psi_{v}\right)^{T}L_{H}^{+}\left(\psi_{u}-\psi_{v}\right)
\end{eqnarray*}

\end_inset

and the latter term is the effective resistance between 
\begin_inset Formula $u$
\end_inset

 and 
\begin_inset Formula $v$
\end_inset

 in the graph 
\begin_inset Formula $H$
\end_inset

:
\begin_inset Formula 
\begin{eqnarray*}
 & = & \sum_{\left(u,v\right)\in E}\omega\left(u,v\right)\text{eff}_{H}\left(u,v\right)\\
 & = & \text{st}_{H}\left(G\right)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
From a practical perspective, the graph stretch can be computed in 
\begin_inset Formula $\tilde{O}\left(m\log n/\epsilon^{2}\right)$
\end_inset


\end_layout

\begin_layout Proposition
There exists an algorithm that computes 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\noun off
\color none
an 
\begin_inset Formula $\epsilon$
\end_inset

-approximation of 
\begin_inset Formula $\text{st}_{H}\left(G\right)$
\end_inset

 in
\family default
\series default
\shape default
\size default
\emph default
\bar default
\noun default
\color inherit
 
\begin_inset Formula $\tilde{O}\left(m\log n/\epsilon^{2}\right)$
\end_inset


\end_layout

\begin_layout Proof
This is the second main result from Spielman and Srivastana (XXX cite).
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Relation between augmented graphs and SDD matrices:
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $A\in SDD_{n}$
\end_inset

 and 
\begin_inset Formula $L_{A}$
\end_inset

 its augmented laplacian.
 Call 
\begin_inset Formula $P=\left[I_{n}\mathbf{0}\right]$
\end_inset

 and 
\begin_inset Formula $Q=\left[I_{n}-\mathbf{1}\right]^{T}$
\end_inset

.
 Then:
\begin_inset Formula 
\[
A=PL_{A}P^{T}
\]

\end_inset


\begin_inset Formula 
\[
B=A^{-1}=QL_{A}^{+}Q^{T}
\]

\end_inset


\begin_inset Formula 
\[
A^{-k}=Q\left(L_{A}^{+}\right)^{k}Q^{T}
\]

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $A_{ij}=e_{i}^{T}QL_{A}^{+}Q^{T}e_{j}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $R_{uv}=\left(\chi_{u}-\chi_{v}\right)^{T}L_{A}^{+}\left(\chi_{u}-\chi_{v}\right)=\left(\chi_{u}-\chi-\left(\chi_{v}-\chi\right)\right)^{T}L_{A}^{+}\left(\chi_{u}-\chi-\left(\chi_{v}-\chi\right)\right)=\left(Qe_{u}-Q_{v}\right)^{T}L_{A}^{+}\left(Qe_{u}-Q_{v}\right)=B_{uu}+B_{vv}-B_{uv}-B_{vu}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $B_{uv}=\frac{B_{uu}+B_{vv}-R_{uv}}{2}$
\end_inset


\end_layout

\begin_layout Plain Layout
Call 
\begin_inset Formula $C=A^{-2}$
\end_inset

 and 
\begin_inset Formula $Q=\left(L_{A}^{+}\right)^{2}$
\end_inset

.
 
\end_layout

\begin_layout Plain Layout
Then 
\begin_inset Formula $Q_{uv}=\left(\chi_{u}-\chi_{v}\right)^{T}\left(L_{A}^{+}\right)^{2}\left(\chi_{u}-\chi_{v}\right)=\left\Vert L_{A}^{+}\left(\chi_{u}-\chi_{v}\right)\right\Vert _{2}^{2}=C_{uu}+C_{vv}-2C_{uv}$
\end_inset


\end_layout

\begin_layout Plain Layout
XXX verify previous relations
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "2012_sparse_info,siam"
options "plain"

\end_inset


\end_layout

\end_body
\end_document
