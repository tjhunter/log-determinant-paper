
\subsection{Incremental sparsifiers\label{sec:Incremental-sparsifiers}}

We can do better and achieve near-linear time by using ultra-sparsifiers.
The main insight of our result is that the class preconditioners presented
by Spielman and Teng are based on incomplete Cholesky factorization,
and hence have a determinant that is relatively easy to compute, and
furthermore that they are excellent spectral preconditioners, so the
procedure \texttt{PreconditionedLogDetMonteCarlo} is efficient to
apply. We reintroduce some concepts presented in \cite{Koutis2010}
to present a self-contained result. The following paragraphs are well-known
facts about Spielman-Teng preconditioners and have been presented
in \cite{Koutis2010,Spielman2009a}.

The central idea to the Spielman-Teng preconditioner is to sample
$O\left(n\right)$ edges from the graph $A$, to form a subgraph $B$
that is close to a tree (hence it is easy to compute some partial
Cholesky factorization), yet it is close to the original $A$ is the
spectral sense ($A\preceq B\preceq\kappa A$), thanks to the additional
edges. The partial Cholesky factorization is computed using the \texttt{GreedyElimination}
algorithm presented in \cite{Koutis2010}. In order for this section
to be self-contained, we include here the main results of Section
4 in \cite{Spielman2009a}.

Consider the Laplacian matrix $L_{B}$ of the subgraph $B$. There
exists an algorithm that computes the partial Cholesky factorization:
\[
L_{B}=PLCL^{T}P^{T}
\]
where: 
\begin{itemize}
\item $P$ is a permutation matrix 
\item $L$ is a non-singular, low triangular matrix of the form 
\[
L=\left(\begin{array}{cc}
L_{1,1} & 0\\
L_{2,1} & I_{n_{1}}
\end{array}\right)
\]
with the diagonal of $L_{1,1}$ being all ones. 
\item $C$ has the form 
\[
C=\left(\begin{array}{cc}
D_{n-n_{1}} & 0\\
0 & L_{A_{1}}
\end{array}\right)
\]
and every row and column of $L_{A_{1}}$ has at least 3 non-zero coefficients.
Furthermore, $L_{A_{1}}$ is itself Laplacian and: 
\[
\text{ld}\left(L_{G}\right)=\sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(L_{A_{1}}\right)
\]

\end{itemize}
The exact algorithm that achieves this factorization is called \texttt{GreedyElimination}
and is presented in \cite{Koutis2010}. Using this factorization,
the PLD of the original Laplacian $L_{A}$ is: 
\begin{eqnarray}
\text{ld}\left(L_{A}\right) & = & \text{ld}\left(L_{B}\right)+\text{ld}\left(B^{+}A\right)\nonumber \\
 & = & \sum_{1}^{n-n_{1}}\log D_{ii}+\mbox{\text{ld}}\left(A_{1}\right)+\text{ld}\left(B^{+}A\right)\label{eq:chain-recursion}
\end{eqnarray}
Thus, we are left with solving a smaller problem $A_{1}$, and we
approximate the value of $\text{ld}\left(B^{+}A\right)$ using the
algorithm \texttt{SampleLogDet}. ST preconditioners are appealing
for this task: they guarantee that $A_{1}$ is substantially smaller
than $A$, so the recursion completes in $O\left(\log\log n\right)$
steps. Furthermore, computing the vector product $B^{+}Ax$ is itself
efficient (in can be done approximated in near-linear time), so we
can apply Theorem \ref{thm:preconditioning-approx}. We formalize
the notion of chain of preconditioners by reintroducing some material
from \cite{Koutis2010}.

\begin{definition} Definition 7.1 from \cite{Koutis2010}. $\kappa\left(n\right)$-good
chain. $\mathcal{C}=\left\{ A_{1}=A,B_{1},A_{2},\dots A_{d}\right\} $
a chain of graphs with $n_{i}$ and $m_{i}$ the number of vertices
and edges of $A_{i}$. $\mathcal{C}$ is $\kappa\left(n\right)$-good
for $A$ if:
\begin{enumerate}
\item $A_{i}\preceq B_{i}\preceq\kappa\left(n_{i}\right)A_{i}$ 
\item $A_{i+1}$= \texttt{GreedyElimination}$\left(B_{i}\right)$ 
\item $m_{i}/m_{i+1}\geq c_{r}\sqrt{\kappa\left(n_{i}\right)}$ for some
constant $c_{r}$. 
\end{enumerate}
\end{definition} 

Good chains exist, as found by Koutis, Miller and Peng:

\begin{lemma} \label{lem:good-chain} (Lemma 7.3 from \cite{Koutis2010})
Given a graph $A$, \texttt{BuildChain}$\left(A,p\right)$ from \cite{Koutis2010}
produces a $\tilde{O}\left(\log^{4}n\right)$-good chain for $A$
with probability $1-p$. The algorithm runs in time 
\[
\tilde{O}\left(\left(m\log n+n\log^{2}n\right)\log\left(1/p\right)\right)
\]


\end{lemma}

These chains furthermore can be used a good preconditioners for conjugate
gradient and lead to near-linear algorithms for approximate inversion
(Lemma 7.2 from \cite{Koutis2010}). This remarkable result has been
significantly strenghtened in the previous years, so that SDD systems
can be considered to be solved in (expected) linear time.

\begin{lemma} \label{lem:linear-precond-existence}(Theorem 4.6 from
\cite{Koutis2011}). Given $A\in SDD_{n}$ with $m$ non-zero entries,
$b\in\mathbb{R}^{n}$ and $\nu>0$, a vector $x$ such that $\left\Vert x-A^{+}b\right\Vert _{A}<\nu\left\Vert A^{+}b\right\Vert _{A}$
can be computed in expected time $\tilde{O}\left(m\log n\log\left(1/\nu\right)\right)$.\end{lemma}

It should now become clear how we can combine a $\kappa\left(n\right)$-good
chain with the Algorithm \texttt{PreconditionedLogDetMonteCarlo}.
We start by building a chain. The partial Cholesky factorizations
at each step of the chain provide an upper bound to $\mbox{ld}\left(A\right)$.
We then refine this upper bound by running \texttt{PreconditionedLogDetMonteCarlo}
at each state of the chain to approximate $\mbox{ld}\left(B_{i}^{+}A_{i}\right)$
with high probability. The complete algorithm is presented in Algorithm
\ref{alg:The-main-algorithm}. We now have all the tools required
to prove Theorem \ref{thm:ultra_main}.

\textbf{Proof of Theorem \ref{thm:ultra_main}.} Using Lemma \ref{lem:good-chain},
consider $\mathcal{C}=\left\{ A_{1}=A,B_{1},A_{2},\dots A_{d}\right\} $
a $\tilde{O}\left(\log^{4}n\right)$-good chain for $L_{A}$, with
$d=\mathcal{O}\left(\log\log n\right)=\tilde{O}\left(1\right)$. We
use Equation \ref{eq:chain-recursion} to compute the Cholesky terms
for each stage of the chain, and we are left with estimating $\tilde{\mathcal{O}}\left(1\right)$
reminders $\text{ld}\left(B_{i}^{+}A_{i}\right)$. By construction,
$A_{i}\preceq B_{i}\preceq\kappa\left(n_{i}\right)A_{i}$ and by Lemma
\ref{lem:linear-precond-existence}, there exists an operator $C$
so that $\left\Vert C\left(b\right)-A^{+}b\right\Vert _{A}<\nu\left\Vert A^{+}b\right\Vert _{A}$
for all $b$ with a choice of relative precision $\nu=\frac{\epsilon}{16\kappa^{3}\kappa\left(B\right)}$.
By Theorem \ref{thm:preconditioning-approx}, each reminder $\text{ld}\left(B_{i}^{+}A_{i}\right)$
can be approximated to precision $\epsilon$ with probability at least
$1-\eta$ using Algorithm \ref{BROKEN: Ref: alg:The-main-alg...}.
Furthermore, this algorithm works in expected time $\tilde{O}\left(m\log n\log\left(1/\nu\right)\kappa\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(\frac{n\kappa}{\nu}\right)\log^{2}\left(\kappa\right)\log\left(\eta^{-1}\right)\right)=\tilde{O}\left(m\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log^{2}n\log^{2}\left(\frac{\kappa\left(B\right)}{\epsilon}\right)\log\left(\eta^{-1}\right)\right)$.
By a union bound, the result also holds on the sum of all the approximations
of the reminders. We can simplify this bound a little by assuming
that $\epsilon\geq n^{-1}$, which then becomes $\tilde{O}\left(m\epsilon^{-1}\log^{2}n\log^{2}\left(\frac{\kappa\left(B\right)}{\epsilon}\right)\log\left(\eta^{-1}\right)\right)$.
