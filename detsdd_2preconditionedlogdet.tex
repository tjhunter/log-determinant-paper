
\section{Preconditioned log-determinants\label{sec:Preconditioned-log-determinants}}

We begin by a close inspection of a simple sampling algorithm to compute
log-determinants, presented first in \cite{Barry1999}. We will first
present some error bounds on this algorithm that expand on bounds
previously presented in \cite{Bai1996} and \cite{Barry1999}. This
section considers general symmetric matrices and does not make assumptions
about diagonal dominance.

Consider a real symmetric matrix $S\in\mathcal{S}_{n}^{+}$ such that
its spectral radius is less than $1$: $0\preceq S\preceq\left(1-\delta\right)I$
for some $\delta\in\left(0,1\right)$. Our goal is to compute $\log\left|I-S\right|$
up to precision $\epsilon$ and with high probability. From the Martin
expansion: 
\begin{equation}
\log\left|I-S\right|=-\Tr\left(\sum_{k=1}^{\infty}\frac{1}{k}S^{k}\right)\label{eq:martin}
\end{equation}


This series of traces can be estimated by Monte Carlo sampling, up
to precision $\epsilon$ with high probability, by truncating the
series and by replacing the exact trace evaluation by $x^{T}S^{k}x$
for some suitably chosen random variables $x$. In order to bound
the errors, we will bound the large deviation errors using the following
Bernstein inequality:

\begin{lemma}{[}Bernstein's inequality{]} \label{lem:bernstein}
Let $X_{1}\cdots X_{n}$ be independent random variables with $\mathbb{E}\left[X_{i}\right]=0$,
$\left|X_{i}\right|<c$ almost surely. Call $\sigma^{2}=\frac{1}{n}\sum_{i}\text{Var}\left(X_{i}\right)$,
then for all $\epsilon>0$: 
\[
\mathbb{P}\left[\frac{1}{n}\Big|\sum_{i}X_{i}\Big|\geq\epsilon\right]\leq2\exp\left(-\frac{n\epsilon^{2}}{2\sigma^{2}+2c\epsilon/3}\right)
\]
\end{lemma}

We can adapt some results from \cite{Barry1999} to prove this bound
on the deviation from the trace.

\begin{lemma} \label{lem:bernstein-trace}Consider $H\in\mathcal{S}_{n}$
with the assumption $\lambda_{\text{min}}I_{n}\preceq H\preceq\lambda_{\text{max}}I$.
Consider $p$ vectors sampled from the standard Normal distribution:
$\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$ for
$i=1\cdots p$. Then for all $\epsilon>0$: 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-\frac{1}{n}\mbox{Tr}\left(H\right)\right|\geq\epsilon\right]\leq2\exp\left(-\frac{p\epsilon^{2}}{4n^{-1}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}+2\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon/3}\right)
\]
\end{lemma}

\begin{proof} The distribution of $\mathbf{u}_{i}$ is invariant
through a rotation, so we can consider $H$ diagonal. We assume without
loss of generality that $H=\text{diag}\left(\lambda_{1},\cdots,\lambda_{n}\right)$.
Again without loss of generality, we assume that $\lambda'_{\max}=\lambda_{\max}-\lambda_{\min}$
and $\lambda'_{\min}=0$ (by considering $H'=H-\lambda_{\min}I$).
Call $V_{i}=\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-n^{-1}\text{Tr}\left(H\right)$.
Using results from \cite{Barry1999}, we have: $\left|V_{i}\right|\leq\lambda_{\max}-\lambda_{\min}$,
$\mathbb{E}\left[V_{i}\right]=0$ and 
\[
\text{Var}\left(V_{i}\right)=\frac{2}{n(n+2)}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}
\]


Each of the variables $V_{i}$ is independent, %(see \cite{Barry1999})
so invoking Lemma \ref{lem:bernstein} gives: 
\[
\mathbb{P}\left[\frac{1}{p}\left|\sum_{i=1}^{p}V_{i}\right|\geq\epsilon\right]\leq2\exp\left(-\frac{p\epsilon^{2}}{2\sigma^{2}+2\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon/3}\right)
\]
with $\sigma^{2}=\frac{2}{n(n+2)}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}\leq\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}=\frac{2}{n}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}$.
%Dropping the factor $2/3$, we get our result. 
\end{proof}

The previous lemma shows that if the eigenspectrum of a matrix is
bounded, we can obtain a Bernstein bound on the error incurred by
sampling the trace. Furthermore, the convergence of the series \ref{eq:martin}
is also determined by the extremal eigenvalues of $S$. If we truncate
the series (\ref{eq:martin}), we can bound the truncation error using
the extremal eigenvalues. We formalize this intuition in the following
theorem, which is adapted from the main theorem in \cite{Barry1999}.
While that main theorem in \cite{Barry1999} only considered a confidence
interval based on the covariance properties of Gaussian distribution,
we generalize this result to a more general Bernstein bound.

\begin{theorem} \label{thm:det-sampling-theorem}Consider $S\in\mathcal{S}_{n}^{+}$
with $0\preceq S\preceq\left(1-\delta\right)I$ for some $\delta\in\left(0,1\right)$.
Call $y=n^{-1}\log\left|I-S\right|$ the quantity to estimate, and
consider $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
for $i=1\cdots p$ all independent. Call $\hat{y}_{p,l}$ an estimator
of the truncated series of $l$ elements computed by sampling the
trace using $p$ samples: 
\[
\hat{y}_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\frac{\mathbf{u}_{j}^{T}S^{k}\mathbf{u}_{j}}{\mathbf{u}_{j}^{T}\mathbf{u}_{j}}
\]
Given $\epsilon>0$ and $\eta\in\left(0,1\right)$, the $\hat{y}_{p,l}$
approximates $y$ up to precision $\epsilon$ with probability at
least $1-\eta$ by choosing $p\geq8\left(\frac{2}{\epsilon}\log^{2}\left(1/\delta\right)+\frac{1}{3n\epsilon^{2}}\log\left(1/\delta\right)\right)\log\left(1/\eta\right)$
and $l\geq2\delta^{-1}\log\left(\frac{n}{\delta\epsilon}\right)$:
\[
\mathbb{P}\left[\left|y-\hat{y}_{p,l}\right|\geq\epsilon\right]\leq\eta
\]
\end{theorem}

The proof of this result is detailed in Appendix A.

From this theorem we derive two results that justify the notion of
preconditioners for determinants: one for exact preconditioners and
one for approximate preconditioners. The corresponding algorithm,
which we call \texttt{PreconditionedLogDetMonteCarlo}, is presented
in Algorithm \ref{alg:SampleLogDet}.

\begin{corollary} \label{cor:preconditioning}Let $A\in\mathcal{S}_{n}^{+}$
and $B\in\mathcal{S}_{n}^{+}$ be positive definite matrices so that
$B$ is a $\kappa-$approximation of $A$: 
\begin{equation}
A\preceq B\preceq\kappa A\label{eq:A-B-bounds}
\end{equation}
Given $\epsilon>0$ and $\eta\in\left(0,1\right)$, the algorithm
\texttt{PreconditionedLogDetMonteCarlo} computes $\frac{1}{n}\log\left|B^{-1}A\right|$
up to precision $\epsilon$ with probability greater than $1-\eta$,
by performing $\mathcal{O}\left(\kappa\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log^{2}\left(\kappa\right)\log\left(\frac{n\kappa}{\epsilon}\right)\log\left(\eta^{-1}\right)\right)$
vector inversions from $B$ and vector multiplies from $A$.

\end{corollary}

The proof of this corollary is presented in Appendix A. Usually, computing
the exact inverse by an SDD matrix is too expensive. We can instead
extend the previous result to consider a black box procedure that
approximatively computes $B^{-1}x$. If the error introduced by the
approximate inversion is small enough, the result from the previous
corollary still holds. This is what the following theorem establishes:

\begin{theorem}\label{thm:preconditioning-approx}Consider $A,B\in\mathcal{S}_{n}^{+}$
positive definite with $B$ a $\kappa-$approximation of $A$ with
$\kappa\geq2$. Furthermore, assume there exists a linear operator
$C$ so that for all $y\in\mathbb{R}^{n}$, $C$ returns an $\nu-$approximation
of $B^{-1}y$: 
\[
\left\Vert C\left(y\right)-B^{-1}y\right\Vert _{B}\leq\nu\left\Vert B^{-1}y\right\Vert _{B}
\]
Given $\eta\in\left(0,1\right)$ and $\epsilon>0$, if $\nu\leq\min\left(\frac{\epsilon}{8\kappa^{3}\kappa\left(B\right)},\frac{1}{2\kappa}\right)$,
then the algorithm \texttt{PreconditionedLogDet- MonteCarlo} returns
a scalar $z$ so that: 
\[
\mathbb{P}\left[\left|z-n^{-1}\log\left|B^{-1}A\right|\right|\geq\epsilon\right]\leq\eta
\]
by performing $64\kappa\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log^{2}\left(\kappa\right)\log\left(\frac{n\kappa}{\epsilon}\right)\log\left(\eta^{-1}\right)$
vector calls to the operator $C$ and vector multiplies from $A$.

\end{theorem}

The proof of this result is detailed in Appendix A. While the overall
bound looks the same, the constant (taken away by the $O\left(\right)$
notation) is twice as large as Corollary \ref{cor:preconditioning}.

This last theorem shows that we can compute a good approximation of
the log-determinant if the preconditioner $B$: (a) is close to $A$
in the spectral sense, and (b) can be approximately inverted and the
error introduced by the approximate inversion can be controlled. This
happens to be the case for symmetric, diagonally dominant matrices.

\begin{algorithm}
\begin{raggedright} Algorithm \textbf{UltraLogDet}($A$,$\epsilon$,$\eta$):

\end{raggedright}

\begin{raggedright} If $A$ is of a small size ($<$100), directly
compute $\text{ld}\left(A\right)$ with a dense Cholesky factorization.

\end{raggedright}

\begin{raggedright} Compute $B=$\textbf{IncrementalSparsify($A$)}

\end{raggedright}

\begin{raggedright} Compute $D,A'=$\textbf{PartialCholesky($B$)}

\end{raggedright}

\begin{raggedright} $\eta\leftarrow\min\left(\frac{\epsilon}{8\kappa^{3}\kappa\left(B\right)},\frac{1}{2\kappa}\right)$

\end{raggedright}

\begin{raggedright} $p\leftarrow8\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(\eta^{-1}\right)\log^{2}\left(\delta^{-1}\right)$

\end{raggedright}

\begin{raggedright} $l\leftarrow\delta^{-1}\log\left(\frac{2}{\epsilon\delta}\right)$

\end{raggedright}

\begin{raggedright} Compute $s=$\textbf{PreconditionedLogDetMonteCarlo($B,A,\eta,p,l$)}

\end{raggedright}

\begin{raggedright} Return $s+\log\left|D\right|+$\textbf{UltraLogDet($A'$,$\epsilon$,$\eta$)}

\end{raggedright}

\caption{Sketch of the main algorithm\label{alg:The-main-algorithm}}
\end{algorithm}


\begin{algorithm}
Algorithm \textbf{PreconditionedLogDetMonteCarlo}($B$,$A$,$\eta$,$p$,$l$):

$y\leftarrow0$

for $j$ from $1$ to $p$:

~~Sample $\mathbf{u}\sim\mathcal{N}\left(\mathbf{0},I\right)$

~~$\mathbf{v}\leftarrow\mathbf{u}/\left\Vert \mathbf{u}\right\Vert $

~~$z\leftarrow0$

~~for $k$ from $1$ to $l$:

~~~~$\mathbf{v}\leftarrow B^{-1}A\mathbf{v}$~up to precision
$\eta$

~~~~$z\leftarrow z+k^{-1}\mathbf{v}^{T}\mathbf{u}$

~~$y\leftarrow y+p^{-1}z$

Return $y$

\caption{PreconditionedLogDetMonteCarlo\label{alg:SampleLogDet}}
\end{algorithm}

