
\section{Preconditioned log-determinants\label{sec:Preconditioned-log-determinants}}

We begin by a close inspection of a simple sampling algorithm to compute
log-determinants, presented first in \cite{Barry1999}. We will first
present some error bounds on this algorithm that expand on bounds
previously presented in \cite{Bai1996} and \cite{Barry1999}. This
section considers general symmetric matrices and does not make assumptions
about diagonal dominance.

Consider a real symmetric matrix $S\in\mathcal{S}_{n}^{+}$ such that
its spectral radius is less than $1$: $0\preceq S\preceq\left(1-\delta\right)I$
for some $\delta\in\left(0,1\right)$. Our goal is to compute $\log\left|I-S\right|$
up to precision $\epsilon$ and with high probability. From the Martin
expansion: 
\begin{equation}
\log\left|I-S\right|=-\mbox{Tr}\left(\sum_{k=1}^{\infty}\frac{1}{k}S^{k}\right)\label{eq:martin}
\end{equation}


This series of traces can be estimated by Monte Carlo sampling, up
to precision $\epsilon$ with high probability, by truncating the
series and by replacing the exact trace evaluation by $x^{T}S^{k}x$
for some suitably chosen random variables $x$. In order to bound
the errors, we will bound the large deviation errors using the following
Bernstein inequality:

\begin{lemma} \label{lem:bernstein}Let $X_{1}\cdots X_{U}$ be independent
random variables with $\mathbb{E}\left[X_{u}\right]=0$, $\left|X_{u}\right|<c$.
Call $\sigma^{2}=\frac{1}{U}\sum_{u}\text{Var}\left(X_{u}\right)$,
then for all $\epsilon>0$: 
\[
\mathbb{P}\left[\frac{1}{U}\sum_{u}X_{u}\geq\epsilon\right]\leq\exp\left(-\frac{U\epsilon^{2}}{2\sigma^{2}+2c\epsilon/3}\right)
\]


\end{lemma}

The proof of this result can be found in a lecture note from Peter
Bartlett's class (stat241B/spring 2003). We can adapt some results
from \cite{Barry1999} to prove this bound on the deviation from the
trace.

\begin{lemma} \label{lem:bernstein-trace}Consider $H\in\mathcal{S}_{n}$
with the assumption $\lambda_{\text{min}}I_{n}\preceq H\preceq\lambda_{\text{max}}I$.
Consider $p$ vectors sampled from the standard Normal distribution:
$\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$ for
$i=1\cdots p$. Then for all $\epsilon>0$: 
\[
\mathbb{P}\left[\left|\frac{1}{p}\sum_{i=1}^{p}\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-\frac{1}{n}\mbox{Tr}\left(H\right)\right|\geq\epsilon\right]\leq\exp\left(-\frac{p\epsilon^{2}}{2n^{-1}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}+\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon}\right)
\]
\end{lemma} 

\begin{proof} The distribution of $\mathbf{u}_{i}$ is invariant
through a rotation, so we can consider $H$ diagonal. We assume without
loss of generality that $H=\text{diag}\left(\lambda_{1}...\lambda_{n}\right)$.
Again without loss of generality, we assume that $\lambda'_{\max}=\lambda_{\max}-\lambda_{\min}$
and $\lambda'_{\min}=0$ (by considering $H'=H-\lambda_{\min}I$).
Call $V_{i}=\frac{\mathbf{u}_{i}^{T}H\mathbf{u}_{i}}{\mathbf{u}_{i}^{T}\mathbf{u}_{i}}-n^{-1}\text{Tr}\left(H\right)$.
Using results from \cite{Barry1999}, we have: $\left|V_{i}\right|\leq\lambda_{\max}-\lambda_{\min}$,
$\mathbb{E}\left[V_{i}\right]=0$ and: 
\[
\text{Var}\left(V_{i}\right)=\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}
\]


Each of the variables $V_{i}$ is independent (see \cite{Barry1999}),
so invoking Lemma \ref{lem:bernstein} gives: 
\[
\mathbb{P}\left[\frac{1}{p}\sum_{i=1}^{p}V_{i}\geq\epsilon\right]\leq\exp\left(-\frac{p\epsilon^{2}}{2\sigma^{2}+2\left(\lambda_{\max}-\lambda_{\min}\right)\epsilon/3}\right)
\]
with $\sigma^{2}=\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{i}-n^{-1}\text{Tr}\left(H\right)\right)^{2}\leq\frac{2}{n^{2}}\sum_{i=1}^{n}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}=2n^{-1}\left(\lambda_{\max}-\lambda_{\min}\right)^{2}$.
Dropping the factor $2/3$, we get our result. \end{proof}

The previous lemma shows that if the eigenspectrum of a matrix is
bounded, we can obtain a Hoeffding bound on the error done by sampling
the trace. Furthermore, the convergence of the series \ref{eq:martin}
is also determined by the extremal eigenvalues of $S$. If we truncate
the series (\ref{eq:martin}), we can bound the truncation error using
the extremal eigenvalues. We formalize this intuition in the following
theorem, which is adapted from the main Theorem in \cite{Barry1999}.
While that main Theorem in \cite{Barry1999} only considered a confidence
interval based on the covariance properties of Gaussian distribution,
we generalize this result to a more general Bernstein bound.

\begin{theorem} \label{thm:det-sampling-theorem}Consider $S\in\mathcal{S}_{n}^{+}$
with $0\preceq S\preceq\left(1-\delta\right)I$ for some $\delta\in\left(0,1\right)$.
Call $y=n^{-1}\log\left|I-S\right|$ the quantity to estimate, and
consider $\mathbf{u}_{i}\sim\mathcal{N}\left(\mathbf{0},I_{n}\right)$
for $i=1\cdots p$ all independent. Call $\hat{y}_{p,l}$ an estimator
of the truncated series of $l$ elements computed by sampling the
trace using $p$ samples: 
\[
\hat{y}_{p,l}=\frac{1}{p}\sum_{j=1}^{p}\sum_{k=1}^{l}\frac{1}{k}\frac{\mathbf{u}_{j}^{T}S^{k}\mathbf{u}_{j}}{\mathbf{u}_{j}^{T}\mathbf{u}_{j}}
\]
Given $\epsilon>0$ and $\eta\in\left(0,1\right)$, the $\hat{y}_{p,l}$
approximates $y$ up to precision $\epsilon$ with probability at
least $1-\eta$ by choosing $p\geq8\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(\eta^{-1}\right)\log^{2}\left(\delta^{-1}\right)$
and $l\geq2\delta^{-1}\log\left(\frac{n}{\delta\epsilon}\right)$:
\[
\mathbb{P}\left[\left|y-\hat{y}_{p,l}\right|\geq\epsilon\right]\leq\eta
\]
\end{theorem}

The proof of this result is detailed in Appendix A.

From this theoem we derive two results that justify the notion of
preconditioners for determinants: one for exact preconditioners and
one for approximate preconditioners. The corresponding algorithm,
which we call \texttt{PreconditionedLogDetMonteCarlo}, is presented
in Algorithm \ref{alg:SampleLogDet}.

\begin{corollary} \label{cor:preconditioning}If $A$ and $B$ are
positive definite matrices so that $B$ is a $\kappa-$approximation
of $A$: 
\begin{equation}
A\preceq B\preceq\kappa A\label{eq:A-B-bounds}
\end{equation}
then the algorithm \texttt{PreconditionedLogDetMonteCarlo} computes
$\frac{1}{n}\log\left|B^{-1}A\right|$ up to some precision $\epsilon$
with probability greater than $1/n$, by performing $\mathcal{O}\left(\kappa\left(\frac{1}{\epsilon}+\frac{1}{n\epsilon^{2}}\right)\log\left(n\right)\log^{2}\left(\kappa\right)\log\left(\frac{n}{\kappa\epsilon}\right)\right)$vector
inversions from $B$ and vector multiplies from $A$.

\end{corollary}

The proof of this corollary is presented in Appendix A. Usually, computing
the exact inverse by an SDD matrix is too expensive. We can instead
extend the previous result to consider a black box that approximatively
computes $B^{-1}x$. If the error introduced by the approximate inversion
is small enough, the result from the previous corollary still holds.
This is what the following theorem establishes:

\begin{theorem}\label{thm:preconditioning-approx}Consider $A,B\in\mathcal{S}_{n}^{+*}$
with $B$ a $\kappa-$approximation of $A$. Furthermore, assume there
exists a function $C$ so that for all $y\in\mathbb{R}^{n}$, $C$
returns an $\nu-$approximation of $B^{-1}y$:
\[
\left\Vert C\left(y\right)-B^{-1}y\right\Vert _{B}\leq\nu\left\Vert B^{-1}y\right\Vert _{B}
\]
Given $\eta>0$ and $\epsilon>0$, if $\nu\leq\frac{\epsilon^{3}\eta}{16\kappa^{3}\kappa\left(B\right)\left(1+n\epsilon\right)\log\left(\eta^{-1}\right)\log^{2}\kappa}$,
then the algorithm \texttt{PreconditionedLogDetMonteCarlo} returns
a scalar $z$ so that:
\[
\mathbb{P}\left[\left|z-n^{-1}\log\left|B^{-1}A\right|\right|\geq\epsilon\right]\leq\eta
\]
by performing $O\left(\kappa\epsilon^{-2}\log\left(\frac{n\kappa}{\nu}\right)\log^{2}\left(\kappa\right)\log\left(\eta^{-1}\right)\right)$
vector calls to the operator $C$ and vector multiplies from $A$. 

\end{theorem}

The proof of this result is detailed in Appendix A.

This last theorem shows that we can compute a good approximation of
the log-determinant if the preconditioner $B$: (a) is close to $A$
in the spectral sense, and (b) can be approximately inverted and the
error introduced by the approximate inversion can be controlled. This
happens to be the case for symmetric, diagonally dominant matrices.
