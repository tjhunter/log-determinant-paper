
\section{Ultra-sparsifiers as determinant preconditioners}


\subsection{Making the problem laplacian}

\label{sec:Making-the-problem}

From now on, we consider all matrices to be in $SDD_{n}$. The techniques
we will develop work on Laplacian matrices instead of SDD matrices.
An SDD matrix is positive semi-definite while a Laplacian matrix is
always singular, since its nullspace is spanned by $\mathbf{1}$.
We generalize the definition of the determinant to handle this technicality.

\begin{definition} \emph{Pseudo-log-determinant (PLD):} Let $A\in\mathcal{S}^{n+}$
be a non-null positive semi-definite matrix. The pseudo-log-determinant
is defined by the sum of the logarithms of all the positive eigenvalues:
\[
\text{ld}\left(A\right)=\sum_{\lambda_{i}>0}\log\left(\lambda_{i}\right)
\]
where $\lambda_{i}$ are the eigenvalues of $A$. \end{definition}

The interest of the PLD lies in the connection between SDD matrices
and their associated Laplacian. Recall that an SDD matrix can be transformed
into a Laplacian (which has all off-diagonal terms non-positive, and
for which the sum of each row and each column is zero). As underlined
in \cite{Gremban1996}, solving an equation involving an SDD can be
turned into solving an equation with an SDD of twice the same size
in which all the off-diagonal terms are non-positive. This transform
is also useful for computing determinants. The following lemma shows
that computing the log-determinant of an SDD with positive off-diagonal
entries can be turned into computing the log-determinants of two SDD
matrices with non-positive off-diagonal entries.

\begin{lemma}\label{non-negative-conversion} For a SDD matrix $A$, let $P$ and $N$ be the positive and the negative parts of $A$ respectively, i.e. $P_{ij} = \max(A_{ij},0)$ and $N_{ij} = \min(A_{ij},0)$ for all $i,j$. Then the matrices $\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)\in SDD_{2n}$ and $P-N\in SDD_{n}$ have all the off-diagonal coefficients non-positive,
and the following relation holds
\[
\log\left|\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)\right|=\log\left|P+N\right|+\log\left|P-N\right|
\]
\end{lemma}

\begin{proof} Call $B=P-N$ and $C=\left(\begin{array}{cc}
P & N\\
N & P
\end{array}\right)$. As a consequence of the above definitions, $A=P+N$ and $\left|A\right| = P - N$. Then $A \in SDD_{n}$ implies that $B$ and $C$ are diagonally dominant. Next, consider $\mu$ an eigenvalue of $A$, and $x$ an associated eigenvector:
$Ax=\mu x$. Then the vector $\left(\begin{array}{c}
x\\
x
\end{array}\right)$
is an eigenvector of $C$ with eigenvalue $\mu$. Similarly, if $y$ is an eigenvector of $B$ with associated eigenvalue $\nu$, then $\left(\begin{array}{c}
y\\
-y
\end{array}\right)$ is also an eigenvector of $C$ with associated eigenvalue $\nu$. Since $C$ is exactly of size $2n$, the set of eigenvalues of $C$
is exactly the concatenation of sets of eigenvalues of $A$ and $B$.
Thus $\log\left|C\right|=\sum_{i}\log\mu_{i}+\sum_{i}\log\nu_{i}=\log\left|A\right|+\log\left|B\right|$.
\end{proof}

Furthermore, computing the determinant of SDD matrices with non-negative
off-diagonal entries is equivalent to computing the PLD of Laplacian.
Recall that a Laplacian matrix can be constructed from any SDD with
non-positive off-diagonal entries matrix by adding a ``ground''
node (see Gremban's thesis \cite{Gremban1996}, Chapter 4).

\begin{definition} \emph{Augmented Laplacian of an SDD matrix.} Let
$A\in SDD_{n}$ with non-positive off-diagonal entries. Let $L,D$ be the decomposition of $A$ into $L$
a Laplacian matrix and $D$ a (non-negative) diagonal matrix such
that $A=L+D$. The augmented matrix of $A$ is: 
\[
L_{A}=\left(\begin{array}{cc}
A & -\mathbf{d}\\
-\mathbf{d}^{T} & h
\end{array}\right)
\]
with $\mathbf{d}\in\mathbb{R}^{n}$: $\mathbf{d}_{i}=D_{ii}$ and
$h=\sum_{i}D_{ii}$. \end{definition}

From now on, given any $A\in SDD_{n}$, we will implicitly define
a Laplacian $L_{A}$ associated to it, using the augmented Laplacian.
This process is also reversible: to any Laplacian $L$ we can associate
a unique positive definite matrix $F_{L}$ (up to a permutation),
and this transform preserves eigenvalues and matrix inequalities.
We call this process ``floating'' of the Laplacian, by analogy to
the ``grounding'' in the electrical sense of the SDD matrix as a
Laplacian.

\begin{definition}\emph{Floating a Laplacian}. Consider $L$ a Laplacian
matrix. Call $F_{L}$ the matrix formed by removing the last row and
the last column from $L$.

\end{definition}

The following lemma shows that the Laplacian matrix overdetermines
a system, and that no information is lost by floating it.

\begin{lemma}\label{lem:floating-properties}Consider $Z$ a (weighted)
Laplacian matrix of a connected graph, then: 
\begin{enumerate}
\item Idempotence: $L_{F_{Z}}=Z$ 
\item The eigenvalues of $F_{Z}$ are the positive eigenvalues of $Z$,
and the corresponding eigenvectors for $F_{Z}$ are the same eigenvectors,
truncated by the last coefficient. 
\item $\text{ld}\left(Z\right)=\log\left|F_{Z}\right|$ 
\item Given $Z_{1},Z_{2}$ Laplacian matrices, we have $Z_{1}\preceq Z_{2}\Rightarrow F_{Z_{1}}\preceq F_{Z_{2}}$ 
\end{enumerate}
\end{lemma}

The proof of this lemma is in Appendix B.

Using the floating and the grounding procedures, we can transform
the PLD of a Laplacian matrix into the log-determinant of a SDD matrix.
The following proposition establishes this equivalence.

\begin{lemma} \label{pro:pld-properties}Let $A\in SDD_{n}$, and
$L_{A}$ its augmented Laplacian. 
\begin{itemize}
\item $\text{ld}\left(L_{A}\right)=\log\left|A\right|$ 
\item Let $A,B\in SDD_{n}$. $\text{ld}\left(L_{A}L_{B}\right)=\text{ld}\left(L_{A}\right)+\text{ld}\left(L_{B}\right)$
and $\text{ld}\left(L_{B}^{+}L_{A}\right)=\log\left|B^{-1}A\right|$
with $L_{A}^{+}$the pseudo inverse of $L_{A}$. 
\end{itemize}
\end{lemma}

\begin{proof}The proofs of this lemma are straightforward and are
contained in Appendix B. \end{proof}

\paragraph{General procedure:} we summarize the reduction of a generic SDD matrix together with the computation of its log-determinant as follows. Given $A \in SDD_n$:
\begin{enumerate}
\item Consider the matrices $C$ and $B$ defined in lemma \ref{non-negative-conversion} (they are SDD with non-positive off-diagonal entries). 
\item Ground $C$ and $B$ to obtain the Laplacian matrices $L_C$ and $L_B$.
\item Compute $ld(L_C)$ and $ld(L_B)$ using \texttt{UltraLogDet}.
\item Return $\log |A| = ld(L_C) - ld(L_B)$.   
\end{enumerate}  


A Laplacian matrix can be considered either for its graphical properties,
or for its algebraic properties. Recent results has shown a deep connection
between these two aspects, and let us develop a general framework
for computing determinants: given $G\in SDD_{n}$, we consider the
graph associated to its Laplacian $L_{G}$, through the grounding
procedure. Using graphical properties of $L_{G}$, we can construct
a weighted subgraph $H$ of $G$ for which the PLD is easier
to compute and that is a good approximation of $G$ in the spectral
sense. We can float the subgraph $H$ and apply results of section
\ref{sec:Preconditioned-log-determinants} to approximate the remainder
with high probability. More precisely: 
\begin{align*}
\log\left|G\right| & =\text{ld}\left(L_{H}\right)-\log\left|F_{L_{H}}\right|+\log\left|L_G\right|\\
 & =\text{ld}\left(L_{H}\right)+\log\left|F_{L_{H}}^{-1}L_G\right|
\end{align*}


The first term $\text{ld}\left(L_{H}\right)$ is usually easier to
compute by considering the graphical properties of $L_{H}$, while
the remainder $\log\left|F_{L_{H}}^{-1}G\right|$ is approximated
by sampling. Preconditioner graphs $L_{H}$ are typically efficient
to factorize using Cholesky factorization, and close enough to $G$
so that the sampling procedure from the previous section can be applied
to compute $\log\left|F_{L_{H}}^{-1}G\right|$. We will see how to
adapt Spielman and Teng's remarkable work on \emph{ultra-sparsifiers}
to produce good preconditioners $H$ for the determinant. 


%\[
%\left(\begin{array}{cc}
%P & N\\
%N & P
%\end{array}\right)\left(\begin{array}{c}
%x\\
%-x
%\end{array}\right)=\left(\begin{array}{c}
%Ax\\
%-Ax
%\end{array}\right)=\mu\left(\begin{array}{c}
%x\\
%-x
%\end{array}\right)
%\]
%which shows that the vector $\left(\begin{array}{c}
%x\\
%-x
%\end{array}\right)$ is an eigenvector of $C$ with associated eigenvalue $\mu$ .
