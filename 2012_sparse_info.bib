## Convex results

The main book
@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, S.P. and Vandenberghe, L.},
  year={2004},
  publisher={Cambridge Univ Pr}
}

@article{banerjee2008model,
  title={Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data},
  author={Banerjee, O. and El Ghaoui, L. and d'Aspremont, A.},
  journal={The Journal of Machine Learning Research},
  volume={9},
  pages={485--516},
  year={2008},
  publisher={JMLR. org}
}

Look for sparse inverse covariance selection

@article{hsieh2011sparse,
  title={Sparse inverse covariance matrix estimation using quadratic approximation},
  author={Hsieh, C.J. and Sustik, M.A. and Ravikumar, P. and Dhillon, I.S.},
  journal={Advances in Neural Information Processing Systems (NIPS)},
  volume={24},
  year={2011}
}
 

@article{malioutov2006walk,
  title={Walk-sums and belief propagation in Gaussian graphical models},
  author={Malioutov, D.M. and Johnson, J.K. and Willsky, A.S.},
  journal={The Journal of Machine Learning Research},
  volume={7},
  pages={2031--2064},
  year={2006},
  publisher={JMLR. org}
}
The walk-summability article (JMLR version)

@article{barry1999monte,
  title={Monte Carlo estimates of the log determinant of large sparse matrices},
  author={Barry, R.P. and Kelley Pace, R.},
  journal={Linear Algebra and its Applications},
  volume={289},
  number={1},
  pages={41--54},
  year={1999},
  publisher={Elsevier}
}


We are basically computing an approximate analytic solution.
http://en.wikipedia.org/wiki/Analytic_function
Maybe find a proof of convergence to non-analytic solutions.


@phdthesis{talwalkar2010matrix,
  title={Matrix Approximation for Large-scale Learning},
  author={Talwalkar, A.},
  year={2010},
  school={New York University}
}
Ameet's thesis.

@phdthesis{halko2012randomized,
  title={Randomized methods for computing low-rank approximations of matrices},
  author={Halko, N.P.},
  year={2012},
  school={University of Colorado}
}
Nathan Halko thesis
FINISH TO READ THIS ONE

